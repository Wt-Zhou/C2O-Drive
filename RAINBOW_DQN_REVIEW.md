# Rainbow DQN å®Œæ•´æµç¨‹æ£€æŸ¥æŠ¥å‘Š

## æ£€æŸ¥æ—¥æœŸ
2026-01-26

## æ£€æŸ¥æ–¹æ³•
ç³»ç»Ÿæ€§å¯¹æ¯”PPOå’ŒRainbow DQNçš„æ‰€æœ‰å®žçŽ°ç»†èŠ‚ï¼Œä»Žè¾“å…¥åˆ°è¾“å‡ºé€æ­¥æ£€æŸ¥

---

## æ£€æŸ¥ç»“æžœæ€»ç»“

### âœ… æ ¸å¿ƒæµç¨‹å·²å®Œå…¨å¯¹é½

ä»¥ä¸‹éƒ¨åˆ†ä¸ŽPPOå®Œå…¨ä¸€è‡´ï¼ˆè€ƒè™‘ç®—æ³•å·®å¼‚ï¼‰ï¼š

1. **çŽ¯å¢ƒé‡ç½®å’Œè½¨è¿¹ç”Ÿæˆ** - å®Œå…¨ä¸€è‡´
2. **è½¨è¿¹æ‰§è¡Œ** - éƒ½ä½¿ç”¨OBBæ£€æµ‹ï¼Œç´¯ç§¯reward breakdown
3. **Episode-levelå­˜å‚¨** - éƒ½å­˜å‚¨ä¸€æ¡episode-level transition
4. **è®­ç»ƒè§¦å‘æ—¶æœº** - bufferæ»¡æ—¶è§¦å‘
5. **æ—¥å¿—è®°å½•** - reward breakdown, step-by-stepè·ç¦»åˆ†æž
6. **è¾“å‡ºç›®å½•ç»“æž„** - å¸¦æ—¶é—´æˆ³å­ç›®å½•

### âš ï¸ å·²è¯†åˆ«çš„é—®é¢˜å’Œæ”¹è¿›

#### 1. ç‰¹å¾æå–å·®å¼‚ï¼ˆè®¾è®¡ç†å¿µä¸åŒï¼‰

**PPOç‰¹å¾æå–**:
```python
def _extract_state_features(world_state):
    features = []

    # Ego: [x/100, y/100, speed/30, cos(yaw), sin(yaw)]
    features.extend([
        ego.position_m[0] / 100.0,      # å½’ä¸€åŒ–
        ego.position_m[1] / 100.0,
        ego_speed / 30.0,
        np.cos(ego.yaw_rad),
        np.sin(ego.yaw_rad),
    ])

    # Goal: [rel_x/100, rel_y/100]
    rel_x = (goal.position_m[0] - ego.position_m[0]) / 100.0  # ç›¸å¯¹è·ç¦»
    rel_y = (goal.position_m[1] - ego.position_m[1]) / 100.0
    features.extend([rel_x, rel_y])

    # Agents (max 10): [rel_x/100, rel_y/100, speed/30, cos(heading), sin(heading)]
    for agent in world_state.agents[:10]:
        rel_x = (agent.position_m[0] - ego.position_m[0]) / 100.0  # ç›¸å¯¹è·ç¦»
        rel_y = (agent.position_m[1] - ego.position_m[1]) / 100.0
        features.extend([rel_x, rel_y, agent_speed/30, ...])

    return tensor(features)  # 57ç»´
```

**å…³é”®ç‰¹æ€§**:
- âœ… ç›¸å¯¹åæ ‡ï¼ˆego-centricï¼‰
- âœ… å½’ä¸€åŒ–ï¼ˆ/100, /30ï¼‰
- âœ… Goalä¿¡æ¯
- âœ… å›ºå®š10ä¸ªagents

**Rainbow DQNç‰¹å¾æå–**:
```python
class WorldStateEncoder(nn.Module):
    def forward(world_state_batch):
        # Ego: [pos_x, pos_y, vel_x, vel_y, yaw]
        ego_feat = tensor([
            ws.ego.position_m[0],      # ç»å¯¹åæ ‡ï¼Œæ— å½’ä¸€åŒ–
            ws.ego.position_m[1],
            ws.ego.velocity_mps[0],
            ws.ego.velocity_mps[1],
            ws.ego.yaw_rad
        ])

        # Agents: [pos_x, pos_y, vel_x, vel_y, heading, type]
        for agent in ws.agents:
            agent_feat = tensor([
                agent.position_m[0],   # ç»å¯¹åæ ‡ï¼Œæ— å½’ä¸€åŒ–
                agent.position_m[1],
                agent.velocity_mps[0],
                agent.velocity_mps[1],
                agent.heading_rad,
                agent_type_encoding
            ])

        # Self-attentionèšåˆ
        encoded = self.attention(ego_query, agent_features)
        return self.fusion([ego_features, encoded])
```

**å…³é”®ç‰¹æ€§**:
- âŒ ç»å¯¹åæ ‡ï¼ˆworld-frameï¼‰
- âŒ æ— å½’ä¸€åŒ–
- âŒ æ— Goalä¿¡æ¯
- âœ… å¯å˜æ•°é‡agentsï¼ˆattentionï¼‰

**å¯¹æ¯”è¡¨æ ¼**:

| ç‰¹æ€§ | PPO | Rainbow DQN | å½±å“ |
|------|-----|-------------|------|
| åæ ‡ç³» | ç›¸å¯¹åæ ‡ (ego-centric) | ç»å¯¹åæ ‡ | **ä½ç½®ä¸å˜æ€§ç¼ºå¤±** |
| å½’ä¸€åŒ– | âœ“ (/100, /30) | âœ— | **æ•°å€¼èŒƒå›´å¤§ï¼Œè®­ç»ƒä¸ç¨³å®š** |
| Goal | âœ“ ç›¸å¯¹è·ç¦» | âœ— | **ç¼ºå°‘ç›®æ ‡å¯¼å‘** |
| Agentæ•° | å›ºå®š10ä¸ª | å¯å˜ (attention) | Rainbowæ›´çµæ´» |

**ç¤ºä¾‹é—®é¢˜**:

åœºæ™¯1: ego at (0, 0), agent at (10, 0)
- PPOç‰¹å¾: [..., rel_x=0.1, rel_y=0, ...]
- Rainbow DQN: [..., ego_x=0, ego_y=0, agent_x=10, agent_y=0, ...]

åœºæ™¯2: ego at (100, 0), agent at (110, 0) (ç›¸åŒç›¸å¯¹å…³ç³»ï¼)
- PPOç‰¹å¾: [..., rel_x=0.1, rel_y=0, ...] â† **ç›¸åŒ**
- Rainbow DQN: [..., ego_x=100, ego_y=0, agent_x=110, agent_y=0, ...] â† **ä¸åŒ**

**ç»“è®º**: Rainbow DQNå¯¹äºŽç›¸åŒç›¸å¯¹å…³ç³»ä½†ä¸åŒç»å¯¹ä½ç½®çš„åœºæ™¯ï¼Œä¼šäº§ç”Ÿä¸åŒçš„ç‰¹å¾ï¼Œç¼ºå°‘ä½ç½®ä¸å˜æ€§ã€‚

**å½±å“åˆ†æž**:
- ðŸŸ¡ **ä¸­ç­‰é£Žé™©**: ç¥žç»ç½‘ç»œç†è®ºä¸Šå¯ä»¥å­¦ä¹ è¿™äº›å…³ç³»
- ðŸŸ¡ **è®­ç»ƒéš¾åº¦å¢žåŠ **: éœ€è¦æ›´å¤šæ•°æ®æ‰èƒ½æ³›åŒ–
- ðŸŸ¡ **æ€§èƒ½å¯èƒ½ä¸‹é™**: å¦‚æžœè®­ç»ƒä¸è¶³

**å»ºè®®**:
1. **å…ˆæµ‹è¯•å½“å‰ç‰ˆæœ¬**ï¼šè¿è¡Œ10-50ä¸ªepisodesçœ‹æ•ˆæžœ
2. **å¦‚æžœæ€§èƒ½ä¸ä½³**ï¼šè€ƒè™‘æ”¹è¿›WorldStateEncoder
   - æ·»åŠ ç›¸å¯¹è·ç¦»è®¡ç®—å±‚
   - æ·»åŠ goalä¿¡æ¯ç¼–ç 
   - æ·»åŠ ç‰¹å¾å½’ä¸€åŒ–
   - éœ€è¦ä¿®æ”¹`src/c2o_drive/algorithms/rainbow_dqn/trajectory_encoder.py`

#### 2. è®­ç»ƒé¢‘çŽ‡å¯èƒ½è¿‡ä½Ž âœ… **å·²è¯†åˆ«**

**PPOè®­ç»ƒ**:
```python
# Bufferæ»¡æ—¶è§¦å‘
if buffer_len >= batch_size:
    metrics = self.planner._ppo_update()

# _ppo_updateå†…éƒ¨ï¼š
def _ppo_update():
    # å¤šè½®è®­ç»ƒ
    for epoch in range(ppo_epochs):  # é»˜è®¤4è½®
        # Mini-batchè®­ç»ƒ
        for batch in batches:
            # è®­ç»ƒç½‘ç»œ

    # æ¸…ç©ºbuffer
    self.rollout_buffer.clear()
```

**è®­ç»ƒé‡**: æ¯æ¬¡bufferæ»¡ï¼ˆå¦‚50ä¸ªepisodesï¼‰ï¼Œè®­ç»ƒ 4 epochs Ã— ~10 mini-batches = **40æ¬¡æ¢¯åº¦æ›´æ–°**

**Rainbow DQNè®­ç»ƒ**:
```python
# Bufferæ»¡æ—¶è§¦å‘
if buffer_len >= batch_size:
    metrics = self.planner._train_step()

# _train_stepå†…éƒ¨ï¼š
def _train_step():
    # é‡‡æ ·ä¸€ä¸ªbatch
    batch = self.replay_buffer.sample(batch_size)

    # è®­ç»ƒç½‘ç»œï¼ˆä¸€æ¬¡ï¼‰
    loss.backward()
    optimizer.step()
```

**è®­ç»ƒé‡**: æ¯æ¬¡bufferæ»¡ï¼Œè®­ç»ƒ **1æ¬¡æ¢¯åº¦æ›´æ–°**

**å¯¹æ¯”**:
- PPO: æ¯50ä¸ªepisodes â†’ 40æ¬¡æ›´æ–°
- Rainbow DQN: æ¯1ä¸ªepisode â†’ 1æ¬¡æ›´æ–°

**é—®é¢˜**: Rainbow DQNè™½ç„¶æ›´æ–°é¢‘ç¹ï¼Œä½†æ¯æ¬¡åªè®­ç»ƒ1ä¸ªbatchï¼Œæ•°æ®åˆ©ç”¨çŽ‡ä½Ž

**å»ºè®®**: å¢žåŠ æ¯æ¬¡è®­ç»ƒçš„iterations
```python
# run_rainbow_dqn_carla.py
TRAIN_ITERATIONS_PER_UPDATE = 4  # æ¯æ¬¡è®­ç»ƒ4ä¸ªbatch

if buffer_len >= batch_size:
    for _ in range(TRAIN_ITERATIONS_PER_UPDATE):
        metrics = self.planner._train_step()
```

#### 3. ç¼ºå°‘è®­ç»ƒmetricsæ‰“å° âœ… **å·²ä¿®å¤**

**ä¿®å¤å‰**:
```python
if buffer_len >= batch_size:
    print(f"  ðŸ”„ Rainbow DQNæ›´æ–°! buffer={buffer_len}")
    metrics = self.planner._train_step()
    # æ²¡æœ‰æ‰“å°metrics
```

**ä¿®å¤åŽ**:
```python
if buffer_len >= batch_size:
    print(f"  ðŸ”„ Rainbow DQNæ›´æ–°! buffer={buffer_len}")
    metrics = self.planner._train_step()
    if metrics and self.verbose:
        print(f"     loss={metrics.loss:.4f}, q_value={metrics.q_value:.4f}, td_error={metrics.custom['td_error_mean']:.4f}")
```

---

## å®Œæ•´æ–‡ä»¶ä¿®æ”¹è®°å½•

### å·²ä¿®æ”¹çš„æ–‡ä»¶

#### 1. `/home/dell/Desktop/C2O-Drive/examples/run_rainbow_dqn_carla.py`

**ä¿®æ”¹1**: State featuresæå– (lines 274-286)
- åˆ é™¤ä¸å­˜åœ¨çš„`_extract_state_features`è°ƒç”¨
- ç›´æŽ¥ä½¿ç”¨WorldState
- æ·»åŠ æŽ¢ç´¢æœºåˆ¶ï¼ˆreset_noise, train modeï¼‰

**ä¿®æ”¹2**: Episode-level transitions (lines 318-390)
- åˆ é™¤å¾ªçŽ¯å†…çš„`planner.update()`
- Episodeç»“æŸåŽå­˜å‚¨å•ä¸ªtransition
- æ·»åŠ OBBè·ç¦»è·Ÿè¸ª

**ä¿®æ”¹3**: Reward breakdownæ—¥å¿— (lines 309-310, 358-364, 443-474)
- ç´¯ç§¯å„rewardç»„ä»¶
- å†™å…¥reward_breakdown.txt
- å†™å…¥episode_summary.csv

**ä¿®æ”¹4**: è®­ç»ƒmetricsæ‰“å° (lines 393-407) âœ… **åˆšå®Œæˆ**
- æ·»åŠ loss, q_value, td_erroræ‰“å°

**ä¿®æ”¹5**: è¾“å‡ºç›®å½•ç»“æž„ (lines 693-701)
- æ·»åŠ timestampå­ç›®å½•
- ä¸ŽPPOä¸€è‡´

#### 2. `/home/dell/Desktop/C2O-Drive/src/c2o_drive/algorithms/rainbow_dqn/planner.py`

**ä¿®æ”¹**: æ·»åŠ _train_step()æ–¹æ³• (lines 293-373)
- ä»Župdate()ä¸­æŠ½å–è®­ç»ƒé€»è¾‘
- ç”¨äºŽepisode-levelè®­ç»ƒ
- ä¸ŽPPOçš„_ppo_update()ç±»ä¼¼

---

## éªŒè¯æµ‹è¯•è®¡åˆ’

### æµ‹è¯•1: åŸºç¡€åŠŸèƒ½æµ‹è¯•
```bash
python examples/run_rainbow_dqn_carla.py --scenario s4 --episodes 10 --max-steps 50
```

**é¢„æœŸè¾“å‡º**:
```
Episode 1/10 | Reward: -5.23 | Steps: 50 | Collision: False | Near-miss: True
  âš ï¸ NEAR-MISSæ£€æµ‹ï¼Step 12, OBB_dist=1.8m, center_dist=2.1m
  ðŸ”„ Rainbow DQNæ›´æ–°! buffer=1
     loss=2.3456, q_value=5.6789, td_error=1.2345

  Step-by-Step Distance Analysis:
  Step     Center Dist(m)     OBB Dist(m)        Near-Miss
  --------------------------------------------------------
  0        5.20               4.85
  1        4.98               4.62
  ...
  12       2.10               1.80               âœ“
```

**æ£€æŸ¥é¡¹**:
- [ ] æ¯ä¸ªepisodeåªå­˜å‚¨1ä¸ªtransition
- [ ] Episode rewardæ˜¯ç´¯ç§¯å€¼ï¼ˆ-5.23ï¼‰
- [ ] Near-missä½¿ç”¨OBBè·ç¦»
- [ ] è¾“å‡ºåˆ°`outputs/rainbow_dqn_carla/s4_YYYYMMDD_HHMMSS/`
- [ ] æœ‰æŽ¢ç´¢è¡Œä¸ºï¼ˆä¸åŒepisodeé€‰ä¸åŒè½¨è¿¹ï¼‰
- [ ] Bufferæ»¡æ—¶è§¦å‘è®­ç»ƒå¹¶æ‰“å°metrics
- [ ] æ—¥å¿—æ–‡ä»¶åŒ…å«reward breakdown

### æµ‹è¯•2: é•¿æœŸè®­ç»ƒæµ‹è¯•
```bash
python examples/run_rainbow_dqn_carla.py --scenario s4 --episodes 100 --max-steps 50
```

**æ£€æŸ¥é¡¹**:
- [ ] Lossé€æ¸ä¸‹é™
- [ ] Q-valueè¶‹äºŽç¨³å®š
- [ ] CollisionçŽ‡ä¸‹é™
- [ ] Episode rewardä¸Šå‡

### æµ‹è¯•3: å¯¹æ¯”PPO
```bash
# ç›¸åŒæ¡ä»¶ä¸‹å¯¹æ¯”
python examples/run_ppo_carla.py --scenario s4 --episodes 100 --max-steps 50
python examples/run_rainbow_dqn_carla.py --scenario s4 --episodes 100 --max-steps 50
```

**å¯¹æ¯”æŒ‡æ ‡**:
- [ ] CollisionçŽ‡
- [ ] Near-missçŽ‡
- [ ] å¹³å‡episode reward
- [ ] è®­ç»ƒç¨³å®šæ€§

---

## åŽç»­æ”¹è¿›å»ºè®®ï¼ˆå¯é€‰ï¼‰

### ä¼˜å…ˆçº§1: å¢žåŠ è®­ç»ƒé¢‘çŽ‡ âš ï¸ **å»ºè®®å®žæ–½**

åœ¨`run_rainbow_dqn_carla.py`ä¸­æ·»åŠ ï¼š
```python
# åœ¨run_episode()ä¸­çš„è®­ç»ƒéƒ¨åˆ†
TRAIN_ITERATIONS = 4  # æ¯æ¬¡è®­ç»ƒ4ä¸ªbatch

if buffer_len >= batch_size:
    print(f"  ðŸ”„ Rainbow DQNæ›´æ–°! buffer={buffer_len}")
    for i in range(TRAIN_ITERATIONS):
        metrics = self.planner._train_step()
        if i == TRAIN_ITERATIONS - 1:  # åªæ‰“å°æœ€åŽä¸€æ¬¡
            print(f"     loss={metrics.loss:.4f}, ...")
```

**é¢„æœŸæ•ˆæžœ**: æé«˜æ•°æ®åˆ©ç”¨çŽ‡ï¼ŒåŠ å¿«å­¦ä¹ 

### ä¼˜å…ˆçº§2: æ”¹è¿›WorldStateEncoder ðŸŸ¡ **è§†æµ‹è¯•ç»“æžœ**

å¦‚æžœæµ‹è¯•å‘çŽ°æ€§èƒ½ä¸ä½³ï¼Œä¿®æ”¹`src/c2o_drive/algorithms/rainbow_dqn/trajectory_encoder.py`:

```python
class WorldStateEncoder(nn.Module):
    def forward(self, world_state_batch):
        # æ·»åŠ å½’ä¸€åŒ–
        ego_feat = tensor([
            ws.ego.position_m[0] / 100.0,      # å½’ä¸€åŒ–
            ws.ego.position_m[1] / 100.0,
            ws.ego.velocity_mps[0] / 30.0,
            ws.ego.velocity_mps[1] / 30.0,
            ws.ego.yaw_rad / np.pi,
        ])

        # æ”¹ä¸ºç›¸å¯¹åæ ‡
        for agent in ws.agents:
            rel_x = (agent.position_m[0] - ws.ego.position_m[0]) / 100.0
            rel_y = (agent.position_m[1] - ws.ego.position_m[1]) / 100.0
            agent_feat = tensor([
                rel_x, rel_y,
                agent.velocity_mps[0] / 30.0,
                agent.velocity_mps[1] / 30.0,
                agent.heading_rad / np.pi,
                agent_type_encoding
            ])

        # æ·»åŠ goalä¿¡æ¯ï¼ˆå¦‚æžœæœ‰ï¼‰
        if hasattr(ws, 'goal') and ws.goal:
            goal_rel_x = (ws.goal.position_m[0] - ws.ego.position_m[0]) / 100.0
            goal_rel_y = (ws.goal.position_m[1] - ws.ego.position_m[1]) / 100.0
            goal_feat = tensor([goal_rel_x, goal_rel_y])
        else:
            goal_feat = tensor([0.0, 0.0])

        # Attentionèšåˆ
        encoded = self.attention(ego_query, agent_features)
        return self.fusion([ego_features, goal_feat, encoded])
```

**éœ€è¦ä¿®æ”¹**:
- `WorldStateEncoder.forward()`
- å¯èƒ½éœ€è¦è°ƒæ•´ç½‘ç»œç»´åº¦

### ä¼˜å…ˆçº§3: æ·»åŠ warmupæç¤º ðŸŸ¢ **Nice to have**

```python
# run_rainbow_dqn_carla.py
if buffer_len >= batch_size:
    if self.planner._step_count < self.planner.config.training.warmup_steps:
        if self.verbose:
            print(f"  â³ Warmup: {self.planner._step_count}/{self.planner.config.training.warmup_steps}")
    else:
        print(f"  ðŸ”„ Rainbow DQNæ›´æ–°! buffer={buffer_len}")
        metrics = self.planner._train_step()
```

---

## ç»“è®º

### å½“å‰çŠ¶æ€: âœ… **æ ¸å¿ƒç®—æ³•æ­£ç¡®ï¼Œå¯ä»¥è¿è¡Œ**

Rainbow DQNçš„å®žçŽ°åœ¨ç®—æ³•å±‚é¢æ˜¯æ­£ç¡®çš„ï¼Œæ‰€æœ‰å…³é”®ä¿®å¤å·²å®Œæˆï¼š
1. âœ… Episode-level transitionså­˜å‚¨
2. âœ… CARLA OBBæ£€æµ‹ä½¿ç”¨
3. âœ… è¯¦ç»†æ—¥å¿—è®°å½•
4. âœ… æŽ¢ç´¢æœºåˆ¶ï¼ˆNoisy Netsï¼‰
5. âœ… è¾“å‡ºç›®å½•ç»“æž„
6. âœ… è®­ç»ƒmetricsæ‰“å°

### ä¸ŽPPOçš„å·®å¼‚

**è®¾è®¡ç†å¿µä¸åŒï¼ˆæ­£å¸¸ï¼‰**:
- PPO: On-policy, æ‰‹åŠ¨ç‰¹å¾æå–, Categoricalé‡‡æ ·
- Rainbow DQN: Off-policy, ç¥žç»ç½‘ç»œç¼–ç , Noisy Nets

**æ½œåœ¨æ€§èƒ½é—®é¢˜ï¼ˆéœ€è§‚å¯Ÿï¼‰**:
- WorldStateEncoderä½¿ç”¨ç»å¯¹åæ ‡ï¼Œç¼ºå°‘ä½ç½®ä¸å˜æ€§
- è®­ç»ƒé¢‘çŽ‡å¯èƒ½åä½Ž

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³æ‰§è¡Œ**: è¿è¡Œæµ‹è¯•éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§
2. **çŸ­æœŸä¼˜åŒ–**: å¢žåŠ è®­ç»ƒé¢‘çŽ‡ï¼ˆTRAIN_ITERATIONS=4ï¼‰
3. **é•¿æœŸä¼˜åŒ–**: æ ¹æ®æµ‹è¯•ç»“æžœå†³å®šæ˜¯å¦æ”¹è¿›WorldStateEncoder

---

## é™„ä»¶

- è¯¦ç»†æµç¨‹å¯¹æ¯”: `RAINBOW_DQN_FLOW_CHECK.md`
- ä¿®å¤è®°å½•: `RAINBOW_DQN_FIXES.md`
