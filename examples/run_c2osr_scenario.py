#!/usr/bin/env python3
"""
C2OSR + ScenarioManager è¿è¡Œè„šæœ¬ï¼ˆæ–°æ¶æ„ç‰ˆæœ¬ï¼‰

ä½¿ç”¨æ–°æ¶æ„çš„ C2OSRPlanner å’Œ ScenarioReplayEnvironmentï¼Œ
å¤ç°åŸ run_sim_cl_simple.py çš„åŠŸèƒ½ï¼Œä½†åŸºäºæ ‡å‡† Gym æ¥å£ã€‚

ç‰¹æ€§ï¼š
- ä½¿ç”¨ C2OSRPlanner é€‚é…å™¨ï¼ˆæ¥è‡ª algorithms/c2osr/ï¼‰
- ä½¿ç”¨ ScenarioReplayEnvironmentï¼ˆGym æ ‡å‡†æ¥å£ï¼‰
- æ”¯æŒå¤š episodes è¿è¡Œ
- ç”Ÿæˆå¯è§†åŒ–å’Œç»Ÿè®¡æŠ¥å‘Š
- é…ç½®é¢„è®¾æ”¯æŒ
"""

from __future__ import annotations
import argparse
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
_repo_root = Path(__file__).resolve().parents[1]
if str(_repo_root) not in sys.path:
    sys.path.insert(0, str(_repo_root))

# æ–°æ¶æ„ç»„ä»¶
from carla_c2osr.algorithms.c2osr import (
    create_c2osr_planner,
    C2OSRPlannerConfig,
    LatticePlannerConfig,
    QValueConfig,
    GridConfig,
    DirichletConfig,
)
from carla_c2osr.environments import ScenarioReplayEnvironment
from carla_c2osr.env.scenario_manager import ScenarioManager
from carla_c2osr.core.planner import Transition

# å¯è§†åŒ–ç»„ä»¶
from visualization_utils import (
    EpisodeVisualizer,
    GlobalVisualizer,
    create_visualization_pipeline,
)


class EpisodeRunner:
    """Episode è¿è¡Œå™¨ï¼ˆæ‰¹é‡è½¨è¿¹è§„åˆ’æ¨¡å¼ï¼‰"""

    def __init__(
        self,
        planner,
        env,
        q_tracker=None,
        global_visualizer: Optional[GlobalVisualizer] = None,
        enable_visualization: bool = True,
        output_dir: Optional[Path] = None,
        verbose: bool = True,
        visualize_distributions: bool = False,
        vis_interval: int = 5,
    ):
        self.planner = planner
        self.env = env
        self.q_tracker = q_tracker
        self.global_visualizer = global_visualizer
        self.enable_visualization = enable_visualization
        self.output_dir = output_dir or Path("outputs/c2osr_scenario")
        self.verbose = verbose
        self.visualize_distributions = visualize_distributions
        self.vis_interval = vis_interval

    def run_episode(
        self,
        episode_id: int,
        max_steps: int,
        seed: Optional[int] = None,
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ª episodeï¼ˆæ‰¹é‡è½¨è¿¹è§„åˆ’æ¨¡å¼ï¼‰

        è¯¥æ–¹æ³•ä½¿ç”¨æ‰¹é‡è½¨è¿¹è§„åˆ’ï¼š
        1. åœ¨ episode å¼€å§‹æ—¶ç”Ÿæˆå¹¶è¯„ä¼°æ‰€æœ‰å€™é€‰è½¨è¿¹
        2. é€‰æ‹©æœ€ä¼˜è½¨è¿¹
        3. æ‰§è¡Œè¯¥è½¨è¿¹ï¼Œæ— éœ€åœ¨æ¯æ­¥é‡æ–°è§„åˆ’

        è¿™ä¸åŸå§‹ run_sim_cl_simple.py çš„è¡Œä¸ºä¸€è‡´ï¼Œæ¯”é€æ­¥è§„åˆ’å¿«çº¦ 20 å€ã€‚

        Args:
            episode_id: Episode ID
            max_steps: æœ€å¤§æ­¥æ•°
            seed: éšæœºç§å­

        Returns:
            Episode ç»Ÿè®¡æ•°æ®å­—å…¸ï¼ŒåŒ…å«è½¨è¿¹ä¿¡æ¯
        """
        # Reset ç¯å¢ƒ
        state, info = self.env.reset(seed=seed)
        reference_path = info.get('reference_path', [])

        if self.verbose:
            print(f"\n{'='*70}")
            print(f" Episode {episode_id + 1}")
            print(f"{'='*70}")

        # Reset è§„åˆ’å™¨
        self.planner.reset()

        # å­˜å‚¨ reference_path åˆ°è§„åˆ’å™¨
        if reference_path:
            self.planner.current_reference_path = [(p[0], p[1]) for p in reference_path]

        episode_start_time = time.time()

        # ===== æ‰¹é‡è½¨è¿¹è§„åˆ’ï¼šä¸€æ¬¡æ€§ç”Ÿæˆå¹¶è¯„ä¼°æ‰€æœ‰å€™é€‰è½¨è¿¹ =====
        if self.verbose:
            print(f"  ç”Ÿæˆå€™é€‰è½¨è¿¹...")

        # ç”Ÿæˆå¹¶è¯„ä¼°æ‰€æœ‰å€™é€‰è½¨è¿¹
        trajectory_q_values = self._generate_and_evaluate_trajectories(state, reference_path)

        if not trajectory_q_values:
            if self.verbose:
                print(f"  è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆè½¨è¿¹ç”Ÿæˆ")
            return {
                'episode_id': episode_id,
                'steps': 0,
                'total_reward': 0.0,
                'avg_reward': 0.0,
                'outcome': 'failure',
                'time': time.time() - episode_start_time,
                'trajectory_q_values': [],
                'selected_trajectory_info': None,
            }

        # é€‰æ‹©æœ€ä¼˜è½¨è¿¹
        selected_trajectory, selected_info = self._select_optimal_trajectory(trajectory_q_values)

        if self.verbose:
            print(f"  é€‰ä¸­è½¨è¿¹ {selected_info['trajectory_id']}: "
                  f"Min_Q={selected_info['min_q']:.2f}, "
                  f"Mean_Q={selected_info['mean_q']:.2f}, "
                  f"P{int(selected_info['selection_percentile']*100)}_Q={selected_info['percentile_q']:.2f}")

        # ===== å¯è§†åŒ–å’Œç»Ÿè®¡ =====
        episode_visualizer = None
        if self.enable_visualization:
            # åˆ›å»º episode å¯è§†åŒ–å™¨
            episode_visualizer = EpisodeVisualizer(
                episode_id=episode_id,
                output_dir=self.output_dir,
                grid_mapper=self.planner.grid_mapper,
                world_state=state,
                horizon=self.planner.config.lattice.horizon,
                verbose=self.verbose,
            )

            if self.verbose:
                print(f"  ç”Ÿæˆè½¨è¿¹é€‰æ‹©å¯è§†åŒ–...")

            # å¯è§†åŒ–è½¨è¿¹é€‰æ‹©
            episode_visualizer.visualize_trajectory_selection(
                trajectory_q_values, selected_info
            )

            # å¯é€‰ï¼šå¯è§†åŒ– Transition/Dirichlet åˆ†å¸ƒï¼ˆæ¯ N ä¸ª episodeï¼‰
            if (hasattr(self, 'visualize_distributions') and
                self.visualize_distributions and
                episode_id % self.vis_interval == 0):
                episode_visualizer.visualize_distributions(
                    q_calculator=self.planner.q_value_calculator,
                    world_state=state,
                    ego_action_trajectory=selected_info['waypoints'],
                    trajectory_buffer=self.planner.trajectory_buffer,
                    bank=self.planner.dirichlet_bank,
                )

        # è®°å½•æ‰€æœ‰è½¨è¿¹çš„ Q å€¼åˆ° tracker
        if self.q_tracker is not None:
            self.q_tracker.add_all_trajectories_data(episode_id, trajectory_q_values)

            # å¯è§†åŒ– Q å€¼æ¼”åŒ–
            if self.global_visualizer is not None:
                self.global_visualizer.visualize_q_evolution(episode_id)

        # ===== æ‰§è¡Œé€‰ä¸­çš„è½¨è¿¹ =====
        if self.verbose:
            print(f"  æ‰§è¡Œè½¨è¿¹...")

        # Episode ç»Ÿè®¡
        total_reward = 0.0
        steps = 0
        outcome = 'success'

        # æ‰§è¡Œè½¨è¿¹ï¼ˆæœ€å¤š max_steps æ­¥ï¼‰
        num_waypoints = len(selected_trajectory.waypoints)

        # è‡ªåŠ¨è°ƒæ•´max_stepsåˆ°è½¨è¿¹é•¿åº¦
        if max_steps > num_waypoints - 1:
            if self.verbose:
                print(f"  è°ƒæ•´: max_stepsä»{max_steps}è°ƒæ•´ä¸º{num_waypoints - 1}ï¼ˆè½¨è¿¹é•¿åº¦é™åˆ¶ï¼‰")
            max_steps = num_waypoints - 1

        actual_steps = min(max_steps, num_waypoints - 1)  # -1 because we start from current position

        for step in range(actual_steps):
            # å°†è½¨è¿¹ç‚¹è½¬æ¢ä¸ºæ§åˆ¶åŠ¨ä½œ
            action = self._trajectory_to_control(state, selected_trajectory, step)

            # æ‰§è¡ŒåŠ¨ä½œ
            step_result = self.env.step(action)

            # ========== å¯è§†åŒ–å½“å‰æ—¶é—´æ­¥ ==========
            if episode_visualizer is not None:
                try:
                    # å‡†å¤‡å¯è§†åŒ–æ•°æ®
                    prob_grid, reachable_sets = self._prepare_timestep_visualization(
                        state=step_result.observation,
                        step=step,
                    )

                    # è·å–ç»Ÿè®¡ä¿¡æ¯
                    buffer_size = len(self.planner.trajectory_buffer)

                    # è®¡ç®—æ€»alphaå€¼ï¼ˆæ‰€æœ‰agentsçš„alphaæ€»å’Œï¼‰
                    total_alpha = 0.0
                    for agent_id in self.planner.dirichlet_bank.agent_alphas:
                        for timestep_idx in self.planner.dirichlet_bank.agent_alphas[agent_id]:
                            total_alpha += self.planner.dirichlet_bank.agent_alphas[agent_id][timestep_idx].sum()

                    # æ¸²æŸ“çƒ­åŠ›å›¾å¸§ï¼ˆä» t=1 å¼€å§‹ç¼–å·ï¼Œä¸åŸç‰ˆæœ¬ä¸€è‡´ï¼‰
                    frame_path = episode_visualizer.render_timestep_heatmap(
                        timestep=step + 1,
                        current_world_state=step_result.observation,
                        prob_grid=prob_grid,
                        multi_timestep_reachable_sets=reachable_sets,
                        buffer_size=buffer_size,
                        matched_transitions=None,  # å¯ä»¥åç»­æ·»åŠ 
                        total_alpha=total_alpha,
                    )
                except Exception as e:
                    if self.verbose:
                        print(f"    è­¦å‘Š: æ—¶é—´æ­¥ {step+1} å¯è§†åŒ–å¤±è´¥: {e}")
            # =======================================

            # æ›´æ–°è§„åˆ’å™¨ï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰
            transition = Transition(
                state=state,
                action=action,
                reward=step_result.reward,
                next_state=step_result.observation,
                terminated=step_result.terminated,
                truncated=step_result.truncated,
                info=step_result.info,
            )
            self.planner.update(transition)

            # æ›´æ–°ç»Ÿè®¡
            total_reward += step_result.reward
            steps += 1
            state = step_result.observation

            if self.verbose and (step + 1) % 5 == 0:
                print(f"  Step {step+1}/{actual_steps}: reward={step_result.reward:.2f}, "
                      f"total={total_reward:.2f}")

            # æ£€æŸ¥ç»ˆæ­¢
            if step_result.terminated:
                outcome = 'collision'
                if self.verbose:
                    print(f"  âœ— ç¢°æ’ï¼Episode åœ¨ç¬¬ {steps} æ­¥ç»“æŸ")
                break
            if step_result.truncated:
                outcome = 'timeout'
                if self.verbose:
                    print(f"  â± è¶…æ—¶ï¼Episode åœ¨ç¬¬ {steps} æ­¥ç»“æŸ")
                break

        # Send final transition for successful completions to trigger buffer storage
        if outcome == 'success':
            final_transition = Transition(
                state=state,
                action=action,
                reward=0.0,
                next_state=state,
                terminated=False,
                truncated=True,  # Mark episode as complete
                info={},
            )
            self.planner.update(final_transition)

        episode_time = time.time() - episode_start_time

        if outcome == 'success' and self.verbose:
            print(f"  âœ“ æˆåŠŸå®Œæˆ {steps} æ­¥ï¼")

        # ===== ç”Ÿæˆ episode GIF =====
        gif_path = ""
        if episode_visualizer is not None and episode_visualizer.frame_paths:
            if self.verbose:
                print(f"  ç”Ÿæˆ episode GIF...")

            gif_path = episode_visualizer.generate_episode_gif()

            # æ·»åŠ æœ€åä¸€å¸§åˆ°æ±‡æ€»
            if self.global_visualizer is not None and episode_visualizer.frame_paths:
                last_frame = episode_visualizer.frame_paths[-1]
                self.global_visualizer.add_summary_frame(last_frame)

        # è¿”å›ç»Ÿè®¡æ•°æ®ï¼ˆåŒ…å«è½¨è¿¹ä¿¡æ¯ï¼‰
        result = {
            'episode_id': episode_id,
            'steps': steps,
            'total_reward': total_reward,
            'avg_reward': total_reward / steps if steps > 0 else 0.0,
            'outcome': outcome,
            'time': episode_time,
            'trajectory_q_values': trajectory_q_values,
            'selected_trajectory_info': selected_info,
            'gif_path': gif_path,
        }

        if self.verbose:
            print(f"  æ€»å¥–åŠ±: {total_reward:.2f}, å¹³å‡: {result['avg_reward']:.2f}")
            print(f"  è€—æ—¶: {episode_time:.2f}s")
            if gif_path:
                print(f"  GIF: {gif_path}")

        return result

    def _generate_and_evaluate_trajectories(
        self,
        state: Any,
        reference_path: List,
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¹¶è¯„ä¼°æ‰€æœ‰å€™é€‰è½¨è¿¹

        Args:
            state: å½“å‰çŠ¶æ€
            reference_path: å‚è€ƒè·¯å¾„

        Returns:
            åŒ…å«æ¯æ¡è½¨è¿¹ Q å€¼ä¿¡æ¯çš„åˆ—è¡¨
        """
        # ç”Ÿæˆå€™é€‰è½¨è¿¹
        ego_state_tuple = (
            state.ego.position_m[0],
            state.ego.position_m[1],
            state.ego.yaw_rad,
        )

        ref_path = [(p[0], p[1]) for p in reference_path] if reference_path else None
        if ref_path is None:
            # åˆ›å»ºç®€å•å‰å‘è·¯å¾„
            ego_x, ego_y = state.ego.position_m
            ref_path = [
                (ego_x + i * 5.0, ego_y) for i in range(self.planner.config.lattice.horizon + 1)
            ]

        candidate_trajectories = self.planner.lattice_planner.generate_trajectories(
            reference_path=ref_path,
            horizon=self.planner.config.lattice.horizon,
            dt=self.planner.config.lattice.dt,
            ego_state=ego_state_tuple,
        )

        if self.verbose:
            print(f"    ç”Ÿæˆ {len(candidate_trajectories)} æ¡å€™é€‰è½¨è¿¹")

        # è¯„ä¼°æ¯æ¡å€™é€‰è½¨è¿¹
        trajectory_q_values = []

        for traj_idx, trajectory in enumerate(candidate_trajectories):
            try:
                # è®¡ç®— Q å€¼
                q_values_list, detailed_info = self.planner.q_value_calculator.compute_q_value(
                    current_world_state=state,
                    ego_action_trajectory=trajectory.waypoints,
                    trajectory_buffer=self.planner.trajectory_buffer,
                    grid=self.planner.grid_mapper,
                    bank=self.planner.dirichlet_bank,
                    reference_path=ref_path,
                )

                if len(q_values_list) > 0:
                    # è®¡ç®— Q å€¼ç»Ÿè®¡
                    min_q = float(np.min(q_values_list))
                    mean_q = float(np.mean(q_values_list))
                    max_q = float(np.max(q_values_list))

                    # è®¡ç®—ç™¾åˆ†ä½ Q å€¼
                    percentile = self.planner.config.q_value.selection_percentile
                    if percentile == 0.0:
                        percentile_q = min_q
                    elif percentile == 1.0:
                        percentile_q = max_q
                    else:
                        percentile_q = float(np.percentile(q_values_list, percentile * 100))

                    # è·å–ç¢°æ’ç‡
                    collision_rate = detailed_info.get('reward_breakdown', {}).get('collision_rate', 0.0)

                    trajectory_info = {
                        'trajectory_id': traj_idx,
                        'trajectory': trajectory,
                        'waypoints': trajectory.waypoints,
                        'lateral_offset': trajectory.lateral_offset,
                        'target_speed': trajectory.target_speed,
                        'min_q': min_q,
                        'mean_q': mean_q,
                        'max_q': max_q,
                        'percentile_q': percentile_q,
                        'selection_percentile': percentile,
                        'collision_rate': collision_rate,
                        'q_values': q_values_list,
                        'detailed_info': detailed_info,
                    }

                    trajectory_q_values.append(trajectory_info)

            except Exception as e:
                if self.verbose:
                    print(f"    è­¦å‘Šï¼šè½¨è¿¹ {traj_idx} çš„ Q å€¼è®¡ç®—å¤±è´¥: {e}")
                continue

        return trajectory_q_values

    def _select_optimal_trajectory(
        self,
        trajectory_q_values: List[Dict[str, Any]],
    ) -> tuple:
        """æ ¹æ®ç™¾åˆ†ä½ Q å€¼é€‰æ‹©æœ€ä¼˜è½¨è¿¹

        Args:
            trajectory_q_values: è½¨è¿¹ Q å€¼åˆ—è¡¨

        Returns:
            (selected_trajectory, selected_info)
        """
        if not trajectory_q_values:
            return None, None

        # é€‰æ‹© percentile_q æœ€å¤§çš„è½¨è¿¹
        best_trajectory_info = max(trajectory_q_values, key=lambda x: x['percentile_q'])

        return best_trajectory_info['trajectory'], best_trajectory_info

    def _trajectory_to_control(self, current_state, trajectory, step_idx):
        """å°†è½¨è¿¹çš„æŸä¸€æ­¥è½¬æ¢ä¸ºæ§åˆ¶åŠ¨ä½œ

        Args:
            current_state: å½“å‰çŠ¶æ€
            trajectory: è½¨è¿¹å¯¹è±¡
            step_idx: æ­¥ç´¢å¼•

        Returns:
            æ§åˆ¶åŠ¨ä½œ
        """
        # ä½¿ç”¨ planner çš„è½¬æ¢æ–¹æ³•
        return self.planner._trajectory_to_control(current_state.ego, trajectory)

    def _prepare_timestep_visualization(self, state, step: int):
        """å‡†å¤‡æ—¶é—´æ­¥å¯è§†åŒ–æ•°æ®

        Args:
            state: å½“å‰ä¸–ç•ŒçŠ¶æ€
            step: æ—¶é—´æ­¥ç´¢å¼•

        Returns:
            (prob_grid, multi_timestep_reachable_sets)
        """
        # 1. è®¡ç®— agents çš„å¤šæ—¶é—´æ­¥å¯è¾¾é›†
        config = self.planner.config
        multi_timestep_reachable_sets = {}

        for i, agent in enumerate(state.agents):
            agent_id = i + 1
            try:
                multi_reachable = self.planner.grid_mapper.multi_timestep_successor_cells(
                    agent,
                    horizon=config.lattice.horizon,
                    dt=config.lattice.dt,
                    n_samples=50,  # é‡‡æ ·æ•°
                )
                multi_timestep_reachable_sets[agent_id] = multi_reachable
            except Exception as e:
                # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å½“å‰ä½ç½®ä½œä¸ºå¯è¾¾é›†
                current_cell = self.planner.grid_mapper.world_to_cell(agent.position_m)
                multi_timestep_reachable_sets[agent_id] = {
                    t: [current_cell] for t in range(1, config.lattice.horizon + 1)
                }

        # 2. è·å–æ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äºçƒ­åŠ›å›¾ï¼‰
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
        # å®Œæ•´ç‰ˆæœ¬éœ€è¦ä» Dirichlet bank ä¸­è·å–åéªŒå‡å€¼
        K = self.planner.grid_mapper.K
        prob_grid = np.ones(K) / K

        return prob_grid, multi_timestep_reachable_sets


class StatisticsCollector:
    """ç»Ÿè®¡æ”¶é›†å™¨"""

    def __init__(self):
        self.episodes_results: List[Dict[str, Any]] = []

    def add_episode(self, result: Dict[str, Any]):
        """æ·»åŠ  episode ç»“æœ"""
        self.episodes_results.append(result)

    def print_summary(self):
        """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""
        if not self.episodes_results:
            print("\næ²¡æœ‰å®Œæˆçš„ episode")
            return

        print(f"\n{'='*70}")
        print(" å®éªŒæ±‡æ€»")
        print(f"{'='*70}")

        # åŸºæœ¬ç»Ÿè®¡
        num_episodes = len(self.episodes_results)
        total_steps = sum(r['steps'] for r in self.episodes_results)
        total_time = sum(r['time'] for r in self.episodes_results)

        # ç»“æœç»Ÿè®¡
        outcomes = [r['outcome'] for r in self.episodes_results]
        success_count = outcomes.count('success')
        collision_count = outcomes.count('collision')
        timeout_count = outcomes.count('timeout')

        # å¥–åŠ±ç»Ÿè®¡
        rewards = [r['total_reward'] for r in self.episodes_results]
        avg_reward = np.mean(rewards)
        std_reward = np.std(rewards)

        # æ­¥æ•°ç»Ÿè®¡
        steps_list = [r['steps'] for r in self.episodes_results]
        avg_steps = np.mean(steps_list)

        print(f"\nEpisodes: {num_episodes}")
        print(f"æˆåŠŸç‡: {success_count/num_episodes*100:.1f}% "
              f"({success_count} æˆåŠŸ, {collision_count} ç¢°æ’, {timeout_count} è¶…æ—¶)")
        print(f"\nå¹³å‡å¥–åŠ±: {avg_reward:.2f} Â± {std_reward:.2f}")
        print(f"å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"å¹³å‡é€Ÿåº¦: {total_steps/total_time:.1f} steps/s")

        # æœ€ä½³å’Œæœ€å·® episode
        best_idx = np.argmax(rewards)
        worst_idx = np.argmin(rewards)

        print(f"\næœ€ä½³ Episode: #{self.episodes_results[best_idx]['episode_id']+1}")
        print(f"  å¥–åŠ±: {self.episodes_results[best_idx]['total_reward']:.2f}, "
              f"æ­¥æ•°: {self.episodes_results[best_idx]['steps']}, "
              f"ç»“æœ: {self.episodes_results[best_idx]['outcome']}")

        print(f"\næœ€å·® Episode: #{self.episodes_results[worst_idx]['episode_id']+1}")
        print(f"  å¥–åŠ±: {self.episodes_results[worst_idx]['total_reward']:.2f}, "
              f"æ­¥æ•°: {self.episodes_results[worst_idx]['steps']}, "
              f"ç»“æœ: {self.episodes_results[worst_idx]['outcome']}")

        print(f"\n{'='*70}")


def create_planner_config(args) -> C2OSRPlannerConfig:
    """åˆ›å»ºè§„åˆ’å™¨é…ç½®

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        C2OSRPlannerConfig å®ä¾‹
    """
    # æ ¹æ®é¢„è®¾åˆ›å»ºé…ç½®
    grid_half = args.grid_size / 2.0

    if args.config_preset == "fast":
        # å¿«é€Ÿæµ‹è¯•é…ç½®
        config = C2OSRPlannerConfig(
            # ğŸ¯ ç»Ÿä¸€çš„horizoné…ç½®
            horizon=args.horizon,

            grid=GridConfig(
                grid_size_m=1.0,  # æ¯ä¸ªcell 1ç±³
                bounds_x=(-grid_half, grid_half),
                bounds_y=(-grid_half, grid_half),
            ),
            lattice=LatticePlannerConfig(
                # horizonç”±__post_init__è‡ªåŠ¨åŒæ­¥
                lateral_offsets=[-2.0, 0.0, 2.0],
                speed_variations=[3.0, 5.0],
                num_trajectories=6,  # 3Ã—2=6 å®Œæ•´ç»„åˆ
                dt=args.dt,
            ),
            q_value=QValueConfig(
                # horizonç”±__post_init__è‡ªåŠ¨åŒæ­¥
                n_samples=20,  # å¿«é€Ÿæ¨¡å¼ï¼šå°‘é‡‡æ ·
            ),
        )
    elif args.config_preset == "high-precision":
        # é«˜ç²¾åº¦é…ç½®
        config = C2OSRPlannerConfig(
            # ğŸ¯ ç»Ÿä¸€çš„horizoné…ç½®
            horizon=args.horizon,

            grid=GridConfig(
                grid_size_m=0.5,  # æ¯ä¸ªcell 0.5ç±³ï¼ˆæ›´ç²¾ç»†ï¼‰
                bounds_x=(-grid_half, grid_half),
                bounds_y=(-grid_half, grid_half),
            ),
            lattice=LatticePlannerConfig(
                # horizonç”±__post_init__è‡ªåŠ¨åŒæ­¥
                lateral_offsets=[-4.0, -2.0, 0.0, 2.0, 4.0],  # æ›´å¤šå€™é€‰
                speed_variations=[2.0, 3.0, 5.0, 7.0],
                num_trajectories=20,  # 5Ã—4=20 å®Œæ•´ç»„åˆ
                dt=args.dt,
            ),
            q_value=QValueConfig(
                # horizonç”±__post_init__è‡ªåŠ¨åŒæ­¥
                n_samples=100,  # é«˜ç²¾åº¦ï¼šå¤šé‡‡æ ·
            ),
        )
    else:
        # é»˜è®¤é…ç½®ï¼ˆä½¿ç”¨ config.py ä¸­çš„é»˜è®¤å€¼ï¼‰
        config = C2OSRPlannerConfig(
            # ğŸ¯ ç»Ÿä¸€çš„horizoné…ç½®
            horizon=args.horizon,

            grid=GridConfig(
                # åªè¦†ç›–è¿è¡Œæ—¶å‚æ•°ï¼Œå…¶ä»–ä½¿ç”¨ config.py é»˜è®¤å€¼
                bounds_x=(-grid_half, grid_half),
                bounds_y=(-grid_half, grid_half),
            ),
            lattice=LatticePlannerConfig(
                # horizonç”±__post_init__è‡ªåŠ¨åŒæ­¥ï¼Œåªè¦†ç›–dt
                dt=args.dt,
            ),
            q_value=QValueConfig(
                # horizonç”±__post_init__è‡ªåŠ¨åŒæ­¥
            ),
        )

    return config


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="C2OSR + ScenarioManager å®éªŒï¼ˆæ–°æ¶æ„ç‰ˆæœ¬ï¼‰"
    )

    # åŸºæœ¬è¿è¡Œå‚æ•°
    parser.add_argument("--episodes", type=int, default=10,
                       help="æ‰§è¡Œ episode æ•°")
    parser.add_argument("--max-steps", type=int, default=50,
                       help="æ¯ä¸ª episode çš„æœ€å¤§æ­¥æ•°")
    parser.add_argument("--seed", type=int, default=2025,
                       help="éšæœºç§å­")

    # åœºæ™¯å‚æ•°
    parser.add_argument("--reference-path-mode",
                       choices=["straight", "curve", "s_curve"],
                       default="straight",
                       help="å‚è€ƒè·¯å¾„æ¨¡å¼")

    # é…ç½®é¢„è®¾
    parser.add_argument("--config-preset",
                       choices=["default", "fast", "high-precision"],
                       default="default",
                       help="é…ç½®é¢„è®¾")

    # è§„åˆ’å‚æ•°
    parser.add_argument("--horizon", type=int, default=10,
                       help="è§„åˆ’æ—¶åŸŸï¼ˆæ­¥æ•°ï¼‰")
    parser.add_argument("--dt", type=float, default=0.5,
                       help="æ—¶é—´æ­¥é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--grid-size", type=float, default=50.0,
                       help="ç½‘æ ¼å¤§å°ï¼ˆç±³ï¼‰")

    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output-dir", type=str,
                       default="outputs/c2osr_scenario",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--quiet", action="store_true",
                       help="é™é»˜æ¨¡å¼ï¼ˆå‡å°‘è¾“å‡ºï¼‰")

    # å¯è§†åŒ–å‚æ•°
    parser.add_argument("--visualize-distributions", action="store_true",
                       default=True,  # é»˜è®¤å¯ç”¨åˆ†å¸ƒå¯è§†åŒ–
                       help="ç”Ÿæˆ Transition/Dirichlet åˆ†å¸ƒå¯è§†åŒ–")
    parser.add_argument("--vis-interval", type=int, default=5,
                       help="åˆ†å¸ƒå¯è§†åŒ–çš„é—´éš”ï¼ˆæ¯ N ä¸ª episode ç”Ÿæˆä¸€æ¬¡ï¼‰")

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # 1. è§£æå‚æ•°
    args = parse_arguments()

    print(f"{'='*70}")
    print(f" C2OSR + ScenarioManager å®éªŒï¼ˆæ–°æ¶æ„ç‰ˆæœ¬ï¼‰")
    print(f"{'='*70}")
    print(f"\né…ç½®:")
    print(f"  Episodes: {args.episodes}")
    print(f"  Max steps: {args.max_steps}")
    print(f"  Horizon: {args.horizon}, dt: {args.dt}s")
    print(f"  Reference path: {args.reference_path_mode}")
    print(f"  Config preset: {args.config_preset}")
    print(f"  Grid size: {args.grid_size}m")
    print(f"  Seed: {args.seed}")

    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"  Output: {output_dir}")

    # 3. åˆ›å»ºå¯è§†åŒ–ç®¡é“
    enable_visualization = not args.quiet
    q_tracker, global_visualizer = create_visualization_pipeline(
        output_dir=output_dir,
        enable_visualization=enable_visualization,
    )
    if enable_visualization:
        print(f"  âœ“ å¯è§†åŒ–ç®¡é“åˆ›å»ºå®Œæˆ")

    # 4. åˆ›å»ºç»„ä»¶
    print(f"\nåˆ›å»ºç»„ä»¶...")

    # åˆ›å»ºåœºæ™¯ç®¡ç†å™¨
    scenario_manager = ScenarioManager(grid_size_m=args.grid_size)

    # åˆ›å»ºç¯å¢ƒ
    env = ScenarioReplayEnvironment(
        scenario_manager=scenario_manager,
        reference_path_mode=args.reference_path_mode,
        dt=args.dt,
        max_episode_steps=args.max_steps,
        horizon=args.horizon,
    )

    # åˆ›å»ºè§„åˆ’å™¨é…ç½®
    planner_config = create_planner_config(args)

    # åˆ›å»ºè§„åˆ’å™¨
    planner = create_c2osr_planner(planner_config)

    print(f"  âœ“ ScenarioManager åˆ›å»ºå®Œæˆ")
    print(f"  âœ“ ScenarioReplayEnvironment åˆ›å»ºå®Œæˆ")
    print(f"  âœ“ C2OSRPlanner åˆ›å»ºå®Œæˆ")

    # 5. åˆ›å»ºè¿è¡Œå™¨å’Œç»Ÿè®¡æ”¶é›†å™¨
    runner = EpisodeRunner(
        planner=planner,
        env=env,
        q_tracker=q_tracker,
        global_visualizer=global_visualizer,
        enable_visualization=enable_visualization,
        output_dir=output_dir,
        verbose=not args.quiet,
        visualize_distributions=args.visualize_distributions,
        vis_interval=args.vis_interval,
    )
    stats_collector = StatisticsCollector()

    # 5. è¿è¡Œ episodes
    print(f"\nå¼€å§‹è¿è¡Œ {args.episodes} ä¸ª episodes...")

    experiment_start_time = time.time()

    for episode_id in range(args.episodes):
        try:
            result = runner.run_episode(
                episode_id=episode_id,
                max_steps=args.max_steps,
                seed=args.seed + episode_id,
            )
            stats_collector.add_episode(result)

        except Exception as e:
            print(f"\nâœ— Episode {episode_id + 1} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    experiment_time = time.time() - experiment_start_time

    # 7. æ‰“å°æ±‡æ€»
    stats_collector.print_summary()

    # 8. ç”Ÿæˆå…¨å±€å¯è§†åŒ–
    if enable_visualization and global_visualizer is not None:
        print(f"\nç”Ÿæˆå…¨å±€å¯è§†åŒ–...")

        # ç”Ÿæˆæ±‡æ€» GIF
        summary_gif = global_visualizer.generate_summary_gif()
        if summary_gif:
            print(f"  âœ“ æ±‡æ€» GIF: {summary_gif}")

        # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡å›¾
        global_visualizer.generate_final_plots()

    print(f"\nå®éªŒæ€»è€—æ—¶: {experiment_time:.2f}s")
    print(f"\nâœ“ å®éªŒå®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_dir}")

    # 9. å…³é—­ç¯å¢ƒ
    env.close()


if __name__ == "__main__":
    main()
