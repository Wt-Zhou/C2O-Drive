# C2O-Drive ç®—æ³•æ¶æ„åˆ†æ

## ç›®å½•
- [æ•´ä½“æ¶æ„æ¦‚è§ˆ](#æ•´ä½“æ¶æ„æ¦‚è§ˆ)
- [æ ¸å¿ƒæ¥å£è®¾è®¡](#æ ¸å¿ƒæ¥å£è®¾è®¡)
- [ç®—æ³•åˆ†ç±»](#ç®—æ³•åˆ†ç±»)
- [è¿è¡Œæµç¨‹å¯¹æ¯”](#è¿è¡Œæµç¨‹å¯¹æ¯”)
- [æ–‡ä»¶ç»„ç»‡ç»“æ„](#æ–‡ä»¶ç»„ç»‡ç»“æ„)

---

## æ•´ä½“æ¶æ„æ¦‚è§ˆ

ä½ çš„ä»£ç åº“é‡‡ç”¨äº†**ä¸¤ç§è®¾è®¡æ¨¡å¼**æ¥ç»„ç»‡ç®—æ³•ï¼š

### 1. **Planneræ¨¡å¼** (æ ‡å‡†åŒ–æ¥å£)
- ç»§æ‰¿è‡ª `EpisodicAlgorithmPlanner[WorldState, EgoControl]`
- éµå¾ªç»Ÿä¸€çš„è§„åˆ’å™¨æ¥å£
- ç”¨äºï¼š**C2OSR, PPO, Rainbow DQN, RCRL**

### 2. **Agentæ¨¡å¼** (ä¼ ç»ŸRLæ¥å£)
- ç‹¬ç«‹çš„Agentç±»ï¼ˆä¸ç»§æ‰¿Plannerï¼‰
- ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ¥å£
- ç”¨äºï¼š**SAC, DQN**

---

## æ ¸å¿ƒæ¥å£è®¾è®¡

### BasePlanner æ¥å£ (`src/c2o_drive/core/planner.py`)

æ‰€æœ‰ç®—æ³•éƒ½éœ€è¦å®ç°ä»¥ä¸‹æ–¹æ³•ï¼š

```python
class BasePlanner(ABC, Generic[ObsType, ActType]):
    @abstractmethod
    def select_action(self, observation: ObsType,
                      deterministic: bool = False,
                      **kwargs) -> ActType:
        """é€‰æ‹©åŠ¨ä½œ"""
        pass

    @abstractmethod
    def update(self, transition: Transition[ObsType, ActType]) -> UpdateMetrics:
        """æ›´æ–°å­¦ä¹ ï¼ˆä»transitionä¸­å­¦ä¹ ï¼‰"""
        pass

    @abstractmethod
    def reset(self) -> None:
        """é‡ç½®å†…éƒ¨çŠ¶æ€ï¼ˆæ¯ä¸ªepisodeå¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
        pass

    def save_checkpoint(self, path: str | Path) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        pass

    def load_checkpoint(self, path: str | Path) -> None:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        pass
```

### EpisodicPlanner æ‰©å±•æ¥å£

è½¨è¿¹çº§åˆ«è§„åˆ’å™¨éœ€è¦é¢å¤–å®ç°ï¼š

```python
class EpisodicPlanner(BasePlanner[ObsType, ActType]):
    @abstractmethod
    def plan_trajectory(self, observation: ObsType,
                       horizon: int,
                       **kwargs) -> List[ActType]:
        """è§„åˆ’ä¸€æ¡å®Œæ•´è½¨è¿¹"""
        pass
```

### EpisodicAlgorithmPlanner

èåˆäº† `BaseAlgorithmPlanner` å’Œ `EpisodicPlanner`ï¼š

```python
class EpisodicAlgorithmPlanner(
    BaseAlgorithmPlanner[ObsType, ActType],
    EpisodicPlanner[ObsType, ActType],
    Generic[ObsType, ActType]
):
    """ç”¨äºè½¨è¿¹çº§åˆ«è§„åˆ’çš„åŸºç±»"""
    pass
```

---

## ç®—æ³•åˆ†ç±»

### ğŸ¯ Planneræ¨¡å¼ç®—æ³•

| ç®—æ³• | ç±»å | ç»§æ‰¿å…³ç³» | åŠ¨ä½œç©ºé—´ | ç‰¹ç‚¹ |
|------|------|----------|----------|------|
| **C2OSR** | `C2OSRPlanner` | `EpisodicAlgorithmPlanner` | ç¦»æ•£ï¼ˆlatticeï¼‰ | è´å¶æ–¯é£é™©æ„ŸçŸ¥ |
| **PPO** | `PPOPlanner` | `EpisodicAlgorithmPlanner` | ç¦»æ•£ï¼ˆlatticeï¼‰ | On-policyï¼ŒActor-Critic |
| **Rainbow DQN** | `RainbowDQNPlanner` | `EpisodicAlgorithmPlanner` | ç¦»æ•£ï¼ˆlatticeï¼‰ | DQNæ”¹è¿›ç‰ˆ |
| **RCRL** | `RCRLPlanner` | `EpisodicAlgorithmPlanner` | ç¦»æ•£ï¼ˆlatticeï¼‰ | å¸¦çº¦æŸçš„RL |

#### å…±åŒç‰¹ç‚¹ï¼š
1. âœ… ç»Ÿä¸€æ¥å£ï¼š`select_action()`, `update()`, `reset()`
2. âœ… è½¨è¿¹çº§åˆ«æ‰§è¡Œï¼šä¸€æ¬¡ç”Ÿæˆå®Œæ•´è½¨è¿¹ï¼Œç„¶åé€æ­¥æ‰§è¡Œ
3. âœ… ä½¿ç”¨ `LatticePlanner` ç”Ÿæˆå€™é€‰è½¨è¿¹
4. âœ… è¾“å…¥ï¼š`WorldState`ï¼Œè¾“å‡ºï¼š`EgoControl`
5. âœ… åŠ¨æ€åŠ¨ä½œç©ºé—´ï¼š`action_dim = len(lateral_offsets) Ã— len(speed_variations)`

### ğŸ¤– Agentæ¨¡å¼ç®—æ³•

| ç®—æ³• | ç±»å | ç»§æ‰¿å…³ç³» | åŠ¨ä½œç©ºé—´ | ç‰¹ç‚¹ |
|------|------|----------|----------|------|
| **SAC** | `SACAgent` | æ— ï¼ˆç‹¬ç«‹ç±»ï¼‰ | è¿ç»­ï¼ˆéœ€è¦rescaleï¼‰ | Off-policyï¼ŒActor-Critic |
| **DQN** | `DQNAgent` | æ— ï¼ˆç‹¬ç«‹ç±»ï¼‰ | ç¦»æ•£ | Q-learning |

#### å…±åŒç‰¹ç‚¹ï¼š
1. âŒ ä¸ç»§æ‰¿ `BasePlanner`
2. âš™ï¸ ä½¿ç”¨ä¼ ç»ŸRLæ¥å£ï¼š`select_action(state_features)`
3. ğŸ”„ éœ€è¦**æ‰‹åŠ¨**ä¸ `LatticePlanner` é›†æˆ
4. ğŸ“¦ æœ‰è‡ªå·±çš„ `ReplayBuffer` å®ç°

---

## è¿è¡Œæµç¨‹å¯¹æ¯”

### ğŸ¯ Planneræ¨¡å¼ç®—æ³•è¿è¡Œæµç¨‹ï¼ˆä»¥PPOä¸ºä¾‹ï¼‰

```python
# 1. åˆ›å»ºPlanner
from c2o_drive.algorithms.ppo import PPOPlanner, PPOConfig

config = PPOConfig(lattice=lattice_config, ...)
planner = PPOPlanner(config)

# 2. Episodeå¾ªç¯
for episode in range(num_episodes):
    state, info = env.reset()
    planner.reset()  # é‡ç½®plannerçŠ¶æ€

    reference_path = info.get('reference_path', [])

    # 3. Stepå¾ªç¯
    while not done:
        # é€‰æ‹©åŠ¨ä½œï¼ˆplannerå†…éƒ¨ç”Ÿæˆè½¨è¿¹å¹¶é€‰æ‹©waypointï¼‰
        control = planner.select_action(
            state,
            deterministic=False,
            reference_path=reference_path
        )

        # æ‰§è¡ŒåŠ¨ä½œ
        step_result = env.step(control)

        # åˆ›å»ºTransition
        transition = Transition(
            state=state,
            action=control,
            reward=step_result.reward,
            next_state=step_result.observation,
            terminated=step_result.terminated,
            truncated=step_result.truncated,
            info=step_result.info,
        )

        # æ›´æ–°plannerï¼ˆå†…éƒ¨å¤„ç†bufferã€è®¡ç®—lossã€æ›´æ–°ç½‘ç»œï¼‰
        metrics = planner.update(transition)

        state = step_result.observation
```

#### PPOå†…éƒ¨æµç¨‹ï¼š

```
select_action() ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼š
  â”œâ”€ ç½‘ç»œè¾“å‡ºactionæ¦‚ç‡åˆ†å¸ƒ
  â”œâ”€ é‡‡æ ·ç¦»æ•£action_idx
  â”œâ”€ ä½¿ç”¨LatticePlannerç”Ÿæˆæ‰€æœ‰å€™é€‰è½¨è¿¹
  â”œâ”€ æ ¹æ®action_idxé€‰æ‹©ä¸€æ¡è½¨è¿¹
  â””â”€ è¿”å›è¯¥è½¨è¿¹çš„ç¬¬ä¸€ä¸ªwaypointå¯¹åº”çš„æ§åˆ¶

select_action() åç»­è°ƒç”¨ï¼š
  â””â”€ ç›´æ¥è¿”å›å½“å‰è½¨è¿¹çš„ä¸‹ä¸€ä¸ªwaypointæ§åˆ¶

update()ï¼š
  â”œâ”€ å­˜å‚¨(state, action, reward, value, log_prob)åˆ°buffer
  â”œâ”€ å½“è½¨è¿¹ç»“æŸæ—¶ï¼š
  â”‚   â”œâ”€ è®¡ç®—GAE advantages
  â”‚   â”œâ”€ æ‰§è¡ŒPPOæ›´æ–°ï¼ˆå¤šä¸ªepochsï¼‰
  â”‚   â””â”€ æ¸…ç©ºbuffer
  â””â”€ è¿”å›UpdateMetrics
```

### ğŸ¤– Agentæ¨¡å¼ç®—æ³•è¿è¡Œæµç¨‹ï¼ˆä»¥SACä¸ºä¾‹ï¼‰

```python
# 1. åˆ›å»ºAgent
from c2o_drive.algorithms.sac import SACAgent, SACConfig

config = SACConfig(...)
agent = SACAgent(config)

# 2. åˆ›å»ºLatticePlannerï¼ˆéœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼ï¼‰
from c2o_drive.utils.lattice_planner import LatticePlanner

lattice_planner = LatticePlanner(
    lateral_offsets=[-3.0, 0.0, 3.0],
    speed_variations=[4.0, 6.0, 8.0],
    num_trajectories=10,
)

# 3. Episodeå¾ªç¯
for episode in range(num_episodes):
    state, info = env.reset()
    reference_path = info.get('reference_path', [])

    # æå–çŠ¶æ€ç‰¹å¾
    state_features = extract_state_features(state)

    # Agentè¾“å‡ºè¿ç»­åŠ¨ä½œ [-1, 1]
    action = agent.select_action(state_features, training=True)

    # Rescaleåˆ°latticeå‚æ•°èŒƒå›´
    lateral_offset = rescale(action[0], range=[-3.0, 3.0])
    target_speed = rescale(action[1], range=[4.0, 8.0])

    # ä½¿ç”¨LatticePlannerç”Ÿæˆè½¨è¿¹ï¼ˆæ‰‹åŠ¨è°ƒç”¨ï¼‰
    trajectory = lattice_planner.generate_single_trajectory(
        reference_path, lateral_offset, target_speed
    )

    # 4. Stepå¾ªç¯ï¼ˆæ‰§è¡Œè½¨è¿¹çš„æ¯ä¸€æ­¥ï¼‰
    for waypoint in trajectory.waypoints:
        control = waypoint_to_control(state, waypoint)
        step_result = env.step(control)

        # æ‰‹åŠ¨å­˜å‚¨åˆ°replay buffer
        agent.replay_buffer.push(
            state_features, action,
            step_result.reward,
            next_state_features,
            done
        )

        # æ‰‹åŠ¨æ›´æ–°agent
        if agent.replay_buffer.size() >= batch_size:
            loss = agent.update()

        state = step_result.observation
```

#### SACä¸Planneræ¨¡å¼çš„å…³é”®åŒºåˆ«ï¼š

| æ–¹é¢ | Planneræ¨¡å¼ (PPO) | Agentæ¨¡å¼ (SAC) |
|------|------------------|-----------------|
| **è½¨è¿¹ç”Ÿæˆ** | âœ… å†…éƒ¨è‡ªåŠ¨å¤„ç† | âŒ éœ€è¦æ‰‹åŠ¨è°ƒç”¨ `LatticePlanner` |
| **Bufferç®¡ç†** | âœ… å†…éƒ¨è‡ªåŠ¨ç®¡ç† | âŒ éœ€è¦æ‰‹åŠ¨push/sample |
| **æ›´æ–°æ—¶æœº** | âœ… è‡ªåŠ¨åˆ¤æ–­ï¼ˆtrajectoryç»“æŸï¼‰ | âŒ éœ€è¦æ‰‹åŠ¨åˆ¤æ–­bufferå¤§å° |
| **çŠ¶æ€ç‰¹å¾** | âœ… å†…éƒ¨æå– | âŒ éœ€è¦æ‰‹åŠ¨æå– |
| **æ¥å£ç»Ÿä¸€æ€§** | âœ… ç»Ÿä¸€ `Transition` | âŒ æ‰‹åŠ¨æ„é€ æ•°æ® |

### ğŸ¯ C2OSRçš„ç‰¹æ®Šæµç¨‹

C2OSRè™½ç„¶ä¹Ÿæ˜¯Planneræ¨¡å¼ï¼Œä½†å®ƒçš„æ‰§è¡Œæµç¨‹ç•¥æœ‰ä¸åŒï¼š

```python
# C2OSRPlannerå†…éƒ¨æµç¨‹
select_action():
  â”œâ”€ ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼š
  â”‚   â”œâ”€ ä½¿ç”¨LatticePlannerç”Ÿæˆæ‰€æœ‰å€™é€‰è½¨è¿¹
  â”‚   â”œâ”€ å¯¹æ¯æ¡è½¨è¿¹è®¡ç®—Qå€¼ï¼ˆä½¿ç”¨Dirichletå…ˆéªŒï¼‰
  â”‚   â”œâ”€ é€‰æ‹©Qå€¼æœ€é«˜çš„è½¨è¿¹
  â”‚   â””â”€ å­˜å‚¨é€‰ä¸­è½¨è¿¹
  â””â”€ åç»­è°ƒç”¨ï¼šè¿”å›å½“å‰è½¨è¿¹çš„ä¸‹ä¸€ä¸ªwaypoint

update():
  â”œâ”€ æ”¶é›†è½¨è¿¹æ‰§è¡Œæ•°æ®
  â”œâ”€ æ›´æ–°DirichletåéªŒï¼ˆè´å¶æ–¯æ›´æ–°ï¼‰
  â””â”€ æ›´æ–°Trajectory Buffer
```

---

## æ–‡ä»¶ç»„ç»‡ç»“æ„

### Planneræ¨¡å¼ç®—æ³•ç»“æ„ï¼ˆä»¥PPOä¸ºä¾‹ï¼‰

```
src/c2o_drive/algorithms/ppo/
â”œâ”€â”€ __init__.py              # å¯¼å‡ºæ¥å£
â”œâ”€â”€ config.py                # PPOConfigé…ç½®ç±»
â”œâ”€â”€ network.py               # ActorCriticNetworkç½‘ç»œ
â”œâ”€â”€ rollout_buffer.py        # PPOä¸“ç”¨buffer
â””â”€â”€ planner.py              # PPOPlannerä¸»ç±»ï¼ˆç»§æ‰¿EpisodicAlgorithmPlannerï¼‰

examples/
â””â”€â”€ run_ppo_carla.py        # è®­ç»ƒè„šæœ¬
```

### Agentæ¨¡å¼ç®—æ³•ç»“æ„ï¼ˆä»¥SACä¸ºä¾‹ï¼‰

```
src/c2o_drive/algorithms/sac/
â”œâ”€â”€ __init__.py              # å¯¼å‡ºæ¥å£
â”œâ”€â”€ config.py                # SACConfigé…ç½®ç±»
â”œâ”€â”€ network.py               # Actorå’ŒCriticç½‘ç»œ
â”œâ”€â”€ replay_buffer.py         # ç»éªŒå›æ”¾buffer
â””â”€â”€ agent.py                # SACAgentä¸»ç±»ï¼ˆç‹¬ç«‹ç±»ï¼Œä¸ç»§æ‰¿Plannerï¼‰

examples/
â””â”€â”€ run_sac_carla.py        # è®­ç»ƒè„šæœ¬ï¼ˆéœ€è¦æ‰‹åŠ¨é›†æˆLatticePlannerï¼‰
```

### C2OSRç®—æ³•ç»“æ„ï¼ˆæœ€å¤æ‚ï¼‰

```
src/c2o_drive/algorithms/c2osr/
â”œâ”€â”€ __init__.py              # å¯¼å‡ºæ¥å£
â”œâ”€â”€ config.py                # å¤šä¸ªConfigç±»ï¼ˆC2OSRPlannerConfig, LatticePlannerConfigç­‰ï¼‰
â”œâ”€â”€ planner.py              # C2OSRPlannerä¸»ç±»
â”œâ”€â”€ factory.py              # åˆ›å»ºplannerçš„å·¥å‚å‡½æ•°
â”œâ”€â”€ dirichlet.py            # DirichletåéªŒæ›´æ–°
â”œâ”€â”€ q_value.py              # Qå€¼è®¡ç®—å™¨
â”œâ”€â”€ trajectory_buffer.py    # è½¨è¿¹buffer
â”œâ”€â”€ grid_mapper.py          # ç½‘æ ¼æ˜ å°„
â”œâ”€â”€ rewards.py              # å¥–åŠ±å‡½æ•°
â””â”€â”€ ... (å…¶ä»–ç»„ä»¶)

examples/
â””â”€â”€ run_c2osr_carla.py      # è®­ç»ƒè„šæœ¬
```

---

## è®­ç»ƒè„šæœ¬çš„ç»Ÿä¸€æ¨¡å¼

æ‰€æœ‰è®­ç»ƒè„šæœ¬éƒ½éµå¾ªç±»ä¼¼çš„ç»“æ„ï¼š

```python
# 1. å¯¼å…¥ç®—æ³•å’Œç¯å¢ƒ
from c2o_drive.algorithms.xxx import XXXPlanner/XXXAgent, XXXConfig
from c2o_drive.environments.carla_env import CarlaEnvironment

# 2. åˆ›å»ºé…ç½®
config = XXXConfig(...)

# 3. åˆ›å»ºç¯å¢ƒ
env = CarlaEnvironment(...)

# 4. åˆ›å»ºPlanner/Agent
planner = XXXPlanner(config)  # æˆ– agent = XXXAgent(config)

# 5. åˆ›å»ºTrainerï¼ˆå°è£…è®­ç»ƒå¾ªç¯ï¼‰
trainer = XXXTrainer(planner, env, ...)

# 6. è¿è¡Œè®­ç»ƒ
trainer.train(num_episodes=1000, max_steps=100)
```

---

## å¦‚ä½•æ·»åŠ æ–°ç®—æ³•ï¼Ÿ

### æ–¹æ³•1ï¼šPlanneræ¨¡å¼ï¼ˆæ¨èï¼‰

```python
from c2o_drive.algorithms.base import EpisodicAlgorithmPlanner
from c2o_drive.core.types import WorldState, EgoControl

class MyPlanner(EpisodicAlgorithmPlanner[WorldState, EgoControl]):
    def __init__(self, config):
        super().__init__(config)
        # åˆå§‹åŒ–ç½‘ç»œã€bufferç­‰
        self.lattice_planner = LatticePlanner(...)

    def select_action(self, observation, deterministic=False, **kwargs):
        # 1. æå–ç‰¹å¾
        # 2. ç½‘ç»œè¾“å‡ºåŠ¨ä½œ
        # 3. ç”Ÿæˆè½¨è¿¹
        # 4. è¿”å›waypointæ§åˆ¶
        pass

    def update(self, transition):
        # 1. å­˜å‚¨æ•°æ®
        # 2. åˆ¤æ–­æ˜¯å¦æ›´æ–°
        # 3. è®¡ç®—loss
        # 4. æ›´æ–°ç½‘ç»œ
        # 5. è¿”å›metrics
        pass

    def reset(self):
        # é‡ç½®å†…éƒ¨çŠ¶æ€
        pass

    def plan_trajectory(self, observation, horizon, **kwargs):
        # å¯é€‰ï¼šç”Ÿæˆå®Œæ•´è½¨è¿¹
        pass
```

### æ–¹æ³•2ï¼šAgentæ¨¡å¼ï¼ˆä¼ ç»ŸRLï¼‰

```python
class MyAgent:
    def __init__(self, config):
        self.network = ...
        self.replay_buffer = ...

    def select_action(self, state_features, training=True):
        # è¿”å›åŠ¨ä½œ
        pass

    def update(self):
        # ä»bufferé‡‡æ ·å¹¶æ›´æ–°
        pass
```

ç„¶åéœ€è¦åœ¨è®­ç»ƒè„šæœ¬ä¸­æ‰‹åŠ¨é›†æˆLatticePlannerã€‚

---

## æ€»ç»“

### ğŸ¯ Planneræ¨¡å¼çš„ä¼˜åŠ¿

1. âœ… **æ¥å£ç»Ÿä¸€**ï¼šæ‰€æœ‰ç®—æ³•éƒ½æ˜¯ `BasePlanner` å­ç±»
2. âœ… **è½¨è¿¹è‡ªåŠ¨ç®¡ç†**ï¼šå†…éƒ¨å¤„ç†è½¨è¿¹ç”Ÿæˆå’Œæ‰§è¡Œ
3. âœ… **çŠ¶æ€è½¬æ¢å°è£…**ï¼šç»Ÿä¸€ä½¿ç”¨ `Transition`
4. âœ… **æ˜“äºæ›¿æ¢**ï¼šå¯ä»¥æ— ç¼åˆ‡æ¢ä¸åŒç®—æ³•
5. âœ… **è®­ç»ƒè„šæœ¬ç®€æ´**ï¼šä¸»å¾ªç¯ä»£ç é«˜åº¦ä¸€è‡´

### ğŸ¤– Agentæ¨¡å¼çš„ç‰¹ç‚¹

1. ğŸ’¡ **çµæ´»æ€§é«˜**ï¼šä¸å—Planneræ¥å£çº¦æŸ
2. âš™ï¸ **æ‰‹åŠ¨æ§åˆ¶**ï¼šéœ€è¦æ‰‹åŠ¨ç®¡ç†è½¨è¿¹ã€bufferã€æ›´æ–°
3. ğŸ“¦ **ä¼ ç»ŸRLé£æ ¼**ï¼šç¬¦åˆç»å…¸RLä»£ç ä¹ æƒ¯
4. ğŸ”§ **é›†æˆæˆæœ¬é«˜**ï¼šéœ€è¦æ›´å¤šèƒ¶æ°´ä»£ç 

### ğŸ’¡ å»ºè®®

- **æ–°ç®—æ³•ä¼˜å…ˆä½¿ç”¨Planneræ¨¡å¼**ï¼ˆå¦‚PPOã€Rainbow DQNã€RCRLï¼‰
- SACå’ŒDQNä½¿ç”¨Agentæ¨¡å¼å¯èƒ½æ˜¯å†å²é—ç•™ï¼Œå¯ä»¥è€ƒè™‘é‡æ„ä¸ºPlanneræ¨¡å¼
- ç»Ÿä¸€åˆ°Planneræ¨¡å¼å¯ä»¥ç®€åŒ–ä»£ç ç»´æŠ¤å’Œç®—æ³•å¯¹æ¯”

---

## å¿«é€Ÿå‚è€ƒ

### å½“å‰ç®—æ³•æ¸…å•

| ç®—æ³• | æ¨¡å¼ | æ–‡ä»¶è·¯å¾„ | è®­ç»ƒè„šæœ¬ | çŠ¶æ€ |
|------|------|----------|----------|------|
| C2OSR | Planner | `algorithms/c2osr/planner.py` | `run_c2osr_carla.py` | âœ… å®Œæ•´ |
| PPO | Planner | `algorithms/ppo/planner.py` | `run_ppo_carla.py` | âœ… å®Œæ•´ |
| Rainbow DQN | Planner | `algorithms/rainbow_dqn/planner.py` | - | âš ï¸ éœ€è¦è®­ç»ƒè„šæœ¬ |
| RCRL | Planner | `algorithms/rcrl/planner.py` | `test_rcrl.py` | âš ï¸ éœ€è¦å®Œæ•´è®­ç»ƒè„šæœ¬ |
| SAC | Agent | `algorithms/sac/agent.py` | `run_sac_carla.py` | âœ… å®Œæ•´ |
| DQN | Agent | `algorithms/dqn/agent.py` | - | âš ï¸ éœ€è¦è®­ç»ƒè„šæœ¬ |

### è¿è¡Œç¤ºä¾‹

```bash
# C2OSR
python examples/run_c2osr_carla.py --scenario s4_wrong_way --episodes 100

# PPO
python examples/run_ppo_carla.py --scenario s4_wrong_way --episodes 1000

# SAC
python examples/run_sac_carla.py --scenario s4_wrong_way --episodes 1000

# RCRLï¼ˆæµ‹è¯•è„šæœ¬ï¼‰
python examples/test_rcrl.py
```
