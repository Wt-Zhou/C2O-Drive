"""
Qå€¼è®¡ç®—è¾…åŠ©æ¨¡å—

ä» run_episode ä¸­æå–çš„Qå€¼è®¡ç®—é€»è¾‘,èŒè´£å•ä¸€,æ˜“äºæµ‹è¯•ã€‚
"""

from typing import Dict, List, Tuple, Optional
import numpy as np

from carla_c2osr.env.types import WorldState
from carla_c2osr.evaluation.q_value_calculator import QValueCalculator, QValueConfig
from carla_c2osr.config import get_global_config
from carla_c2osr.runner.episode_context import EpisodeContext


def compute_q_value_for_episode(
    ctx: EpisodeContext,
    world_state: WorldState,
    timestep: int
) -> Optional[Dict]:
    """è®¡ç®—episodeçš„Qå€¼(ä»…åœ¨timestep=0æ—¶æ‰§è¡Œ)

    Args:
        ctx: Episodeè¿è¡Œä¸Šä¸‹æ–‡
        world_state: å½“å‰ä¸–ç•ŒçŠ¶æ€
        timestep: å½“å‰æ—¶é—´æ­¥

    Returns:
        Qå€¼è®¡ç®—ç»“æœå­—å…¸,å¦‚æœä¸éœ€è¦è®¡ç®—åˆ™è¿”å›None
        {
            'q_values': List[float],
            'avg_q_value': float,
            'detailed_info': Dict
        }
    """
    # åªåœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥è®¡ç®—Qå€¼
    if timestep != 0:
        return None

    # æ„é€ è‡ªè½¦æœªæ¥åŠ¨ä½œè½¨è¿¹
    ego_action_trajectory = _extract_ego_action_trajectory(ctx, timestep)

    # è·å–verboseçº§åˆ«
    from carla_c2osr.config import get_global_config
    verbose = get_global_config().visualization.verbose_level

    if verbose >= 2:
        print(f"    å¼€å§‹Qå€¼è®¡ç®—: è‡ªè½¦åŠ¨ä½œåºåˆ—é•¿åº¦={len(ego_action_trajectory)}")

    try:
        # åˆ›å»ºQå€¼é…ç½®å’Œè®¡ç®—å™¨
        q_config = QValueConfig.from_global_config()
        global_config = get_global_config()
        reward_config = global_config.reward

        q_calculator = QValueCalculator(q_config, reward_config)

        # è®¡ç®—Qå€¼
        q_values, detailed_info = q_calculator.compute_q_value(
            current_world_state=world_state,
            ego_action_trajectory=ego_action_trajectory,
            trajectory_buffer=ctx.trajectory_buffer,
            grid=ctx.grid,
            bank=ctx.bank,
            rng=ctx.rng
        )

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_q_value = np.mean(q_values)

        # æ‰“å°ç»“æœ
        _print_q_value_results(q_values, avg_q_value, detailed_info)

        # è®°å½•åˆ°tracker
        if ctx.q_tracker is not None:
            _record_q_value_to_tracker(ctx, avg_q_value, q_values, detailed_info)

        # ç”Ÿæˆå¯è§†åŒ–(æ¯5ä¸ªepisode)
        if ctx.should_visualize():
            _generate_q_value_visualizations(
                ctx, world_state, ego_action_trajectory,
                q_calculator, timestep
            )

        return {
            'q_values': q_values,
            'avg_q_value': avg_q_value,
            'detailed_info': detailed_info
        }

    except Exception as e:
        print(f"    Qå€¼è®¡ç®—å¤±è´¥: {e}")
        _record_q_value_failure(ctx, e)
        return None


def _extract_ego_action_trajectory(ctx: EpisodeContext, timestep: int) -> List[Tuple[float, float]]:
    """æå–è‡ªè½¦åŠ¨ä½œè½¨è¿¹"""
    ego_action_trajectory = []
    for action_t in range(timestep, min(timestep + ctx.horizon, len(ctx.ego_trajectory))):
        ego_action_trajectory.append(tuple(ctx.ego_trajectory[action_t]))
    return ego_action_trajectory


def _print_q_value_results(q_values: List[float], avg_q_value: float, detailed_info: Dict):
    """æ‰“å°Qå€¼è®¡ç®—ç»“æœ"""
    from carla_c2osr.config import get_global_config
    verbose = get_global_config().visualization.verbose_level

    # å§‹ç»ˆæ‰“å°æ‘˜è¦(å•è¡Œ)
    min_q = min(q_values) if q_values else 0.0
    collision_rate = detailed_info['reward_breakdown']['collision_rate']
    print(f"    Q: Avg={avg_q_value:.2f}, Min={min_q:.2f}, Collision={collision_rate:.1%}")

    # è¯¦ç»†ä¿¡æ¯(ä»…verbose >= 2)
    if verbose >= 2:
        print(f"    æ‰€æœ‰Qå€¼: {[f'{q:.2f}' for q in q_values]}")
        print(f"    Qå€¼æ ‡å‡†å·®: {detailed_info['reward_breakdown']['q_value_std']:.2f}")

        # æ‰“å°æ¯ä¸ªæ™ºèƒ½ä½“çš„ä¿¡æ¯
        for agent_id, agent_info in detailed_info.get('agent_info', {}).items():
            reachable_total = agent_info.get('reachable_cells_total', 0)
            historical_total = agent_info.get('historical_data_count', 0)
            print(f"    Agent {agent_id}: å¯è¾¾é›†(æ€»è®¡)={reachable_total}, å†å²æ•°æ®={historical_total}")


def _record_q_value_to_tracker(
    ctx: EpisodeContext,
    avg_q_value: float,
    q_values: List[float],
    detailed_info: Dict
):
    """è®°å½•Qå€¼åˆ°tracker"""
    q_distribution = detailed_info['reward_breakdown']['all_q_values']
    collision_rate = detailed_info['reward_breakdown']['collision_rate']

    ctx.q_tracker.add_episode_data(
        episode_id=ctx.episode_id,
        q_value=avg_q_value,
        q_distribution=q_distribution,
        collision_rate=collision_rate,
        detailed_info=detailed_info
    )


def _record_q_value_failure(ctx: EpisodeContext, error: Exception):
    """è®°å½•Qå€¼è®¡ç®—å¤±è´¥"""
    if ctx.q_tracker is None:
        return

    config = get_global_config()
    n_samples = config.sampling.q_value_samples

    ctx.q_tracker.add_episode_data(
        episode_id=ctx.episode_id,
        q_value=0.0,
        q_distribution=[0.0] * n_samples,
        collision_rate=0.0,
        detailed_info={'error': str(error)}
    )


def _generate_q_value_visualizations(
    ctx: EpisodeContext,
    world_state: WorldState,
    ego_action_trajectory: List[Tuple[float, float]],
    q_calculator: QValueCalculator,
    timestep: int
):
    """ç”Ÿæˆtransitionå’ŒDirichletåˆ†å¸ƒå¯è§†åŒ–"""
    try:
        from carla_c2osr.visualization.transition_visualizer import (
            visualize_transition_distributions,
            visualize_dirichlet_distributions
        )

        from carla_c2osr.config import get_global_config
        verbose = get_global_config().visualization.verbose_level
        if verbose >= 1:
            print(f"  ğŸ¨ ç”Ÿæˆtransitionåˆ†å¸ƒå’ŒDirichletåˆ†å¸ƒå¯è§†åŒ–...")

        # è·å–transitionåˆ†å¸ƒæ•°æ®
        agent_transition_samples = q_calculator._build_agent_transition_distributions(
            world_state, ego_action_trajectory, ctx.trajectory_buffer,
            ctx.grid, ctx.bank, ctx.horizon
        )

        # å¯è§†åŒ–transitionåˆ†å¸ƒ
        visualize_transition_distributions(
            agent_transition_samples=agent_transition_samples,
            current_world_state=world_state,
            grid=ctx.grid,
            episode_idx=ctx.episode_id,
            output_dir=ctx.output_dir
        )

        # å¯è§†åŒ–Dirichletåˆ†å¸ƒ
        visualize_dirichlet_distributions(
            bank=ctx.bank,
            current_world_state=world_state,
            grid=ctx.grid,
            episode_idx=ctx.episode_id,
            output_dir=ctx.output_dir
        )

    except Exception as e:
        print(f"  âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
