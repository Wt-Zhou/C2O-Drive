#!/usr/bin/env python3
"""
åŸºäºLatticeè§„åˆ’å™¨çš„å¤šæ¬¡åœºæ™¯æ‰§è¡Œï¼ˆQå€¼ä¼˜åŒ–ç‰ˆï¼‰

æ¼”ç¤ºlatticeè½¨è¿¹è§„åˆ’ä¸Qå€¼è¯„ä¼°ç»“åˆçš„è´å¶æ–¯å­¦ä¹ è¿‡ç¨‹ï¼š
- æ¯ä¸ªepisodeä½¿ç”¨lattice plannerç”Ÿæˆå€™é€‰è½¨è¿¹
- ä¸ºæ¯æ¡å€™é€‰è½¨è¿¹è®¡ç®—Qå€¼
- ä½¿ç”¨min-maxå‡†åˆ™é€‰æ‹©æœ€ä¼˜è½¨è¿¹ï¼ˆæœ€å¤§åŒ–æœ€å°Qå€¼ï¼‰
- è·Ÿè¸ªQå€¼éšepisodeçš„æ”¹è¿›æƒ…å†µ
"""

from __future__ import annotations
import argparse
import sys
from pathlib import Path
import numpy as np
import yaml
from typing import Any, List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
_repo_root = Path(__file__).resolve().parents[2]
if str(_repo_root) not in sys.path:
    sys.path.insert(0, str(_repo_root))

from carla_c2osr.env.types import AgentState, EgoState, WorldState, AgentType
from carla_c2osr.agents.c2osr.grid import GridSpec, GridMapper
from carla_c2osr.agents.c2osr.spatial_dirichlet import DirichletParams, SpatialDirichletBank, MultiTimestepSpatialDirichletBank, OptimizedMultiTimestepSpatialDirichletBank
from carla_c2osr.agents.c2osr.trajectory_buffer import TrajectoryBuffer, AgentTrajectoryData, ScenarioState
from carla_c2osr.agents.c2osr.risk import compose_union_singlelayer
from carla_c2osr.visualization.vis import grid_heatmap, make_gif
from carla_c2osr.visualization.transition_visualizer import visualize_transition_distributions, visualize_dirichlet_distributions
from carla_c2osr.visualization.lattice_visualizer import visualize_lattice_selection, visualize_lattice_trajectories_detailed

# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from carla_c2osr.evaluation.rewards import RewardCalculator, DistanceBasedCollisionDetector
from carla_c2osr.evaluation.q_evaluator import QEvaluator
from carla_c2osr.evaluation.buffer_analyzer import BufferAnalyzer
from carla_c2osr.evaluation.q_value_calculator import QValueCalculator, QValueConfig
from carla_c2osr.evaluation.q_distribution_tracker import QDistributionTracker
from carla_c2osr.utils.simple_trajectory_generator import SimpleTrajectoryGenerator
from carla_c2osr.utils.lattice_planner import LatticePlanner
from carla_c2osr.utils.checkpoint_manager import CheckpointManager
from carla_c2osr.env.scenario_manager import ScenarioManager
from carla_c2osr.config import get_global_config, update_dt, update_horizon, get_dt, get_horizon


def setup_output_dirs(base_dir: str = "outputs/replay_experiment") -> Path:
    """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„ã€‚"""
    base_path = Path(base_dir)
    base_path.mkdir(parents=True, exist_ok=True)
    return base_path


def get_percentile_q_value(q_values: np.ndarray, percentile: float) -> float:
    """
    æ ¹æ®ç™¾åˆ†ä½æ•°è·å–Qå€¼

    Args:
        q_values: Qå€¼æ•°ç»„
        percentile: ç™¾åˆ†ä½æ•° [0.0, 1.0]ï¼Œ0.0è¡¨ç¤ºæœ€å°å€¼ï¼Œ1.0è¡¨ç¤ºæœ€å¤§å€¼

    Returns:
        ç™¾åˆ†ä½å¯¹åº”çš„Qå€¼ï¼ˆä½¿ç”¨çº¿æ€§æ’å€¼ï¼‰

    Examples:
        >>> q = np.array([1, 2, 3, 4, 5])
        >>> get_percentile_q_value(q, 0.0)  # æœ€å°å€¼
        1.0
        >>> get_percentile_q_value(q, 0.5)  # ä¸­ä½æ•°
        3.0
        >>> get_percentile_q_value(q, 0.25)  # 25%åˆ†ä½
        2.0
    """
    if len(q_values) == 0:
        return 0.0

    if len(q_values) == 1:
        return float(q_values[0])

    # æ’åºQå€¼
    sorted_q = np.sort(q_values)
    n = len(sorted_q)

    # è®¡ç®—ç²¾ç¡®ä½ç½®ï¼ˆ0-based indexï¼‰
    position = percentile * (n - 1)

    # ä¸‹ç•Œå’Œä¸Šç•Œç´¢å¼•
    lower_idx = int(np.floor(position))
    upper_idx = int(np.ceil(position))

    # å¦‚æœæ­£å¥½æ˜¯æ•´æ•°ä½ç½®
    if lower_idx == upper_idx:
        return float(sorted_q[lower_idx])

    # çº¿æ€§æ’å€¼
    weight = position - lower_idx
    return float(sorted_q[lower_idx] * (1 - weight) + sorted_q[upper_idx] * weight)


def run_episode(episode_id: int,
                horizon: int,
                reference_path: List[np.ndarray],
                world_init: WorldState,
                grid: GridMapper,
                bank: SpatialDirichletBank,
                trajectory_buffer: TrajectoryBuffer,
                scenario_state: ScenarioState,
                rng: np.random.Generator,
                output_dir: Path,
                sigma: float,
                lattice_planner: LatticePlanner = None,
                q_evaluator: QEvaluator = None,
                trajectory_generator: TrajectoryGenerator = None,
                scenario_manager: ScenarioManager = None,
                buffer_analyzer: BufferAnalyzer = None,
                q_tracker: QDistributionTracker = None) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªepisode - ä½¿ç”¨latticeè§„åˆ’å™¨é€‰æ‹©æœ€ä¼˜è½¨è¿¹ã€‚"""

    # åˆå§‹åŒ–ç»„ä»¶
    if q_evaluator is None:
        q_evaluator = QEvaluator()
    if scenario_manager is None:
        scenario_manager = ScenarioManager()
    if buffer_analyzer is None:
        buffer_analyzer = BufferAnalyzer(trajectory_buffer)
    if lattice_planner is None:
        lattice_planner = LatticePlanner.from_config(get_global_config())

    # åˆ›å»ºepisodeè¾“å‡ºç›®å½•
    ep_dir = output_dir / f"ep_{episode_id:02d}"
    ep_dir.mkdir(exist_ok=True)

    # ===== ç¬¬1æ­¥ï¼šç”Ÿæˆå€™é€‰è½¨è¿¹ =====
    config = get_global_config()

    # ä»world_initè·å–è‡ªè½¦å½“å‰çŠ¶æ€
    ego_state = (
        world_init.ego.position_m[0],
        world_init.ego.position_m[1],
        world_init.ego.yaw_rad
    )

    candidate_trajectories = lattice_planner.generate_trajectories(
        reference_path=reference_path,
        horizon=horizon,
        dt=config.time.dt,
        ego_state=ego_state  # ä¼ å…¥è‡ªè½¦çŠ¶æ€
    )
    print(f"  ç”Ÿæˆ {len(candidate_trajectories)} æ¡å€™é€‰è½¨è¿¹")

    # ===== ç¬¬2æ­¥ï¼šä¸ºæ¯æ¡å€™é€‰è½¨è¿¹è®¡ç®—Qå€¼ =====
    trajectory_q_values = []

    for traj in candidate_trajectories:
        # æ„é€ è‡ªè½¦åŠ¨ä½œè½¨è¿¹
        ego_action_trajectory = traj.waypoints

        # åˆ›å»ºQå€¼é…ç½®å’Œè®¡ç®—å™¨
        q_config = QValueConfig.from_global_config()
        reward_config = config.reward
        q_calculator = QValueCalculator(q_config, reward_config)

        # è®¡ç®—Qå€¼ï¼ˆä¼ å…¥reference_pathç”¨äºä¸­å¿ƒçº¿åç§»æƒ©ç½šï¼‰
        try:
            q_values, detailed_info = q_calculator.compute_q_value(
                current_world_state=world_init,
                ego_action_trajectory=ego_action_trajectory,
                trajectory_buffer=trajectory_buffer,
                grid=grid,
                bank=bank,
                rng=rng,
                reference_path=reference_path
            )

            # è®¡ç®—Qå€¼ç»Ÿè®¡æŒ‡æ ‡
            min_q = np.min(q_values)
            mean_q = np.mean(q_values)
            percentile_q = get_percentile_q_value(q_values, q_config.q_selection_percentile)

            trajectory_q_values.append({
                'trajectory_id': traj.trajectory_id,
                'lateral_offset': traj.lateral_offset,
                'target_speed': traj.target_speed,
                'min_q': min_q,
                'mean_q': mean_q,
                'percentile_q': percentile_q,
                'q_values': q_values,
                'collision_rate': detailed_info['reward_breakdown']['collision_rate'],
                'trajectory': traj.waypoints
            })

        except Exception as e:
            print(f"  è­¦å‘Š: è½¨è¿¹{traj.trajectory_id}è®¡ç®—å¤±è´¥: {e}")
            continue

    # ===== ç¬¬3æ­¥ï¼šä½¿ç”¨ç™¾åˆ†ä½Qå€¼é€‰æ‹©æœ€ä¼˜è½¨è¿¹ =====
    if not trajectory_q_values:
        print(f"  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆè½¨è¿¹ï¼Œä½¿ç”¨reference path")
        ego_trajectory = reference_path
        selected_trajectory_info = None
    else:
        # é€‰æ‹©percentile_qæœ€å¤§çš„è½¨è¿¹
        best_trajectory = max(trajectory_q_values, key=lambda x: x['percentile_q'])
        ego_trajectory = best_trajectory['trajectory']
        selected_trajectory_info = best_trajectory

        # è·å–ç™¾åˆ†ä½æ•°é…ç½®ç”¨äºæ—¥å¿—
        q_config = QValueConfig.from_global_config()
        percentile = q_config.q_selection_percentile

        print(f"  é€‰ä¸­è½¨è¿¹{best_trajectory['trajectory_id']}: "
              f"åç§»={best_trajectory['lateral_offset']:.1f}m, "
              f"é€Ÿåº¦={best_trajectory['target_speed']:.1f}m/s")
        print(f"    Min_Q={best_trajectory['min_q']:.2f}, "
              f"Mean_Q={best_trajectory['mean_q']:.2f}, "
              f"P{int(percentile*100)}_Q={best_trajectory['percentile_q']:.2f}, "
              f"ç¢°æ’ç‡={best_trajectory['collision_rate']:.3f}")

        # ===== å¯è§†åŒ–latticeè½¨è¿¹é€‰æ‹© =====
        try:
            # ä¸»è¦å¯è§†åŒ–ï¼šè½¨è¿¹ + Qå€¼æŸ±çŠ¶å›¾
            visualize_lattice_selection(
                trajectory_q_values=trajectory_q_values,
                selected_trajectory_info=selected_trajectory_info,
                current_world_state=world_init,
                grid=grid,
                episode_idx=episode_id,
                output_dir=ep_dir
            )

            # è¯¦ç»†å¯è§†åŒ–ï¼šQå€¼åˆ†å¸ƒç®±çº¿å›¾
            visualize_lattice_trajectories_detailed(
                trajectory_q_values=trajectory_q_values,
                selected_trajectory_info=selected_trajectory_info,
                current_world_state=world_init,
                grid=grid,
                episode_idx=episode_id,
                output_dir=ep_dir
            )

        except Exception as e:
            print(f"  è­¦å‘Š: Latticeå¯è§†åŒ–å¤±è´¥: {e}")

    # ä¸ºæ¯ä¸ªç¯å¢ƒæ™ºèƒ½ä½“ç”Ÿæˆå›ºå®šçš„åŠ¨åŠ›å­¦è½¨è¿¹
    agent_trajectories = {}
    agent_trajectory_cells = {}  # ç”¨äºå­˜å‚¨åˆ°bufferçš„è½¨è¿¹å•å…ƒID

    # è½¨è¿¹ç”Ÿæˆéšæœºæºï¼šä»é…ç½®è¯»å–
    # æ³¨æ„ï¼šè½¨è¿¹æ¨¡å¼ç°åœ¨å¯ä»¥é€šè¿‡ config.agent_trajectory.mode è®¾ç½® ("dynamic", "straight", "stationary")
    rng = np.random.default_rng(config.agent_trajectory.random_seed)

    for i, agent in enumerate(world_init.agents):
        agent_id = i + 1
        try:
            # ç”Ÿæˆç¬¦åˆåŠ¨åŠ›å­¦çº¦æŸçš„è½¨è¿¹
            trajectory = trajectory_generator.generate_agent_trajectory(agent, horizon)
            agent_trajectories[agent_id] = trajectory

            # å°†è½¨è¿¹è½¬æ¢ä¸ºç½‘æ ¼å•å…ƒID
            trajectory_cells = []
            for pos in trajectory:
                cell_id = grid.world_to_cell(tuple(pos))
                trajectory_cells.append(cell_id)
            agent_trajectory_cells[agent_id] = trajectory_cells

        except Exception as e:
            print(f"  è­¦å‘Š: Agent {agent_id} è½¨è¿¹ç”Ÿæˆå¤±è´¥: {e}")
            # ä½¿ç”¨ç®€å•çš„ç›´çº¿è½¨è¿¹ä½œä¸ºåå¤‡
            fallback_trajectory = []
            fallback_cells = []
            start_pos = np.array(agent.position_m)
            for t in range(horizon):
                next_pos = start_pos + np.array([0.5 * t, 0.1 * t])  # ç®€å•ç§»åŠ¨
                grid_half_size = grid.size_m / 2.0
                next_pos = np.clip(next_pos, -grid_half_size, grid_half_size)
                fallback_trajectory.append(next_pos)
                fallback_cells.append(grid.world_to_cell(tuple(next_pos)))
            agent_trajectories[agent_id] = fallback_trajectory
            agent_trajectory_cells[agent_id] = fallback_cells
    
    # é€æ—¶åˆ»æ‰§è¡Œå’Œå¯è§†åŒ–
    frame_paths = []
    episode_stats = []
    
    for t in range(horizon):
        # åˆ›å»ºå½“å‰ä¸–ç•ŒçŠ¶æ€
        world_current = scenario_manager.create_world_state_from_trajectories(
            t, ego_trajectory, agent_trajectories, world_init
        )
        
        # åŸºäºå½“å‰æ—¶åˆ»çŠ¶æ€åˆ›å»ºScenarioStateï¼ˆç”¨äºæŸ¥è¯¢å†å²æ•°æ®ï¼‰
        current_scenario_state = scenario_manager.create_scenario_state(world_current)
        
        # è®¡ç®—æ¯ä¸ªæ™ºèƒ½ä½“å½“å‰ä½ç½®çš„å¤šæ—¶é—´æ­¥å¯è¾¾é›†
        config = get_global_config()
        current_reachable = {}
        multi_timestep_reachable = {}
        for i, agent in enumerate(world_current.agents):
            agent_id = i + 1
            # è®¡ç®—å•æ—¶é—´æ­¥å¯è¾¾é›†ï¼ˆå‘åå…¼å®¹ï¼‰
            reachable = grid.successor_cells(agent, n_samples=config.sampling.reachable_set_samples_legacy)
            current_reachable[agent_id] = reachable
            # è®¡ç®—å¤šæ—¶é—´æ­¥å¯è¾¾é›†
            multi_reachable = grid.multi_timestep_successor_cells(
                agent, 
                horizon=horizon, 
                dt=config.time.dt, 
                n_samples=config.sampling.reachable_set_samples
            )
            multi_timestep_reachable[agent_id] = multi_reachable
        
        # ä½¿ç”¨æ–°çš„Qå€¼è®¡ç®—å™¨è¿›è¡ŒQå€¼è¯„ä¼°
        if t == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥è®¡ç®—Qå€¼
            # æ„é€ è‡ªè½¦æœªæ¥åŠ¨ä½œè½¨è¿¹
            ego_action_trajectory = []
            for action_t in range(t, min(t + horizon, len(ego_trajectory))):
                ego_action_trajectory.append(tuple(ego_trajectory[action_t]))

            try:
                # åˆ›å»ºQå€¼é…ç½®
                q_config = QValueConfig.from_global_config()

                # ç›´æ¥ä½¿ç”¨å…¨å±€é…ç½®ä¸­çš„å¥–åŠ±é…ç½®
                global_config = get_global_config()
                reward_config = global_config.reward

                # åˆ›å»ºQå€¼è®¡ç®—å™¨
                q_calculator = QValueCalculator(q_config, reward_config)

                # è®¡ç®—Qå€¼ï¼ˆä¼ å…¥æŒä¹…çš„Dirichlet Bankå’Œreference_pathï¼‰
                q_values, detailed_info = q_calculator.compute_q_value(
                    current_world_state=world_current,
                    ego_action_trajectory=ego_action_trajectory,
                    trajectory_buffer=trajectory_buffer,
                    grid=grid,
                    bank=bank,  # ä¼ å…¥æŒä¹…çš„Bankï¼Œç¡®ä¿å­¦ä¹ ç´¯ç§¯
                    rng=rng,
                    reference_path=reference_path
                )

                # è®¡ç®—å¹³å‡Qå€¼ç”¨äºæ˜¾ç¤º
                avg_q_value = np.mean(q_values)

                # è®°å½•Qå€¼åˆ†å¸ƒæ•°æ®
                if q_tracker is not None:
                    q_distribution = detailed_info['reward_breakdown']['all_q_values']
                    collision_rate = detailed_info['reward_breakdown']['collision_rate']
                    q_tracker.add_episode_data(
                        episode_id=episode_id,
                        q_value=avg_q_value,
                        q_distribution=q_distribution,
                        collision_rate=collision_rate,
                        detailed_info=detailed_info
                    )

                # ç”Ÿæˆtransitionåˆ†å¸ƒå’ŒDirichletåˆ†å¸ƒå¯è§†åŒ–ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªæ—¶åˆ»ï¼‰
                if t == 0 and episode_id % 5 == 0:  # æ¯5ä¸ªepisodeç”Ÿæˆä¸€æ¬¡å¯è§†åŒ–
                    try:
                        print(f"  ç”Ÿæˆåˆ†å¸ƒå¯è§†åŒ–...")
                        # è·å–transitionåˆ†å¸ƒæ•°æ®ï¼ˆä»Qå€¼è®¡ç®—å™¨å†…éƒ¨è·å–ï¼‰
                        agent_transition_samples = q_calculator._build_agent_transition_distributions(
                            world_current, ego_action_trajectory, trajectory_buffer, grid, bank, horizon
                        )

                        # å¯è§†åŒ–transitionåˆ†å¸ƒ
                        visualize_transition_distributions(
                            agent_transition_samples=agent_transition_samples,
                            current_world_state=world_current,
                            grid=grid,
                            episode_idx=episode_id,
                            output_dir=output_dir
                        )

                        # å¯è§†åŒ–Dirichletåˆ†å¸ƒ
                        visualize_dirichlet_distributions(
                            bank=bank,
                            current_world_state=world_current,
                            grid=grid,
                            episode_idx=episode_id,
                            output_dir=output_dir
                        )
                        print(f"  åˆ†å¸ƒå¯è§†åŒ–å®Œæˆ")

                    except Exception as e:
                        print(f"  è­¦å‘Š: å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")

            except Exception as e:
                print(f"  è­¦å‘Š: Qå€¼è®¡ç®—å¤±è´¥: {e}")
                # å³ä½¿Qå€¼è®¡ç®—å¤±è´¥ï¼Œä¹Ÿè¦è®°å½•å¤±è´¥ä¿¡æ¯
                if q_tracker is not None:
                    # åˆ›å»ºä¸æˆåŠŸæƒ…å†µç›¸åŒé•¿åº¦çš„é›¶åˆ†å¸ƒ
                    config = get_global_config()
                    n_samples = config.sampling.q_value_samples
                    q_tracker.add_episode_data(
                        episode_id=episode_id,
                        q_value=0.0,
                        q_distribution=[0.0] * n_samples,  # ä¿æŒä¸€è‡´çš„é•¿åº¦
                        collision_rate=0.0,
                        detailed_info={'error': str(e)}
                    )
        
        # ä¸ºå¯è§†åŒ–åˆå§‹åŒ–MultiTimestepSpatialDirichletBank
        for i, agent in enumerate(world_current.agents):
            agent_id = i + 1
            try:
                # è®¡ç®—å¤šæ—¶é—´æ­¥å¯è¾¾é›†ç”¨äºåˆå§‹åŒ–
                agent_multi_reachable = grid.multi_timestep_successor_cells(
                    agent,
                    horizon=len(ego_action_trajectory),
                    dt=config.time.dt,
                    n_samples=config.sampling.reachable_set_samples
                )
                if agent_multi_reachable and agent_id not in bank.agent_alphas:
                    bank.init_agent(agent_id, agent_multi_reachable)
            except Exception as e:
                print(f"  è­¦å‘Š: Agent {agent_id} åˆå§‹åŒ–å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—å½“å‰"è®¡æ•°å›¾"æˆ–æ¦‚ç‡å›¾ç”¨äºå¯è§†åŒ–
        # è¿™é‡Œæ ¹æ®vis_modeé€‰æ‹©ï¼š
        # - qmax / pmean-*: æ˜¾ç¤ºæ¦‚ç‡
        # - counts-agent1/2/avg: æ˜¾ç¤ºè®¡æ•°ï¼ˆalpha - alpha_initï¼‰å½’ä¸€åŒ–åˆ°[0,1]
        # ç®€åŒ–çš„ç»Ÿä¸€å¯è§†åŒ–æ¨¡å¼ï¼šæ˜¾ç¤ºagentå¯è¾¾é›†ã€å†å²è½¨è¿¹å’Œè‡ªè½¦æœªæ¥è½¨è¿¹
        # 1. æ„é€ è‡ªè½¦æœªæ¥åŠ¨ä½œè½¨è¿¹
        ego_action_trajectory = []
        for action_t in range(t, min(t + horizon, len(ego_trajectory))):
            ego_action_trajectory.append(tuple(ego_trajectory[action_t]))
        
        # 2. è·å–å†å²è½¨è¿¹æ•°æ®
        current_ego_state = (world_current.ego.position_m[0], world_current.ego.position_m[1], world_current.ego.yaw_rad)
        current_agents_states = []
        for agent in world_current.agents:
            current_agents_states.append((agent.position_m[0], agent.position_m[1], 
                                        agent.velocity_mps[0], agent.velocity_mps[1], 
                                        agent.heading_rad, agent.agent_type.value))
        
        # 3. åˆå§‹åŒ–å¯è§†åŒ–æ•°æ®
        c = np.zeros(grid.spec.num_cells)

        # 4. å¤„ç†æ¯ä¸ªAgentï¼ˆä¸Qå€¼è®¡ç®—å®Œå…¨å¯¹é½çš„å¯è§†åŒ–ï¼‰
        config = get_global_config()
        multi_timestep_reachable = {}  # æ”¶é›†æ‰€æœ‰agentçš„å¤šæ—¶é—´æ­¥å¯è¾¾é›†
        historical_data_sets = {}  # æ”¶é›†æ‰€æœ‰agentçš„å†å²è½¨è¿¹æ•°æ®

        for i, agent in enumerate(world_current.agents):
            agent_id = i + 1

            # 4a. è®¡ç®—Agentçš„å¤šæ—¶é—´æ­¥å¯è¾¾é›†ï¼ˆä¸Qå€¼è®¡ç®—ä½¿ç”¨ç›¸åŒå‚æ•°ï¼‰
            agent_multi_reachable = grid.multi_timestep_successor_cells(
                agent,
                horizon=len(ego_action_trajectory),
                dt=config.time.dt,
                n_samples=config.sampling.reachable_set_samples
            )

            if not agent_multi_reachable:
                continue

            # ä¿å­˜åˆ°å¯è§†åŒ–æ•°æ®ç»“æ„
            multi_timestep_reachable[agent_id] = agent_multi_reachable

            # 4b. å°†å¤šæ—¶é—´æ­¥å¯è¾¾é›†æ·»åŠ åˆ°å¯è§†åŒ–ï¼ˆæŒ‰æ—¶é—´æ­¥åˆ†æƒé‡ï¼‰
            for timestep, reachable_cells in agent_multi_reachable.items():
                # æ—¶é—´æ­¥è¶Šè¿œï¼Œæƒé‡è¶Šä½
                timestep_weight = 0.3 / (timestep + 1)  # t=0: 0.3, t=1: 0.15, t=2: 0.1...
                for cell in reachable_cells:
                    if 0 <= cell < grid.spec.num_cells:
                        c[cell] += timestep_weight

            # 4c. è·å–Agentçš„å†å²è½¨è¿¹æ•°æ®ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„é˜ˆå€¼ï¼‰
            agent_historical_data = trajectory_buffer.get_agent_historical_transitions_strict_matching(
                agent_id=agent_id,
                current_ego_state=current_ego_state,
                current_agents_states=current_agents_states,
                ego_action_trajectory=ego_action_trajectory,
                ego_state_threshold=config.matching.ego_state_threshold,
                agents_state_threshold=config.matching.agents_state_threshold,
                ego_action_threshold=config.matching.ego_action_threshold
            )

            # ä¿å­˜å†å²æ•°æ®åˆ°å¯è§†åŒ–æ•°æ®ç»“æ„ï¼ˆä¸å†æ··å…¥æ¦‚ç‡å›¾ï¼‰
            historical_data_sets[agent_id] = agent_historical_data

        # 5. å°†è‡ªè½¦æœªæ¥è½¨è¿¹æ·»åŠ åˆ°å¯è§†åŒ–ï¼ˆé«˜æƒé‡ï¼‰
        for step_idx, ego_pos in enumerate(ego_action_trajectory):
            ego_cell = grid.world_to_cell(ego_pos)
            if 0 <= ego_cell < grid.spec.num_cells:
                c[ego_cell] += 1.0  # è‡ªè½¦è½¨è¿¹ç”¨é«˜æƒé‡æ˜¾ç¤º
        
        # 6. å½’ä¸€åŒ–å¯è§†åŒ–æ•°æ®
        p_plot = c / (np.max(c) + 1e-12)
        
        # è½¬æ¢åæ ‡ç”¨äºå¯è§†åŒ–ï¼ˆè½¬æ¢åˆ°ç½‘æ ¼åæ ‡ç³»ï¼‰
        ego_grid = grid.to_grid_frame(world_current.ego.position_m)
        agents_grid = []
        for agent in world_current.agents:
            agent_grid = grid.to_grid_frame(agent.position_m)
            agents_grid.append(np.array(agent_grid))
        
        # æ¸²æŸ“çƒ­åŠ›å›¾
        frame_path = ep_dir / f"t_{t+1:02d}.png"
        title = f"Episode {episode_id+1}, t={t+1}s: å¯è¾¾é›†+å†å²è½¨è¿¹+è‡ªè½¦è½¨è¿¹"
        try:
            # ä¼ å…¥å¤šæ—¶é—´æ­¥å¯è¾¾é›†æ•°æ®å’Œå†å²æ•°æ®è¿›è¡Œå¯è§†åŒ–
            grid_heatmap(
                p_plot,
                grid.N,
                np.array(ego_grid),
                agents_grid,
                title,
                str(frame_path),
                grid.size_m,
                multi_timestep_reachable_sets=multi_timestep_reachable,
                historical_data_sets=historical_data_sets,
            )
            frame_paths.append(str(frame_path))
        except Exception as e:
            print(f"  è­¦å‘Š: æ¸²æŸ“å¤±è´¥ t={t+1}: {e}")
            continue
        
        # ç»Ÿè®¡ä¿¡æ¯
        # åŠ¨æ€è·å–æ‰€æœ‰å·²åˆå§‹åŒ–çš„agent ID
        initialized_agent_ids = list(bank.agent_alphas.keys()) if hasattr(bank, 'agent_alphas') else []

        # è®¡ç®—Alphaæ€»å’Œå’ŒçœŸå®éé›¶å•å…ƒæ•°ï¼ˆå…¼å®¹ä¸åŒçš„Bankç±»å‹ï¼‰
        if isinstance(bank, (MultiTimestepSpatialDirichletBank, OptimizedMultiTimestepSpatialDirichletBank)):
            alpha_sum = 0.0
            bank_nonzero_cells = 0
            alpha_out_threshold = bank.params.alpha_out

            for aid in initialized_agent_ids:
                if aid in bank.agent_alphas:
                    for timestep, alpha in bank.agent_alphas[aid].items():
                        alpha_sum += alpha.sum()
                        # ç»Ÿè®¡è¶…è¿‡å…ˆéªŒå€¼çš„å•å…ƒï¼ˆçœŸæ­£å­¦åˆ°äº†çŸ¥è¯†ï¼‰
                        bank_nonzero_cells += int(np.count_nonzero(alpha > alpha_out_threshold))
        else:
            # å¯¹äºæ—§ç‰ˆæœ¬çš„å•æ—¶é—´æ­¥Bank
            alpha_sum = sum(bank.get_agent_alpha(aid).sum() for aid in initialized_agent_ids)
            # ç®€åŒ–ç»Ÿè®¡
            bank_nonzero_cells = sum(int(np.count_nonzero(bank.get_agent_alpha(aid) > 1e-6))
                                    for aid in initialized_agent_ids)

        stats = {
            't': t + 1,
            'alpha_sum': alpha_sum,
            'qmax_max': float(np.max(p_plot)),
            'nz_cells': bank_nonzero_cells,  # æ”¹ä¸ºBankçœŸå®éé›¶å•å…ƒç»Ÿè®¡
            'reachable_cells': {aid: len(current_reachable[aid]) for aid in current_reachable.keys()}
        }
        episode_stats.append(stats)
    
    # ç”Ÿæˆepisode GIF
    gif_path = output_dir / f"episode_{episode_id:02d}.gif"
    make_gif(frame_paths, str(gif_path), fps=2)
    
    # å°†è½¨è¿¹æ•°æ®å­˜å‚¨åˆ°bufferï¼ˆæŒ‰æ—¶é—´æ­¥å­˜å‚¨ï¼‰
    timestep_scenarios = []
    
    # ä¸ºæ¯ä¸ªæ—¶åˆ»åˆ›å»ºè½¨è¿¹æ•°æ®
    for t in range(horizon):
        # è·å–å½“å‰æ—¶åˆ»çš„ä¸–ç•ŒçŠ¶æ€
        world_current = scenario_manager.create_world_state_from_trajectories(
            t, ego_trajectory, agent_trajectories, world_init
        )
        
        # åˆ›å»ºå½“å‰æ—¶åˆ»çš„åœºæ™¯çŠ¶æ€
        current_scenario_state = scenario_manager.create_scenario_state(world_current)
        
        # åˆ›å»ºå½“å‰æ—¶åˆ»çš„è½¨è¿¹æ•°æ®ï¼ˆåªåŒ…å«ä¸‹ä¸€æ­¥ï¼‰
        timestep_trajectory_data = []
        for i, agent in enumerate(world_current.agents):
            agent_id = i + 1
            if agent_id in agent_trajectory_cells and t < len(agent_trajectory_cells[agent_id]):
                # åªå­˜å‚¨ä»å½“å‰æ—¶åˆ»å¼€å§‹çš„å‰©ä½™è½¨è¿¹
                remaining_cells = agent_trajectory_cells[agent_id][t:]
                traj_data = AgentTrajectoryData(
                    agent_id=agent_id,
                    agent_type=agent.agent_type.value,
                    init_position=agent.position_m,
                    init_velocity=agent.velocity_mps,
                    init_heading=agent.heading_rad,
                    trajectory_cells=remaining_cells
                )
                timestep_trajectory_data.append(traj_data)
        
        timestep_scenarios.append((current_scenario_state, timestep_trajectory_data))
    
    # å­˜å‚¨æŒ‰æ—¶é—´æ­¥ç»„ç»‡çš„æ•°æ®ï¼Œä¼ å…¥è‡ªè½¦è½¨è¿¹
    ego_trajectory_tuples = [tuple(pos) for pos in ego_trajectory]
    trajectory_buffer.store_episode_trajectories_by_timestep(episode_id, timestep_scenarios, ego_trajectory_tuples)
    
    return {
        'episode_id': episode_id,
        'frame_paths': frame_paths,
        'gif_path': str(gif_path),
        'stats': episode_stats,
        'selected_trajectory': selected_trajectory_info,
        'all_trajectories': trajectory_q_values
    }


def main():
    parser = argparse.ArgumentParser(description="åŸºäºLatticeè§„åˆ’å™¨çš„Qå€¼ä¼˜åŒ–å®éªŒ")
    # åŸºæœ¬è¿è¡Œå‚æ•°
    parser.add_argument("--episodes", type=int, default=20, help="æ‰§è¡Œepisodeæ•°")
    parser.add_argument("--seed", type=int, default=2025, help="éšæœºç§å­")
    parser.add_argument("--gif-fps", type=int, default=2, help="GIFå¸§ç‡")
    parser.add_argument("--ego-mode", choices=["straight", "fixed-traj"],
                       default="straight", help="è‡ªè½¦è¿åŠ¨æ¨¡å¼")
    parser.add_argument("--sigma", type=float, default=0.5, help="è½¯è®¡æ•°æ ¸å®½åº¦")

    # Checkpointå‚æ•°
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    parser.add_argument("--checkpoint-interval", type=int, default=0, help="Checkpointä¿å­˜é—´éš”ï¼ˆæ¯Nä¸ªepisodeï¼Œ0è¡¨ç¤ºä¸å®šæœŸä¿å­˜ï¼‰")
    parser.add_argument("--resume-from", type=str, help="ä»æŒ‡å®šcheckpointæ¢å¤è®­ç»ƒ")

    # é…ç½®é¢„è®¾å‚æ•°ï¼ˆä¸»è¦é…ç½®æ–¹å¼ï¼‰
    parser.add_argument("--config-preset", choices=["default", "fast", "high-precision", "long-horizon"],
                       default="default", help="é¢„è®¾é…ç½®æ¨¡æ¿")

    # å¯é€‰è¦†ç›–å‚æ•°ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼Œä¸è®¾ç½®é»˜è®¤å€¼é¿å…è¦†ç›–é¢„è®¾é…ç½®ï¼‰
    parser.add_argument("--dt", type=float, help="è¦†ç›–æ—¶é—´æ­¥é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--horizon", type=int, help="è¦†ç›–é¢„æµ‹æ—¶é—´æ­¥æ•°")
    parser.add_argument("--reachable-samples", type=int, help="è¦†ç›–å¯è¾¾é›†é‡‡æ ·æ•°é‡")
    parser.add_argument("--q-samples", type=int, help="è¦†ç›–Qå€¼é‡‡æ ·æ•°é‡")
    # å·²ç®€åŒ–ä¸ºå•ä¸€å¯è§†åŒ–æ¨¡å¼ï¼Œä¸å†éœ€è¦vis-modeå‚æ•°

    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€é…ç½®
    from carla_c2osr.config import ConfigPresets, set_global_config
    
    # é¦–å…ˆåº”ç”¨é¢„è®¾é…ç½®
    if args.config_preset == "fast":
        config = ConfigPresets.fast_testing()
    elif args.config_preset == "high-precision":
        config = ConfigPresets.high_precision()
    elif args.config_preset == "long-horizon":
        config = ConfigPresets.long_horizon()
    else:
        config = get_global_config()
    
    # ä»…åœ¨ç”¨æˆ·æ˜ç¡®æŒ‡å®šæ—¶æ‰è¦†ç›–é¢„è®¾é…ç½®
    if args.dt is not None:
        config.time.dt = args.dt
    if args.horizon is not None:
        config.time.default_horizon = args.horizon
    if args.reachable_samples is not None:
        config.sampling.reachable_set_samples = args.reachable_samples
    if args.q_samples is not None:
        config.sampling.q_value_samples = args.q_samples
    
    # è¿™äº›å‚æ•°æ€»æ˜¯ä»å‘½ä»¤è¡Œè·å–ï¼ˆå› ä¸ºå®ƒä»¬ä¸æ˜¯é¢„è®¾é…ç½®çš„ä¸€éƒ¨åˆ†ï¼‰
    config.random_seed = args.seed
    config.visualization.gif_fps = args.gif_fps
    
    set_global_config(config)
    
    print(f"=== Latticeè§„åˆ’å™¨ + Qå€¼ä¼˜åŒ–å®éªŒ ===")
    print(f"Episodes: {args.episodes}, Horizon: {config.time.default_horizon}")
    print(f"Ego mode: {args.ego_mode}, Sigma: {args.sigma}")
    print(f"Seed: {args.seed}")
    print(f"é…ç½®é¢„è®¾: {args.config_preset}")
    print(f"æ—¶é—´æ­¥é•¿: {config.time.dt}s, é¢„æµ‹æ—¶é—´: {config.time.horizon_seconds:.1f}s")
    print(f"å¯è¾¾é›†é‡‡æ ·: {config.sampling.reachable_set_samples}, Qå€¼é‡‡æ ·: {config.sampling.q_value_samples}")
    print(f"Latticeè½¨è¿¹æ•°: {config.lattice.num_trajectories}, "
          f"æ¨ªå‘åç§»: {config.lattice.lateral_offsets}, "
          f"é€Ÿåº¦å˜åŒ–: {config.lattice.speed_variations}")
    print(f"è½¨è¿¹å­˜å‚¨å€æ•°: {config.matching.trajectory_storage_multiplier}x (æ•°æ®å¢å¼º)")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    
    # åˆå§‹åŒ–ç»„ä»¶
    scenario_manager = ScenarioManager()
    # å…ˆåˆ›å»ºç½‘æ ¼ï¼Œç„¶åä½¿ç”¨æ­£ç¡®çš„è¾¹ç•Œåˆå§‹åŒ–è½¨è¿¹ç”Ÿæˆå™¨
    world_init = scenario_manager.create_scenario()
    ego_start_pos = world_init.ego.position_m

    # ä»å…¨å±€é…ç½®è¯»å–ç½‘æ ¼å‚æ•°
    grid_spec = GridSpec(
        size_m=config.grid.grid_size_m,
        cell_m=config.grid.cell_size_m,
        macro=True
    )
    grid = GridMapper(grid_spec, world_center=ego_start_pos)

    # ä½¿ç”¨æ­£ç¡®çš„ç½‘æ ¼è¾¹ç•Œåˆå§‹åŒ–è½¨è¿¹ç”Ÿæˆå™¨
    grid_half_size = grid.size_m / 2.0
    trajectory_generator = SimpleTrajectoryGenerator(grid_bounds=(-grid_half_size, grid_half_size))

    # åˆ›å»ºåœºæ™¯çŠ¶æ€
    scenario_state = scenario_manager.create_scenario_state(world_init)

    # ä»å…¨å±€é…ç½®è¯»å–Dirichletå‚æ•°
    dirichlet_params = DirichletParams(
        alpha_in=config.dirichlet.alpha_in,
        alpha_out=config.dirichlet.alpha_out,
        delta=config.dirichlet.delta,
        cK=config.dirichlet.cK
    )
    # ä½¿ç”¨ç»ˆæä¼˜åŒ–ç‰ˆæœ¬çš„Bank - æ”¯æŒç›´æ¥æœŸæœ›è®¡ç®—ï¼Œé›¶é‡‡æ ·
    bank = OptimizedMultiTimestepSpatialDirichletBank(grid.K, dirichlet_params, horizon=config.time.default_horizon)
    print(f"ğŸš€ ä½¿ç”¨ç»ˆæä¼˜åŒ–ç‰ˆæœ¬çš„Dirichlet Bank - ç»´åº¦è‡ªé€‚åº”ï¼Œé›¶é‡‡æ ·è®¡ç®—")
    
    # åˆå§‹åŒ–è½¨è¿¹ç¼“å†²åŒº
    trajectory_buffer = TrajectoryBuffer(horizon=config.time.default_horizon)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨å’Œåˆ†æå™¨
    q_evaluator = QEvaluator()
    buffer_analyzer = BufferAnalyzer(trajectory_buffer)
    q_tracker = QDistributionTracker()  # åˆ›å»ºQå€¼åˆ†å¸ƒè·Ÿè¸ªå™¨

    # åˆå§‹åŒ–Latticeè§„åˆ’å™¨
    lattice_planner = LatticePlanner.from_config(config)

    # ä»åœºæ™¯ç®¡ç†å™¨ç”Ÿæˆreference pathï¼ˆä½œä¸ºlatticeè§„åˆ’å’ŒQå€¼è®¡ç®—çš„ç»Ÿä¸€ä¸­å¿ƒçº¿ï¼‰
    reference_path = scenario_manager.generate_reference_path(
        mode=args.ego_mode,
        horizon=config.time.default_horizon,
        ego_start=world_init.ego.position_m
    )
    print(f"\nç”ŸæˆReference Path: {len(reference_path)} ä¸ªwaypoints (mode={args.ego_mode})")
    
    # åªå¯¹ç¯å¢ƒæ™ºèƒ½ä½“åˆå§‹åŒ–Dirichletåˆ†å¸ƒï¼ˆä¸åŒ…æ‹¬è‡ªè½¦ï¼‰
    for i, agent in enumerate(world_init.agents):
        agent_id = i + 1
        # ä½¿ç”¨å¤šæ—¶é—´æ­¥å¯è¾¾é›†è¿›è¡Œåˆå§‹åŒ–ï¼ˆé€‚é…ä¼˜åŒ–ç‰ˆBankï¼‰
        # ä½¿ç”¨å…¨å±€é…ç½®ä¸­çš„é‡‡æ ·æ•°
        multi_reachable = grid.multi_timestep_successor_cells(
            agent,
            horizon=config.time.default_horizon,
            dt=config.time.dt,
            n_samples=config.sampling.reachable_set_samples
        )
        if not multi_reachable:
            # å¦‚æœæ²¡æœ‰å¯è¾¾é›†ï¼Œä¸ºæ¯ä¸ªæ—¶é—´æ­¥æ·»åŠ å½“å‰ä½ç½®ä½œä¸ºå¯è¾¾
            current_cell = grid.world_to_cell(agent.position_m)
            multi_reachable = {t: [current_cell] for t in range(1, config.time.default_horizon + 1)}

        bank.init_agent(agent_id, multi_reachable)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = setup_output_dirs()

    # Checkpointç®¡ç†
    checkpoint_manager = CheckpointManager(checkpoint_dir=args.checkpoint_dir)
    start_episode = 0

    # ä»checkpointæ¢å¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.resume_from:
        print(f"\nğŸ”„ ä»checkpointæ¢å¤è®­ç»ƒ...")
        checkpoint_data = checkpoint_manager.load_checkpoint(args.resume_from)

        # æ¢å¤training_state
        start_episode = checkpoint_data['training_state']['episode_id'] + 1

        # æ¢å¤TrajectoryBuffer
        trajectory_buffer = TrajectoryBuffer.from_dict(checkpoint_data['trajectory_buffer_data'])

        # æ¢å¤DirichletBank
        bank = OptimizedMultiTimestepSpatialDirichletBank.from_dict(checkpoint_data['dirichlet_bank_data'])

        # æ¢å¤QDistributionTracker
        q_tracker = QDistributionTracker()
        q_tracker_data = checkpoint_data['q_tracker_data']
        q_tracker.episode_data = q_tracker_data.get('episode_data', [])
        q_tracker.q_value_history = q_tracker_data.get('q_value_history', [])
        q_tracker.percentile_q_history = q_tracker_data.get('percentile_q_history', [])
        q_tracker.collision_rate_history = q_tracker_data.get('collision_rate_history', [])
        q_tracker.q_distribution_history = [ep['q_distribution'] for ep in q_tracker.episode_data]
        q_tracker.detailed_info_history = [ep.get('detailed_info', {}) for ep in q_tracker.episode_data]

        # æ›´æ–°buffer_analyzer
        buffer_analyzer = BufferAnalyzer(trajectory_buffer)

        print(f"âœ… å·²æ¢å¤åˆ°Episode {start_episode}ï¼Œç»§ç»­è®­ç»ƒ...")

    # è¿è¡Œæ‰€æœ‰episodes
    all_episodes = []
    summary_frames = []

    for e in range(start_episode, args.episodes):
        try:
            rng = np.random.default_rng(args.seed + e)
            
            print(f"\nRunning Episode {e+1}/{args.episodes}")
            episode_result = run_episode(
                e, config.time.default_horizon, reference_path, world_init, grid, bank,
                trajectory_buffer, scenario_state, rng, output_dir, args.sigma,
                lattice_planner=lattice_planner,
                q_evaluator=q_evaluator,
                trajectory_generator=trajectory_generator,
                scenario_manager=scenario_manager,
                buffer_analyzer=buffer_analyzer,
                q_tracker=q_tracker
            )
            all_episodes.append(episode_result)
            
            # æ”¶é›†æœ€åä¸€å¸§ç”¨äºæ±‡æ€»GIFï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if episode_result['frame_paths']:
                summary_frames.append(episode_result['frame_paths'][-1])
            
            # æ‰“å°episodeå®ŒæˆçŠ¶æ€ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
            if episode_result['stats']:
                final_stats = episode_result['stats'][-1]
                print(f"  å®Œæˆ: alpha_sum={final_stats['alpha_sum']:.1f}, "
                      f"nz_cells={final_stats['nz_cells']}")
            
            # æ¯10ä¸ªepisodeæ¸…ç†ä¸€æ¬¡matplotlibå†…å­˜
            if (e + 1) % 10 == 0:
                import matplotlib.pyplot as plt
                plt.close('all')
                print(f"  å†…å­˜æ¸…ç†: Episode {e+1}")

            # å®šæœŸä¿å­˜checkpoint
            if args.checkpoint_interval > 0 and (e + 1) % args.checkpoint_interval == 0:
                try:
                    # å‡†å¤‡é…ç½®å­—å…¸
                    config_dict = {
                        'time': config.time.__dict__,
                        'sampling': config.sampling.__dict__,
                        'grid': config.grid.__dict__,
                        'dirichlet': config.dirichlet.__dict__,
                        'matching': config.matching.__dict__,
                        'reward': config.reward.__dict__,
                        'lattice': config.lattice.__dict__,
                        'visualization': config.visualization.__dict__
                    }

                    checkpoint_manager.save_checkpoint(
                        episode_id=e,
                        trajectory_buffer=trajectory_buffer,
                        dirichlet_bank=bank,
                        q_tracker=q_tracker,
                        config=config_dict,
                        metadata={
                            'episodes_total': args.episodes,
                            'checkpoint_interval': args.checkpoint_interval
                        }
                    )
                except Exception as checkpoint_ex:
                    print(f"  âš ï¸ Checkpointä¿å­˜å¤±è´¥: {checkpoint_ex}")

        except Exception as ex:
            print(f"Episode {e+1} æ‰§è¡Œå¤±è´¥: {ex}")
            print("ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªepisode...")
            continue
    
    # ç”Ÿæˆæ±‡æ€»GIF
    if summary_frames:
        summary_gif_path = output_dir / "summary.gif"
        make_gif(summary_frames, str(summary_gif_path), fps=1)
        print(f"\n=== å®Œæˆ ===")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
    else:
        print(f"\n=== è­¦å‘Šï¼šæ‰€æœ‰episodeéƒ½å¤±è´¥ï¼Œæ²¡æœ‰ç”ŸæˆGIF ===")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # æ‰“å°è½¨è¿¹é€‰æ‹©æ”¹è¿›è¶‹åŠ¿
    selected_trajectories = [ep['selected_trajectory'] for ep in all_episodes if ep['selected_trajectory']]

    if selected_trajectories:
        first_selected = selected_trajectories[0]
        last_selected = selected_trajectories[-1]

        # è·å–ç™¾åˆ†ä½æ•°é…ç½®
        q_config = QValueConfig.from_global_config()
        percentile = q_config.q_selection_percentile

        print(f"\nè½¨è¿¹é€‰æ‹©æ”¹è¿›:")
        print(f"  ç¬¬1ä¸ªepisode: è½¨è¿¹{first_selected['trajectory_id']}")
        print(f"    Min_Q={first_selected['min_q']:.2f}, "
              f"Mean_Q={first_selected['mean_q']:.2f}, "
              f"P{int(percentile*100)}_Q={first_selected['percentile_q']:.2f}, "
              f"ç¢°æ’ç‡={first_selected['collision_rate']:.3f}")
        print(f"  ç¬¬{len(selected_trajectories)}ä¸ªepisode: è½¨è¿¹{last_selected['trajectory_id']}")
        print(f"    Min_Q={last_selected['min_q']:.2f}, "
              f"Mean_Q={last_selected['mean_q']:.2f}, "
              f"P{int(percentile*100)}_Q={last_selected['percentile_q']:.2f}, "
              f"ç¢°æ’ç‡={last_selected['collision_rate']:.3f}")

        # è®¡ç®—æ”¹è¿›
        percentile_q_improvement = last_selected['percentile_q'] - first_selected['percentile_q']
        collision_rate_improvement = first_selected['collision_rate'] - last_selected['collision_rate']

        print(f"  P{int(percentile*100)}_Qæ”¹è¿›: {percentile_q_improvement:+.2f}, ç¢°æ’ç‡é™ä½: {collision_rate_improvement:+.3f}")

    # æ‰“å°å­¦ä¹ è¶‹åŠ¿
    first_stats = all_episodes[0]['stats'][-1]
    last_stats = all_episodes[-1]['stats'][-1]
    print(f"\nDirichletå­¦ä¹ : Alpha {first_stats['alpha_sum']:.1f} -> {last_stats['alpha_sum']:.1f}, "
          f"éé›¶å•å…ƒ {first_stats['nz_cells']} -> {last_stats['nz_cells']}")
    
    # æ‰“å°è½¨è¿¹bufferç»Ÿè®¡
    buffer_stats = buffer_analyzer.get_buffer_stats()
    storage_multiplier = config.matching.trajectory_storage_multiplier
    actual_episodes = buffer_stats['total_episodes'] // storage_multiplier if storage_multiplier > 1 else buffer_stats['total_episodes']
    print(f"\nBuffer: {buffer_stats['total_agents']} agents, "
          f"{buffer_stats['total_episodes']} æ¡å­˜å‚¨è®°å½• (å®é™…{actual_episodes}ä¸ªepisode Ã— {storage_multiplier}å€), "
          f"{buffer_stats['total_agent_episodes']} agent-episodes")

    # ç”ŸæˆQå€¼åˆ†å¸ƒå¯è§†åŒ–
    if len(q_tracker.q_value_history) > 0:
        q_evolution_path = output_dir / "q_distribution_evolution.png"
        collision_rate_path = output_dir / "collision_rate_evolution.png"
        q_data_path = output_dir / "q_distribution_data.json"

        try:
            # ç”ŸæˆQå€¼åˆ†å¸ƒæ¼”åŒ–å›¾ï¼ˆæ‰€æœ‰Qå€¼éšepisodeå˜åŒ–ï¼‰
            q_tracker.plot_q_distribution_evolution(str(q_evolution_path))

            # ç”Ÿæˆç¢°æ’ç‡å˜åŒ–å›¾
            q_tracker.plot_collision_rate_evolution(str(collision_rate_path))

            # ä¿å­˜æ•°æ®
            q_tracker.save_data(str(q_data_path))

            print(f"\nå¯è§†åŒ–å·²ç”Ÿæˆ: {q_evolution_path.name}, {collision_rate_path.name}, {q_data_path.name}")

        except Exception as e:
            print(f"è­¦å‘Š: Qå€¼åˆ†å¸ƒå¯è§†åŒ–å¤±è´¥: {e}")
    else:
        print(f"\nè­¦å‘Š: æ²¡æœ‰Qå€¼æ•°æ®")

    # ä¿å­˜æœ€ç»ˆcheckpoint
    if all_episodes:  # åªåœ¨æœ‰æˆåŠŸçš„episodesæ—¶ä¿å­˜
        try:
            print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆcheckpoint...")
            # å‡†å¤‡é…ç½®å­—å…¸
            config_dict = {
                'time': config.time.__dict__,
                'sampling': config.sampling.__dict__,
                'grid': config.grid.__dict__,
                'dirichlet': config.dirichlet.__dict__,
                'matching': config.matching.__dict__,
                'reward': config.reward.__dict__,
                'lattice': config.lattice.__dict__,
                'visualization': config.visualization.__dict__
            }

            checkpoint_manager.save_checkpoint(
                episode_id=args.episodes - 1,
                trajectory_buffer=trajectory_buffer,
                dirichlet_bank=bank,
                q_tracker=q_tracker,
                config=config_dict,
                metadata={
                    'episodes_total': args.episodes,
                    'is_final': True
                }
            )
        except Exception as checkpoint_ex:
            print(f"  âš ï¸ æœ€ç»ˆcheckpointä¿å­˜å¤±è´¥: {checkpoint_ex}")


if __name__ == "__main__":
    main()
