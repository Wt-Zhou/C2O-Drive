#!/usr/bin/env python3
"""
åŸºäºLatticeè§„åˆ’å™¨çš„å¤šæ¬¡åœºæ™¯æ‰§è¡Œï¼ˆç®€åŒ–é‡æ„ç‰ˆï¼‰

è¿™æ˜¯replay_openloop_lattice.pyçš„ç®€åŒ–é‡æ„ç‰ˆæœ¬ï¼š
- å°†å¤æ‚çš„run_episodeå‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹æ¨¡å—
- ä½¿ç”¨EpisodeContextå°è£…å‚æ•°ï¼Œæé«˜å¯è¯»æ€§
- ä¿æŒä¸åŸç‰ˆæœ¬å®Œå…¨ç›¸åŒçš„åŠŸèƒ½

æ¼”ç¤ºlatticeè½¨è¿¹è§„åˆ’ä¸Qå€¼è¯„ä¼°ç»“åˆçš„è´å¶æ–¯å­¦ä¹ è¿‡ç¨‹ï¼š
- æ¯ä¸ªepisodeä½¿ç”¨lattice plannerç”Ÿæˆå€™é€‰è½¨è¿¹
- ä¸ºæ¯æ¡å€™é€‰è½¨è¿¹è®¡ç®—Qå€¼
- ä½¿ç”¨ç™¾åˆ†ä½å‡†åˆ™é€‰æ‹©æœ€ä¼˜è½¨è¿¹
- è·Ÿè¸ªQå€¼éšepisodeçš„æ”¹è¿›æƒ…å†µ
"""

from __future__ import annotations
import argparse
import sys
from pathlib import Path
import numpy as np
from typing import Any, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
_repo_root = Path(__file__).resolve().parents[2]
if str(_repo_root) not in sys.path:
    sys.path.insert(0, str(_repo_root))

from carla_c2osr.agents.c2osr.grid import GridSpec, GridMapper
from carla_c2osr.agents.c2osr.spatial_dirichlet import DirichletParams, OptimizedMultiTimestepSpatialDirichletBank
from carla_c2osr.agents.c2osr.trajectory_buffer import TrajectoryBuffer
from carla_c2osr.evaluation.buffer_analyzer import BufferAnalyzer
from carla_c2osr.evaluation.q_evaluator import QEvaluator
from carla_c2osr.evaluation.q_distribution_tracker import QDistributionTracker
from carla_c2osr.evaluation.q_value_calculator import QValueConfig
from carla_c2osr.utils.simple_trajectory_generator import SimpleTrajectoryGenerator
from carla_c2osr.utils.lattice_planner import LatticePlanner
from carla_c2osr.utils.checkpoint_manager import CheckpointManager
from carla_c2osr.env.scenario_manager import ScenarioManager
from carla_c2osr.config import get_global_config, set_global_config, ConfigPresets

# å¯¼å…¥é‡æ„çš„æ¨¡å—
from carla_c2osr.runner.refactored import (
    EpisodeContext,
    TrajectoryEvaluator,
    TimestepExecutor,
    VisualizationManager,
    DataManager
)


def setup_output_dirs(base_dir: str = "outputs/replay_experiment") -> Path:
    """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
    base_path = Path(base_dir)
    base_path.mkdir(parents=True, exist_ok=True)
    return base_path


def run_episode(episode_id: int,
                horizon: int,
                reference_path,
                world_init,
                grid,
                bank,
                trajectory_buffer,
                scenario_state,
                rng,
                output_dir,
                sigma: float,
                lattice_planner=None,
                q_evaluator=None,
                trajectory_generator=None,
                scenario_manager=None,
                buffer_analyzer=None,
                q_tracker=None) -> Dict[str, Any]:
    """
    è¿è¡Œå•ä¸ªepisodeï¼ˆç®€åŒ–é‡æ„ç‰ˆï¼‰

    ä½¿ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå°†åŸæ¥492è¡Œçš„å‡½æ•°ç®€åŒ–åˆ°çº¦80è¡Œã€‚
    """
    # 1. åˆ›å»ºEpisodeä¸Šä¸‹æ–‡
    ctx = EpisodeContext.create(
        episode_id=episode_id,
        horizon=horizon,
        reference_path=reference_path,
        world_init=world_init,
        grid=grid,
        bank=bank,
        trajectory_buffer=trajectory_buffer,
        scenario_state=scenario_state,
        rng=rng,
        output_dir=output_dir,
        sigma=sigma,
        lattice_planner=lattice_planner,
        q_evaluator=q_evaluator,
        trajectory_generator=trajectory_generator,
        scenario_manager=scenario_manager,
        buffer_analyzer=buffer_analyzer,
        q_tracker=q_tracker
    )

    # 2. ç”Ÿæˆå¹¶è¯„ä¼°å€™é€‰è½¨è¿¹
    evaluator = TrajectoryEvaluator(ctx)
    trajectory_q_values = evaluator.generate_and_evaluate_trajectories()

    # 3. é€‰æ‹©æœ€ä¼˜è½¨è¿¹
    ego_trajectory, selected_trajectory_info = evaluator.select_optimal_trajectory(trajectory_q_values)

    # 4. å¯è§†åŒ–è½¨è¿¹é€‰æ‹©
    vis_manager = VisualizationManager(ctx)
    vis_manager.visualize_trajectory_selection(trajectory_q_values, selected_trajectory_info)

    # 5. ç”Ÿæˆagentè½¨è¿¹
    data_manager = DataManager(ctx)
    agent_trajectories, agent_trajectory_cells = data_manager.generate_agent_trajectories()

    # 6. æ‰§è¡Œæ‰€æœ‰æ—¶é—´æ­¥
    timestep_executor = TimestepExecutor(ctx)
    episode_stats, frame_paths = timestep_executor.execute_all_timesteps(
        ego_trajectory, agent_trajectories
    )

    # 7. ç”Ÿæˆepisode GIF
    gif_path = vis_manager.generate_episode_gif(frame_paths)

    # 8. è®°å½•æ‰€æœ‰è½¨è¿¹çš„Qå€¼æ•°æ®
    if ctx.q_tracker is not None:
        ctx.q_tracker.add_all_trajectories_data(episode_id, trajectory_q_values)
        # å¯è§†åŒ–æ‰€æœ‰è½¨è¿¹Qå€¼æ¼”åŒ–ï¼ˆä»ç¬¬1ä¸ªepisodeåˆ°å½“å‰episodeï¼‰
        vis_manager.visualize_q_evolution()

    # 9. å­˜å‚¨è½¨è¿¹æ•°æ®åˆ°buffer
    data_manager.store_episode_trajectories(
        ego_trajectory, agent_trajectories, agent_trajectory_cells
    )

    return {
        'episode_id': episode_id,
        'frame_paths': frame_paths,
        'gif_path': gif_path,
        'stats': episode_stats,
        'selected_trajectory': selected_trajectory_info,
        'all_trajectories': trajectory_q_values
    }


def initialize_components(args, world_init, output_dir):
    """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
    config = get_global_config()
    ego_start_pos = world_init.ego.position_m

    # åˆ›å»ºç½‘æ ¼
    grid_spec = GridSpec(
        size_m=config.grid.grid_size_m,
        cell_m=config.grid.cell_size_m,
        macro=True
    )
    grid = GridMapper(grid_spec, world_center=ego_start_pos)

    # åˆ›å»ºè½¨è¿¹ç”Ÿæˆå™¨
    grid_half_size = grid.size_m / 2.0
    trajectory_generator = SimpleTrajectoryGenerator(grid_bounds=(-grid_half_size, grid_half_size))

    # åˆ›å»ºDirichlet Bank
    dirichlet_params = DirichletParams(
        alpha_in=config.dirichlet.alpha_in,
        alpha_out=config.dirichlet.alpha_out,
        delta=config.dirichlet.delta,
        cK=config.dirichlet.cK
    )
    bank = OptimizedMultiTimestepSpatialDirichletBank(grid.K, dirichlet_params, horizon=config.time.default_horizon)
    print(f"ğŸš€ ä½¿ç”¨ç»ˆæä¼˜åŒ–ç‰ˆæœ¬çš„Dirichlet Bank - ç»´åº¦è‡ªé€‚åº”ï¼Œé›¶é‡‡æ ·è®¡ç®—")

    # åˆ›å»ºå…¶ä»–ç»„ä»¶
    trajectory_buffer = TrajectoryBuffer(horizon=config.time.default_horizon)
    q_evaluator = QEvaluator()
    buffer_analyzer = BufferAnalyzer(trajectory_buffer)
    q_tracker = QDistributionTracker()
    lattice_planner = LatticePlanner.from_config(config)
    scenario_manager = ScenarioManager()

    # åˆå§‹åŒ–agentçš„Dirichletåˆ†å¸ƒ
    for i, agent in enumerate(world_init.agents):
        agent_id = i + 1
        multi_reachable = grid.multi_timestep_successor_cells(
            agent,
            horizon=config.time.default_horizon,
            dt=config.time.dt,
            n_samples=config.sampling.reachable_set_samples
        )
        if not multi_reachable:
            current_cell = grid.world_to_cell(agent.position_m)
            multi_reachable = {t: [current_cell] for t in range(1, config.time.default_horizon + 1)}
        bank.init_agent(agent_id, multi_reachable)

    return {
        'grid': grid,
        'bank': bank,
        'trajectory_buffer': trajectory_buffer,
        'trajectory_generator': trajectory_generator,
        'q_evaluator': q_evaluator,
        'buffer_analyzer': buffer_analyzer,
        'q_tracker': q_tracker,
        'lattice_planner': lattice_planner,
        'scenario_manager': scenario_manager
    }


def run_all_episodes(args, components, reference_path, world_init, scenario_state, output_dir,
                     checkpoint_manager=None, start_episode=0):
    """è¿è¡Œæ‰€æœ‰episodes"""
    all_episodes = []
    summary_frames = []

    for e in range(start_episode, args.episodes):
        try:
            rng = np.random.default_rng(args.seed + e)

            print(f"\nRunning Episode {e+1}/{args.episodes}")
            config = get_global_config()
            episode_result = run_episode(
                e, config.time.default_horizon, reference_path, world_init,
                components['grid'], components['bank'], components['trajectory_buffer'],
                scenario_state, rng, output_dir, args.sigma,
                lattice_planner=components['lattice_planner'],
                q_evaluator=components['q_evaluator'],
                trajectory_generator=components['trajectory_generator'],
                scenario_manager=components['scenario_manager'],
                buffer_analyzer=components['buffer_analyzer'],
                q_tracker=components['q_tracker']
            )
            all_episodes.append(episode_result)

            # æ”¶é›†æœ€åä¸€å¸§ç”¨äºæ±‡æ€»GIF
            if episode_result['frame_paths']:
                summary_frames.append(episode_result['frame_paths'][-1])

            # æ‰“å°å®ŒæˆçŠ¶æ€
            if episode_result['stats']:
                final_stats = episode_result['stats'][-1]
                print(f"  å®Œæˆ: alpha_sum={final_stats['alpha_sum']:.1f}, "
                      f"nz_cells={final_stats['nz_cells']}")

            # æ¯10ä¸ªepisodeæ¸…ç†matplotlibå†…å­˜
            if (e + 1) % 10 == 0:
                import matplotlib.pyplot as plt
                plt.close('all')
                print(f"  å†…å­˜æ¸…ç†: Episode {e+1}")

            # å®šæœŸä¿å­˜checkpoint
            if checkpoint_manager and args.checkpoint_interval > 0 and (e + 1) % args.checkpoint_interval == 0:
                try:
                    # å‡†å¤‡é…ç½®å­—å…¸
                    config_dict = {
                        'time': config.time.__dict__,
                        'sampling': config.sampling.__dict__,
                        'grid': config.grid.__dict__,
                        'dirichlet': config.dirichlet.__dict__,
                        'matching': config.matching.__dict__,
                        'reward': config.reward.__dict__,
                        'lattice': config.lattice.__dict__,
                        'visualization': config.visualization.__dict__
                    }

                    checkpoint_manager.save_checkpoint(
                        episode_id=e,
                        trajectory_buffer=components['trajectory_buffer'],
                        dirichlet_bank=components['bank'],
                        q_tracker=components['q_tracker'],
                        config=config_dict,
                        metadata={
                            'episodes_total': args.episodes,
                            'checkpoint_interval': args.checkpoint_interval
                        }
                    )
                except Exception as checkpoint_ex:
                    print(f"  âš ï¸ Checkpointä¿å­˜å¤±è´¥: {checkpoint_ex}")

        except Exception as ex:
            print(f"Episode {e+1} æ‰§è¡Œå¤±è´¥: {ex}")
            print("ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªepisode...")
            continue

    return all_episodes, summary_frames


def print_summary(all_episodes, components, output_dir):
    """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
    print(f"\n=== å®Œæˆ ===")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # æ‰“å°è½¨è¿¹é€‰æ‹©æ”¹è¿›è¶‹åŠ¿
    selected_trajectories = [ep['selected_trajectory'] for ep in all_episodes if ep['selected_trajectory']]

    if selected_trajectories:
        first_selected = selected_trajectories[0]
        last_selected = selected_trajectories[-1]

        q_config = QValueConfig.from_global_config()
        percentile = q_config.q_selection_percentile

        print(f"\nè½¨è¿¹é€‰æ‹©æ”¹è¿›:")
        print(f"  ç¬¬1ä¸ªepisode: è½¨è¿¹{first_selected['trajectory_id']}")
        print(f"    Min_Q={first_selected['min_q']:.2f}, "
              f"Mean_Q={first_selected['mean_q']:.2f}, "
              f"P{int(percentile*100)}_Q={first_selected['percentile_q']:.2f}, "
              f"ç¢°æ’ç‡={first_selected['collision_rate']:.3f}")
        print(f"  ç¬¬{len(selected_trajectories)}ä¸ªepisode: è½¨è¿¹{last_selected['trajectory_id']}")
        print(f"    Min_Q={last_selected['min_q']:.2f}, "
              f"Mean_Q={last_selected['mean_q']:.2f}, "
              f"P{int(percentile*100)}_Q={last_selected['percentile_q']:.2f}, "
              f"ç¢°æ’ç‡={last_selected['collision_rate']:.3f}")

        percentile_q_improvement = last_selected['percentile_q'] - first_selected['percentile_q']
        collision_rate_improvement = first_selected['collision_rate'] - last_selected['collision_rate']

        print(f"  P{int(percentile*100)}_Qæ”¹è¿›: {percentile_q_improvement:+.2f}, "
              f"ç¢°æ’ç‡é™ä½: {collision_rate_improvement:+.3f}")

    # æ‰“å°å­¦ä¹ è¶‹åŠ¿
    first_stats = all_episodes[0]['stats'][-1]
    last_stats = all_episodes[-1]['stats'][-1]
    print(f"\nDirichletå­¦ä¹ : Alpha {first_stats['alpha_sum']:.1f} -> {last_stats['alpha_sum']:.1f}, "
          f"éé›¶å•å…ƒ {first_stats['nz_cells']} -> {last_stats['nz_cells']}")

    # æ‰“å°bufferç»Ÿè®¡
    buffer_stats = components['buffer_analyzer'].get_buffer_stats()
    config = get_global_config()
    storage_multiplier = config.matching.trajectory_storage_multiplier
    actual_episodes = buffer_stats['total_episodes'] // storage_multiplier if storage_multiplier > 1 else buffer_stats['total_episodes']
    print(f"\nBuffer: {buffer_stats['total_agents']} agents, "
          f"{buffer_stats['total_episodes']} æ¡å­˜å‚¨è®°å½• (å®é™…{actual_episodes}ä¸ªepisode Ã— {storage_multiplier}å€), "
          f"{buffer_stats['total_agent_episodes']} agent-episodes")

    # ç”ŸæˆQå€¼åˆ†å¸ƒå¯è§†åŒ–
    q_tracker = components['q_tracker']
    if len(q_tracker.q_value_history) > 0:
        q_evolution_path = output_dir / "q_distribution_evolution.png"
        collision_rate_path = output_dir / "collision_rate_evolution.png"
        q_data_path = output_dir / "q_distribution_data.json"

        try:
            q_tracker.plot_q_distribution_evolution(str(q_evolution_path))
            q_tracker.plot_collision_rate_evolution(str(collision_rate_path))
            q_tracker.save_data(str(q_data_path))

            print(f"\nå¯è§†åŒ–å·²ç”Ÿæˆ: {q_evolution_path.name}, {collision_rate_path.name}, {q_data_path.name}")

        except Exception as e:
            print(f"è­¦å‘Š: Qå€¼åˆ†å¸ƒå¯è§†åŒ–å¤±è´¥: {e}")
    else:
        print(f"\nè­¦å‘Š: æ²¡æœ‰Qå€¼æ•°æ®")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åŸºäºLatticeè§„åˆ’å™¨çš„Qå€¼ä¼˜åŒ–å®éªŒï¼ˆç®€åŒ–é‡æ„ç‰ˆï¼‰")

    # åŸºæœ¬è¿è¡Œå‚æ•°
    parser.add_argument("--episodes", type=int, default=20, help="æ‰§è¡Œepisodeæ•°")
    parser.add_argument("--seed", type=int, default=2025, help="éšæœºç§å­")
    parser.add_argument("--gif-fps", type=int, default=2, help="GIFå¸§ç‡")
    parser.add_argument("--ego-mode", choices=["straight", "fixed-traj"],
                       default="straight", help="è‡ªè½¦è¿åŠ¨æ¨¡å¼")
    parser.add_argument("--sigma", type=float, default=0.5, help="è½¯è®¡æ•°æ ¸å®½åº¦")

    # Checkpointå‚æ•°
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    parser.add_argument("--checkpoint-interval", type=int, default=0, help="Checkpointä¿å­˜é—´éš”ï¼ˆæ¯Nä¸ªepisodeï¼Œ0è¡¨ç¤ºä¸å®šæœŸä¿å­˜ï¼‰")
    parser.add_argument("--resume-from", type=str, help="ä»æŒ‡å®šcheckpointæ¢å¤è®­ç»ƒ")

    # é…ç½®é¢„è®¾å‚æ•°
    parser.add_argument("--config-preset", choices=["default", "fast", "high-precision", "long-horizon"],
                       default="default", help="é¢„è®¾é…ç½®æ¨¡æ¿")

    # å¯é€‰è¦†ç›–å‚æ•°
    parser.add_argument("--dt", type=float, help="è¦†ç›–æ—¶é—´æ­¥é•¿ï¼ˆç§’ï¼‰")
    parser.add_argument("--horizon", type=int, help="è¦†ç›–é¢„æµ‹æ—¶é—´æ­¥æ•°")
    parser.add_argument("--reachable-samples", type=int, help="è¦†ç›–å¯è¾¾é›†é‡‡æ ·æ•°é‡")
    parser.add_argument("--q-samples", type=int, help="è¦†ç›–Qå€¼é‡‡æ ·æ•°é‡")

    return parser.parse_args()


def configure_system(args):
    """é…ç½®ç³»ç»Ÿå‚æ•°"""
    # åº”ç”¨é¢„è®¾é…ç½®
    if args.config_preset == "fast":
        config = ConfigPresets.fast_testing()
    elif args.config_preset == "high-precision":
        config = ConfigPresets.high_precision()
    elif args.config_preset == "long-horizon":
        config = ConfigPresets.long_horizon()
    else:
        config = get_global_config()

    # è¦†ç›–ç‰¹å®šå‚æ•°
    if args.dt is not None:
        config.time.dt = args.dt
    if args.horizon is not None:
        config.time.default_horizon = args.horizon
    if args.reachable_samples is not None:
        config.sampling.reachable_set_samples = args.reachable_samples
    if args.q_samples is not None:
        config.sampling.q_value_samples = args.q_samples

    # è®¾ç½®è¿è¡Œå‚æ•°
    config.random_seed = args.seed
    config.visualization.gif_fps = args.gif_fps

    set_global_config(config)

    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"=== Latticeè§„åˆ’å™¨ + Qå€¼ä¼˜åŒ–å®éªŒï¼ˆç®€åŒ–é‡æ„ç‰ˆï¼‰ ===")
    print(f"Episodes: {args.episodes}, Horizon: {config.time.default_horizon}")
    print(f"Ego mode: {args.ego_mode}, Sigma: {args.sigma}")
    print(f"Seed: {args.seed}")
    print(f"é…ç½®é¢„è®¾: {args.config_preset}")
    print(f"æ—¶é—´æ­¥é•¿: {config.time.dt}s, é¢„æµ‹æ—¶é—´: {config.time.horizon_seconds:.1f}s")
    print(f"å¯è¾¾é›†é‡‡æ ·: {config.sampling.reachable_set_samples}, Qå€¼é‡‡æ ·: {config.sampling.q_value_samples}")
    print(f"Latticeè½¨è¿¹æ•°: {config.lattice.num_trajectories}, "
          f"æ¨ªå‘åç§»: {config.lattice.lateral_offsets}, "
          f"é€Ÿåº¦å˜åŒ–: {config.lattice.speed_variations}")
    print(f"è½¨è¿¹å­˜å‚¨å€æ•°: {config.matching.trajectory_storage_multiplier}x (æ•°æ®å¢å¼º)")

    # è®¾ç½®numpyéšæœºç§å­
    np.random.seed(args.seed)

    return config


def main():
    # 1. è§£æå‚æ•°
    args = parse_arguments()

    # 2. é…ç½®ç³»ç»Ÿ
    config = configure_system(args)

    # 3. è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = setup_output_dirs()

    # 4. åˆ›å»ºåˆå§‹åœºæ™¯
    scenario_manager = ScenarioManager()
    world_init = scenario_manager.create_scenario()
    scenario_state = scenario_manager.create_scenario_state(world_init)

    # 5. åˆå§‹åŒ–ç»„ä»¶
    components = initialize_components(args, world_init, output_dir)

    # 6. ç”Ÿæˆreference path
    reference_path = components['scenario_manager'].generate_reference_path(
        mode=args.ego_mode,
        horizon=config.time.default_horizon,
        ego_start=world_init.ego.position_m
    )
    print(f"\nç”ŸæˆReference Path: {len(reference_path)} ä¸ªwaypoints (mode={args.ego_mode})")

    # 6.5. Checkpointç®¡ç†å’Œæ¢å¤
    checkpoint_manager = CheckpointManager(checkpoint_dir=args.checkpoint_dir)
    start_episode = 0

    if args.resume_from:
        print(f"\nğŸ”„ ä»checkpointæ¢å¤è®­ç»ƒ...")
        checkpoint_data = checkpoint_manager.load_checkpoint(args.resume_from)

        # æ¢å¤training_state
        start_episode = checkpoint_data['training_state']['episode_id'] + 1

        # æ¢å¤ç»„ä»¶çŠ¶æ€
        components['trajectory_buffer'] = TrajectoryBuffer.from_dict(checkpoint_data['trajectory_buffer_data'])
        components['bank'] = OptimizedMultiTimestepSpatialDirichletBank.from_dict(checkpoint_data['dirichlet_bank_data'])

        # æ¢å¤QDistributionTracker
        q_tracker_data = checkpoint_data['q_tracker_data']
        components['q_tracker'].episode_data = q_tracker_data.get('episode_data', [])
        components['q_tracker'].q_value_history = q_tracker_data.get('q_value_history', [])
        components['q_tracker'].percentile_q_history = q_tracker_data.get('percentile_q_history', [])
        components['q_tracker'].collision_rate_history = q_tracker_data.get('collision_rate_history', [])
        components['q_tracker'].all_trajectories_history = q_tracker_data.get('all_trajectories_history', [])  # æ–°å¢ï¼šæ¢å¤æ‰€æœ‰è½¨è¿¹Qå€¼å†å²
        components['q_tracker'].q_distribution_history = [ep['q_distribution'] for ep in components['q_tracker'].episode_data]
        components['q_tracker'].detailed_info_history = [ep.get('detailed_info', {}) for ep in components['q_tracker'].episode_data]

        # æ›´æ–°buffer_analyzer
        components['buffer_analyzer'] = BufferAnalyzer(components['trajectory_buffer'])

        print(f"âœ… å·²æ¢å¤åˆ°Episode {start_episode}ï¼Œç»§ç»­è®­ç»ƒ...")

    # 7. è¿è¡Œæ‰€æœ‰episodes
    all_episodes, summary_frames = run_all_episodes(
        args, components, reference_path, world_init, scenario_state, output_dir,
        checkpoint_manager=checkpoint_manager, start_episode=start_episode
    )

    # 8. ç”Ÿæˆæ±‡æ€»GIF
    if summary_frames:
        VisualizationManager.generate_summary_gif(summary_frames, output_dir)
    else:
        print("\nè­¦å‘Š: æ²¡æœ‰æˆåŠŸçš„episodeï¼Œè·³è¿‡æ±‡æ€»GIFç”Ÿæˆ")

    # 9. æ‰“å°æ‘˜è¦
    if all_episodes:
        print_summary(all_episodes, components, output_dir)
    else:
        print("\nè­¦å‘Š: æ‰€æœ‰episodeéƒ½å¤±è´¥äº†")

    # 10. ä¿å­˜æœ€ç»ˆcheckpoint
    if all_episodes:
        try:
            print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆcheckpoint...")
            # å‡†å¤‡é…ç½®å­—å…¸
            config_dict = {
                'time': config.time.__dict__,
                'sampling': config.sampling.__dict__,
                'grid': config.grid.__dict__,
                'dirichlet': config.dirichlet.__dict__,
                'matching': config.matching.__dict__,
                'reward': config.reward.__dict__,
                'lattice': config.lattice.__dict__,
                'visualization': config.visualization.__dict__
            }

            checkpoint_manager.save_checkpoint(
                episode_id=args.episodes - 1,
                trajectory_buffer=components['trajectory_buffer'],
                dirichlet_bank=components['bank'],
                q_tracker=components['q_tracker'],
                config=config_dict,
                metadata={
                    'episodes_total': args.episodes,
                    'is_final': True
                }
            )
        except Exception as checkpoint_ex:
            print(f"  âš ï¸ æœ€ç»ˆcheckpointä¿å­˜å¤±è´¥: {checkpoint_ex}")


if __name__ == "__main__":
    main()
