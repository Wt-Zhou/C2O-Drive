#!/usr/bin/env python3
"""
Âü∫‰∫éLatticeËßÑÂàíÂô®ÁöÑÂ§öÊ¨°Âú∫ÊôØÊâßË°åÔºàÁÆÄÂåñÈáçÊûÑÁâàÔºâ

ËøôÊòØreplay_openloop_lattice.pyÁöÑÁÆÄÂåñÈáçÊûÑÁâàÊú¨Ôºö
- Â∞ÜÂ§çÊùÇÁöÑrun_episodeÂáΩÊï∞ÊãÜÂàÜ‰∏∫Â§ö‰∏™Áã¨Á´ãÊ®°Âùó
- ‰ΩøÁî®EpisodeContextÂ∞ÅË£ÖÂèÇÊï∞ÔºåÊèêÈ´òÂèØËØªÊÄß
- ‰øùÊåÅ‰∏éÂéüÁâàÊú¨ÂÆåÂÖ®Áõ∏ÂêåÁöÑÂäüËÉΩ

ÊºîÁ§∫latticeËΩ®ËøπËßÑÂàí‰∏éQÂÄºËØÑ‰º∞ÁªìÂêàÁöÑË¥ùÂè∂ÊñØÂ≠¶‰π†ËøáÁ®ãÔºö
- ÊØè‰∏™episode‰ΩøÁî®lattice plannerÁîüÊàêÂÄôÈÄâËΩ®Ëøπ
- ‰∏∫ÊØèÊù°ÂÄôÈÄâËΩ®ËøπËÆ°ÁÆóQÂÄº
- ‰ΩøÁî®ÁôæÂàÜ‰ΩçÂáÜÂàôÈÄâÊã©ÊúÄ‰ºòËΩ®Ëøπ
- Ë∑üË∏™QÂÄºÈöèepisodeÁöÑÊîπËøõÊÉÖÂÜµ
"""

from __future__ import annotations
import argparse
import sys
from pathlib import Path
import numpy as np
from typing import Any, Dict

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞Ë∑ØÂæÑ
_repo_root = Path(__file__).resolve().parents[2]
if str(_repo_root) not in sys.path:
    sys.path.insert(0, str(_repo_root))

from carla_c2osr.agents.c2osr.grid import GridSpec, GridMapper
from carla_c2osr.agents.c2osr.spatial_dirichlet import DirichletParams, OptimizedMultiTimestepSpatialDirichletBank
from carla_c2osr.agents.c2osr.trajectory_buffer import TrajectoryBuffer
from carla_c2osr.evaluation.buffer_analyzer import BufferAnalyzer
from carla_c2osr.evaluation.q_evaluator import QEvaluator
from carla_c2osr.evaluation.q_distribution_tracker import QDistributionTracker
from carla_c2osr.evaluation.q_value_calculator import QValueConfig
from carla_c2osr.utils.simple_trajectory_generator import SimpleTrajectoryGenerator
from carla_c2osr.utils.lattice_planner import LatticePlanner
from carla_c2osr.utils.checkpoint_manager import CheckpointManager
from carla_c2osr.env.scenario_manager import ScenarioManager
from carla_c2osr.config import get_global_config, set_global_config, ConfigPresets

# ÂØºÂÖ•ÈáçÊûÑÁöÑÊ®°Âùó
from carla_c2osr.runner.refactored import (
    EpisodeContext,
    TrajectoryEvaluator,
    TimestepExecutor,
    VisualizationManager,
    DataManager
)


def setup_output_dirs(base_dir: str = "outputs/replay_experiment") -> Path:
    """ÂàõÂª∫ËæìÂá∫ÁõÆÂΩïÁªìÊûÑ"""
    base_path = Path(base_dir)
    base_path.mkdir(parents=True, exist_ok=True)
    return base_path


def run_episode(episode_id: int,
                horizon: int,
                reference_path,
                world_init,
                grid,
                bank,
                trajectory_buffer,
                scenario_state,
                rng,
                output_dir,
                sigma: float,
                lattice_planner=None,
                q_evaluator=None,
                trajectory_generator=None,
                scenario_manager=None,
                buffer_analyzer=None,
                q_tracker=None) -> Dict[str, Any]:
    """
    ËøêË°åÂçï‰∏™episodeÔºàÁÆÄÂåñÈáçÊûÑÁâàÔºâ

    ‰ΩøÁî®Ê®°ÂùóÂåñËÆæËÆ°ÔºåÂ∞ÜÂéüÊù•492Ë°åÁöÑÂáΩÊï∞ÁÆÄÂåñÂà∞Á∫¶80Ë°å„ÄÇ
    """
    # 1. ÂàõÂª∫Episode‰∏ä‰∏ãÊñá
    ctx = EpisodeContext.create(
        episode_id=episode_id,
        horizon=horizon,
        reference_path=reference_path,
        world_init=world_init,
        grid=grid,
        bank=bank,
        trajectory_buffer=trajectory_buffer,
        scenario_state=scenario_state,
        rng=rng,
        output_dir=output_dir,
        sigma=sigma,
        lattice_planner=lattice_planner,
        q_evaluator=q_evaluator,
        trajectory_generator=trajectory_generator,
        scenario_manager=scenario_manager,
        buffer_analyzer=buffer_analyzer,
        q_tracker=q_tracker
    )

    # 2. ÁîüÊàêÂπ∂ËØÑ‰º∞ÂÄôÈÄâËΩ®Ëøπ
    evaluator = TrajectoryEvaluator(ctx)
    trajectory_q_values = evaluator.generate_and_evaluate_trajectories()

    # 3. ÈÄâÊã©ÊúÄ‰ºòËΩ®Ëøπ
    ego_trajectory, selected_trajectory_info = evaluator.select_optimal_trajectory(trajectory_q_values)

    # 4. ÂèØËßÜÂåñËΩ®ËøπÈÄâÊã©
    vis_manager = VisualizationManager(ctx)
    vis_manager.visualize_trajectory_selection(trajectory_q_values, selected_trajectory_info)

    # 5. ÁîüÊàêagentËΩ®Ëøπ
    data_manager = DataManager(ctx)
    agent_trajectories, agent_trajectory_cells = data_manager.generate_agent_trajectories()

    # 6. ÊâßË°åÊâÄÊúâÊó∂Èó¥Ê≠•
    timestep_executor = TimestepExecutor(ctx)
    episode_stats, frame_paths = timestep_executor.execute_all_timesteps(
        ego_trajectory, agent_trajectories
    )

    # 7. ÁîüÊàêepisode GIF
    gif_path = vis_manager.generate_episode_gif(frame_paths)

    # 8. Â≠òÂÇ®ËΩ®ËøπÊï∞ÊçÆÂà∞buffer
    data_manager.store_episode_trajectories(
        ego_trajectory, agent_trajectories, agent_trajectory_cells
    )

    return {
        'episode_id': episode_id,
        'frame_paths': frame_paths,
        'gif_path': gif_path,
        'stats': episode_stats,
        'selected_trajectory': selected_trajectory_info,
        'all_trajectories': trajectory_q_values
    }


def initialize_components(args, world_init, output_dir):
    """ÂàùÂßãÂåñÊâÄÊúâÁªÑ‰ª∂"""
    config = get_global_config()
    ego_start_pos = world_init.ego.position_m

    # ÂàõÂª∫ÁΩëÊ†º
    grid_spec = GridSpec(
        size_m=config.grid.grid_size_m,
        cell_m=config.grid.cell_size_m,
        macro=True
    )
    grid = GridMapper(grid_spec, world_center=ego_start_pos)

    # ÂàõÂª∫ËΩ®ËøπÁîüÊàêÂô®
    grid_half_size = grid.size_m / 2.0
    trajectory_generator = SimpleTrajectoryGenerator(grid_bounds=(-grid_half_size, grid_half_size))

    # ÂàõÂª∫Dirichlet Bank
    dirichlet_params = DirichletParams(
        alpha_in=config.dirichlet.alpha_in,
        alpha_out=config.dirichlet.alpha_out,
        delta=config.dirichlet.delta,
        cK=config.dirichlet.cK
    )
    bank = OptimizedMultiTimestepSpatialDirichletBank(grid.K, dirichlet_params, horizon=config.time.default_horizon)
    print(f"üöÄ ‰ΩøÁî®ÁªàÊûÅ‰ºòÂåñÁâàÊú¨ÁöÑDirichlet Bank - Áª¥Â∫¶Ëá™ÈÄÇÂ∫îÔºåÈõ∂ÈááÊ†∑ËÆ°ÁÆó")

    # ÂàõÂª∫ÂÖ∂‰ªñÁªÑ‰ª∂
    trajectory_buffer = TrajectoryBuffer(horizon=config.time.default_horizon)
    q_evaluator = QEvaluator()
    buffer_analyzer = BufferAnalyzer(trajectory_buffer)
    q_tracker = QDistributionTracker()
    lattice_planner = LatticePlanner.from_config(config)
    scenario_manager = ScenarioManager()

    # ÂàùÂßãÂåñagentÁöÑDirichletÂàÜÂ∏É
    for i, agent in enumerate(world_init.agents):
        agent_id = i + 1
        multi_reachable = grid.multi_timestep_successor_cells(
            agent,
            horizon=config.time.default_horizon,
            dt=config.time.dt,
            n_samples=config.sampling.reachable_set_samples
        )
        if not multi_reachable:
            current_cell = grid.world_to_cell(agent.position_m)
            multi_reachable = {t: [current_cell] for t in range(1, config.time.default_horizon + 1)}
        bank.init_agent(agent_id, multi_reachable)

    return {
        'grid': grid,
        'bank': bank,
        'trajectory_buffer': trajectory_buffer,
        'trajectory_generator': trajectory_generator,
        'q_evaluator': q_evaluator,
        'buffer_analyzer': buffer_analyzer,
        'q_tracker': q_tracker,
        'lattice_planner': lattice_planner,
        'scenario_manager': scenario_manager
    }


def run_all_episodes(args, components, reference_path, world_init, scenario_state, output_dir,
                     checkpoint_manager=None, start_episode=0):
    """ËøêË°åÊâÄÊúâepisodes"""
    all_episodes = []
    summary_frames = []

    for e in range(start_episode, args.episodes):
        try:
            rng = np.random.default_rng(args.seed + e)

            print(f"\nRunning Episode {e+1}/{args.episodes}")
            config = get_global_config()
            episode_result = run_episode(
                e, config.time.default_horizon, reference_path, world_init,
                components['grid'], components['bank'], components['trajectory_buffer'],
                scenario_state, rng, output_dir, args.sigma,
                lattice_planner=components['lattice_planner'],
                q_evaluator=components['q_evaluator'],
                trajectory_generator=components['trajectory_generator'],
                scenario_manager=components['scenario_manager'],
                buffer_analyzer=components['buffer_analyzer'],
                q_tracker=components['q_tracker']
            )
            all_episodes.append(episode_result)

            # Êî∂ÈõÜÊúÄÂêé‰∏ÄÂ∏ßÁî®‰∫éÊ±áÊÄªGIF
            if episode_result['frame_paths']:
                summary_frames.append(episode_result['frame_paths'][-1])

            # ÊâìÂç∞ÂÆåÊàêÁä∂ÊÄÅ
            if episode_result['stats']:
                final_stats = episode_result['stats'][-1]
                print(f"  ÂÆåÊàê: alpha_sum={final_stats['alpha_sum']:.1f}, "
                      f"nz_cells={final_stats['nz_cells']}")

            # ÊØè10‰∏™episodeÊ∏ÖÁêÜmatplotlibÂÜÖÂ≠ò
            if (e + 1) % 10 == 0:
                import matplotlib.pyplot as plt
                plt.close('all')
                print(f"  ÂÜÖÂ≠òÊ∏ÖÁêÜ: Episode {e+1}")

            # ÂÆöÊúü‰øùÂ≠òcheckpoint
            if checkpoint_manager and args.checkpoint_interval > 0 and (e + 1) % args.checkpoint_interval == 0:
                try:
                    # ÂáÜÂ§áÈÖçÁΩÆÂ≠óÂÖ∏
                    config_dict = {
                        'time': config.time.__dict__,
                        'sampling': config.sampling.__dict__,
                        'grid': config.grid.__dict__,
                        'dirichlet': config.dirichlet.__dict__,
                        'matching': config.matching.__dict__,
                        'reward': config.reward.__dict__,
                        'lattice': config.lattice.__dict__,
                        'visualization': config.visualization.__dict__
                    }

                    checkpoint_manager.save_checkpoint(
                        episode_id=e,
                        trajectory_buffer=components['trajectory_buffer'],
                        dirichlet_bank=components['bank'],
                        q_tracker=components['q_tracker'],
                        config=config_dict,
                        metadata={
                            'episodes_total': args.episodes,
                            'checkpoint_interval': args.checkpoint_interval
                        }
                    )
                except Exception as checkpoint_ex:
                    print(f"  ‚ö†Ô∏è Checkpoint‰øùÂ≠òÂ§±Ë¥•: {checkpoint_ex}")

        except Exception as ex:
            print(f"Episode {e+1} ÊâßË°åÂ§±Ë¥•: {ex}")
            print("ÁªßÁª≠ÊâßË°å‰∏ã‰∏Ä‰∏™episode...")
            continue

    return all_episodes, summary_frames


def print_summary(all_episodes, components, output_dir):
    """ÊâìÂç∞ÊâßË°åÊëòË¶Å"""
    print(f"\n=== ÂÆåÊàê ===")
    print(f"ËæìÂá∫ÁõÆÂΩï: {output_dir}")

    # ÊâìÂç∞ËΩ®ËøπÈÄâÊã©ÊîπËøõË∂ãÂäø
    selected_trajectories = [ep['selected_trajectory'] for ep in all_episodes if ep['selected_trajectory']]

    if selected_trajectories:
        first_selected = selected_trajectories[0]
        last_selected = selected_trajectories[-1]

        q_config = QValueConfig.from_global_config()
        percentile = q_config.q_selection_percentile

        print(f"\nËΩ®ËøπÈÄâÊã©ÊîπËøõ:")
        print(f"  Á¨¨1‰∏™episode: ËΩ®Ëøπ{first_selected['trajectory_id']}")
        print(f"    Min_Q={first_selected['min_q']:.2f}, "
              f"Mean_Q={first_selected['mean_q']:.2f}, "
              f"P{int(percentile*100)}_Q={first_selected['percentile_q']:.2f}, "
              f"Á¢∞ÊíûÁéá={first_selected['collision_rate']:.3f}")
        print(f"  Á¨¨{len(selected_trajectories)}‰∏™episode: ËΩ®Ëøπ{last_selected['trajectory_id']}")
        print(f"    Min_Q={last_selected['min_q']:.2f}, "
              f"Mean_Q={last_selected['mean_q']:.2f}, "
              f"P{int(percentile*100)}_Q={last_selected['percentile_q']:.2f}, "
              f"Á¢∞ÊíûÁéá={last_selected['collision_rate']:.3f}")

        percentile_q_improvement = last_selected['percentile_q'] - first_selected['percentile_q']
        collision_rate_improvement = first_selected['collision_rate'] - last_selected['collision_rate']

        print(f"  P{int(percentile*100)}_QÊîπËøõ: {percentile_q_improvement:+.2f}, "
              f"Á¢∞ÊíûÁéáÈôç‰Ωé: {collision_rate_improvement:+.3f}")

    # ÊâìÂç∞Â≠¶‰π†Ë∂ãÂäø
    first_stats = all_episodes[0]['stats'][-1]
    last_stats = all_episodes[-1]['stats'][-1]
    print(f"\nDirichletÂ≠¶‰π†: Alpha {first_stats['alpha_sum']:.1f} -> {last_stats['alpha_sum']:.1f}, "
          f"ÈùûÈõ∂ÂçïÂÖÉ {first_stats['nz_cells']} -> {last_stats['nz_cells']}")

    # ÊâìÂç∞bufferÁªüËÆ°
    buffer_stats = components['buffer_analyzer'].get_buffer_stats()
    config = get_global_config()
    storage_multiplier = config.matching.trajectory_storage_multiplier
    actual_episodes = buffer_stats['total_episodes'] // storage_multiplier if storage_multiplier > 1 else buffer_stats['total_episodes']
    print(f"\nBuffer: {buffer_stats['total_agents']} agents, "
          f"{buffer_stats['total_episodes']} Êù°Â≠òÂÇ®ËÆ∞ÂΩï (ÂÆûÈôÖ{actual_episodes}‰∏™episode √ó {storage_multiplier}ÂÄç), "
          f"{buffer_stats['total_agent_episodes']} agent-episodes")

    # ÁîüÊàêQÂÄºÂàÜÂ∏ÉÂèØËßÜÂåñ
    q_tracker = components['q_tracker']
    if len(q_tracker.q_value_history) > 0:
        q_evolution_path = output_dir / "q_distribution_evolution.png"
        collision_rate_path = output_dir / "collision_rate_evolution.png"
        q_data_path = output_dir / "q_distribution_data.json"

        try:
            q_tracker.plot_q_distribution_evolution(str(q_evolution_path))
            q_tracker.plot_collision_rate_evolution(str(collision_rate_path))
            q_tracker.save_data(str(q_data_path))

            print(f"\nÂèØËßÜÂåñÂ∑≤ÁîüÊàê: {q_evolution_path.name}, {collision_rate_path.name}, {q_data_path.name}")

        except Exception as e:
            print(f"Ë≠¶Âëä: QÂÄºÂàÜÂ∏ÉÂèØËßÜÂåñÂ§±Ë¥•: {e}")
    else:
        print(f"\nË≠¶Âëä: Ê≤°ÊúâQÂÄºÊï∞ÊçÆ")


def parse_arguments():
    """Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞"""
    parser = argparse.ArgumentParser(description="Âü∫‰∫éLatticeËßÑÂàíÂô®ÁöÑQÂÄº‰ºòÂåñÂÆûÈ™åÔºàÁÆÄÂåñÈáçÊûÑÁâàÔºâ")

    # Âü∫Êú¨ËøêË°åÂèÇÊï∞
    parser.add_argument("--episodes", type=int, default=20, help="ÊâßË°åepisodeÊï∞")
    parser.add_argument("--seed", type=int, default=2025, help="ÈöèÊú∫ÁßçÂ≠ê")
    parser.add_argument("--gif-fps", type=int, default=2, help="GIFÂ∏ßÁéá")
    parser.add_argument("--ego-mode", choices=["straight", "fixed-traj"],
                       default="straight", help="Ëá™ËΩ¶ËøêÂä®Ê®°Âºè")
    parser.add_argument("--sigma", type=float, default=0.5, help="ËΩØËÆ°Êï∞Ê†∏ÂÆΩÂ∫¶")

    # CheckpointÂèÇÊï∞
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpoint‰øùÂ≠òÁõÆÂΩï")
    parser.add_argument("--checkpoint-interval", type=int, default=0, help="Checkpoint‰øùÂ≠òÈó¥ÈöîÔºàÊØèN‰∏™episodeÔºå0Ë°®Á§∫‰∏çÂÆöÊúü‰øùÂ≠òÔºâ")
    parser.add_argument("--resume-from", type=str, help="‰ªéÊåáÂÆöcheckpointÊÅ¢Â§çËÆ≠ÁªÉ")

    # ÈÖçÁΩÆÈ¢ÑËÆæÂèÇÊï∞
    parser.add_argument("--config-preset", choices=["default", "fast", "high-precision", "long-horizon"],
                       default="default", help="È¢ÑËÆæÈÖçÁΩÆÊ®°Êùø")

    # ÂèØÈÄâË¶ÜÁõñÂèÇÊï∞
    parser.add_argument("--dt", type=float, help="Ë¶ÜÁõñÊó∂Èó¥Ê≠•ÈïøÔºàÁßíÔºâ")
    parser.add_argument("--horizon", type=int, help="Ë¶ÜÁõñÈ¢ÑÊµãÊó∂Èó¥Ê≠•Êï∞")
    parser.add_argument("--reachable-samples", type=int, help="Ë¶ÜÁõñÂèØËææÈõÜÈááÊ†∑Êï∞Èáè")
    parser.add_argument("--q-samples", type=int, help="Ë¶ÜÁõñQÂÄºÈááÊ†∑Êï∞Èáè")

    return parser.parse_args()


def configure_system(args):
    """ÈÖçÁΩÆÁ≥ªÁªüÂèÇÊï∞"""
    # Â∫îÁî®È¢ÑËÆæÈÖçÁΩÆ
    if args.config_preset == "fast":
        config = ConfigPresets.fast_testing()
    elif args.config_preset == "high-precision":
        config = ConfigPresets.high_precision()
    elif args.config_preset == "long-horizon":
        config = ConfigPresets.long_horizon()
    else:
        config = get_global_config()

    # Ë¶ÜÁõñÁâπÂÆöÂèÇÊï∞
    if args.dt is not None:
        config.time.dt = args.dt
    if args.horizon is not None:
        config.time.default_horizon = args.horizon
    if args.reachable_samples is not None:
        config.sampling.reachable_set_samples = args.reachable_samples
    if args.q_samples is not None:
        config.sampling.q_value_samples = args.q_samples

    # ËÆæÁΩÆËøêË°åÂèÇÊï∞
    config.random_seed = args.seed
    config.visualization.gif_fps = args.gif_fps

    set_global_config(config)

    # ÊâìÂç∞ÈÖçÁΩÆ‰ø°ÊÅØ
    print(f"=== LatticeËßÑÂàíÂô® + QÂÄº‰ºòÂåñÂÆûÈ™åÔºàÁÆÄÂåñÈáçÊûÑÁâàÔºâ ===")
    print(f"Episodes: {args.episodes}, Horizon: {config.time.default_horizon}")
    print(f"Ego mode: {args.ego_mode}, Sigma: {args.sigma}")
    print(f"Seed: {args.seed}")
    print(f"ÈÖçÁΩÆÈ¢ÑËÆæ: {args.config_preset}")
    print(f"Êó∂Èó¥Ê≠•Èïø: {config.time.dt}s, È¢ÑÊµãÊó∂Èó¥: {config.time.horizon_seconds:.1f}s")
    print(f"ÂèØËææÈõÜÈááÊ†∑: {config.sampling.reachable_set_samples}, QÂÄºÈááÊ†∑: {config.sampling.q_value_samples}")
    print(f"LatticeËΩ®ËøπÊï∞: {config.lattice.num_trajectories}, "
          f"Ê®™ÂêëÂÅèÁßª: {config.lattice.lateral_offsets}, "
          f"ÈÄüÂ∫¶ÂèòÂåñ: {config.lattice.speed_variations}")
    print(f"ËΩ®ËøπÂ≠òÂÇ®ÂÄçÊï∞: {config.matching.trajectory_storage_multiplier}x (Êï∞ÊçÆÂ¢ûÂº∫)")

    # ËÆæÁΩÆnumpyÈöèÊú∫ÁßçÂ≠ê
    np.random.seed(args.seed)

    return config


def main():
    # 1. Ëß£ÊûêÂèÇÊï∞
    args = parse_arguments()

    # 2. ÈÖçÁΩÆÁ≥ªÁªü
    config = configure_system(args)

    # 3. ËÆæÁΩÆËæìÂá∫ÁõÆÂΩï
    output_dir = setup_output_dirs()

    # 4. ÂàõÂª∫ÂàùÂßãÂú∫ÊôØ
    scenario_manager = ScenarioManager()
    world_init = scenario_manager.create_scenario()
    scenario_state = scenario_manager.create_scenario_state(world_init)

    # 5. ÂàùÂßãÂåñÁªÑ‰ª∂
    components = initialize_components(args, world_init, output_dir)

    # 6. ÁîüÊàêreference path
    reference_path = components['scenario_manager'].generate_reference_path(
        mode=args.ego_mode,
        horizon=config.time.default_horizon,
        ego_start=world_init.ego.position_m
    )
    print(f"\nÁîüÊàêReference Path: {len(reference_path)} ‰∏™waypoints (mode={args.ego_mode})")

    # 6.5. CheckpointÁÆ°ÁêÜÂíåÊÅ¢Â§ç
    checkpoint_manager = CheckpointManager(checkpoint_dir=args.checkpoint_dir)
    start_episode = 0

    if args.resume_from:
        print(f"\nüîÑ ‰ªécheckpointÊÅ¢Â§çËÆ≠ÁªÉ...")
        checkpoint_data = checkpoint_manager.load_checkpoint(args.resume_from)

        # ÊÅ¢Â§çtraining_state
        start_episode = checkpoint_data['training_state']['episode_id'] + 1

        # ÊÅ¢Â§çÁªÑ‰ª∂Áä∂ÊÄÅ
        components['trajectory_buffer'] = TrajectoryBuffer.from_dict(checkpoint_data['trajectory_buffer_data'])
        components['bank'] = OptimizedMultiTimestepSpatialDirichletBank.from_dict(checkpoint_data['dirichlet_bank_data'])

        # ÊÅ¢Â§çQDistributionTracker
        q_tracker_data = checkpoint_data['q_tracker_data']
        components['q_tracker'].episode_data = q_tracker_data.get('episode_data', [])
        components['q_tracker'].q_value_history = q_tracker_data.get('q_value_history', [])
        components['q_tracker'].percentile_q_history = q_tracker_data.get('percentile_q_history', [])
        components['q_tracker'].collision_rate_history = q_tracker_data.get('collision_rate_history', [])
        components['q_tracker'].q_distribution_history = [ep['q_distribution'] for ep in components['q_tracker'].episode_data]
        components['q_tracker'].detailed_info_history = [ep.get('detailed_info', {}) for ep in components['q_tracker'].episode_data]

        # Êõ¥Êñ∞buffer_analyzer
        components['buffer_analyzer'] = BufferAnalyzer(components['trajectory_buffer'])

        print(f"‚úÖ Â∑≤ÊÅ¢Â§çÂà∞Episode {start_episode}ÔºåÁªßÁª≠ËÆ≠ÁªÉ...")

    # 7. ËøêË°åÊâÄÊúâepisodes
    all_episodes, summary_frames = run_all_episodes(
        args, components, reference_path, world_init, scenario_state, output_dir,
        checkpoint_manager=checkpoint_manager, start_episode=start_episode
    )

    # 8. ÁîüÊàêÊ±áÊÄªGIF
    if summary_frames:
        VisualizationManager.generate_summary_gif(summary_frames, output_dir)
    else:
        print("\nË≠¶Âëä: Ê≤°ÊúâÊàêÂäüÁöÑepisodeÔºåË∑≥ËøáÊ±áÊÄªGIFÁîüÊàê")

    # 9. ÊâìÂç∞ÊëòË¶Å
    if all_episodes:
        print_summary(all_episodes, components, output_dir)
    else:
        print("\nË≠¶Âëä: ÊâÄÊúâepisodeÈÉΩÂ§±Ë¥•‰∫Ü")

    # 10. ‰øùÂ≠òÊúÄÁªàcheckpoint
    if all_episodes:
        try:
            print(f"\nüíæ ‰øùÂ≠òÊúÄÁªàcheckpoint...")
            # ÂáÜÂ§áÈÖçÁΩÆÂ≠óÂÖ∏
            config_dict = {
                'time': config.time.__dict__,
                'sampling': config.sampling.__dict__,
                'grid': config.grid.__dict__,
                'dirichlet': config.dirichlet.__dict__,
                'matching': config.matching.__dict__,
                'reward': config.reward.__dict__,
                'lattice': config.lattice.__dict__,
                'visualization': config.visualization.__dict__
            }

            checkpoint_manager.save_checkpoint(
                episode_id=args.episodes - 1,
                trajectory_buffer=components['trajectory_buffer'],
                dirichlet_bank=components['bank'],
                q_tracker=components['q_tracker'],
                config=config_dict,
                metadata={
                    'episodes_total': args.episodes,
                    'is_final': True
                }
            )
        except Exception as checkpoint_ex:
            print(f"  ‚ö†Ô∏è ÊúÄÁªàcheckpoint‰øùÂ≠òÂ§±Ë¥•: {checkpoint_ex}")


if __name__ == "__main__":
    main()
