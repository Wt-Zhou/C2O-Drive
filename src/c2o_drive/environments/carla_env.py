"""CARLAä»¿çœŸç¯å¢ƒçš„Gymnasiumå°è£…

å°†CARLAä»¿çœŸå™¨å°è£…ä¸ºæ ‡å‡†Gymç¯å¢ƒæ¥å£ï¼Œä½¿å…¶å¯ä»¥ä¸ç®—æ³•æ— ç¼é›†æˆã€‚
"""

from __future__ import annotations
from typing import Dict, Any, Tuple, Optional, List, TYPE_CHECKING
import numpy as np

from c2o_drive.core.environment import DrivingEnvironment, StepResult, Box
from c2o_drive.environments.carla.types import WorldState, EgoControl
from c2o_drive.environments.rewards import RewardFunction, create_default_reward
from c2o_drive.environments.carla.scenarios import (
    CarlaScenarioLibrary,
    ScenarioDefinition,
)

if TYPE_CHECKING:
    from c2o_drive.environments.carla.carla_scenario_1 import CarlaSimulator


class CarlaEnvironment(DrivingEnvironment[WorldState, EgoControl]):
    """CARLAä»¿çœŸç¯å¢ƒçš„Gymå°è£…

    å°è£…CarlaSimulatorï¼Œæä¾›æ ‡å‡†çš„reset/stepæ¥å£ã€‚
    """

    def __init__(
        self,
        host: str = 'localhost',
        port: int = 2000,
        town: str = 'Town03',
        dt: float = 0.1,
        max_episode_steps: int = 500,
        reward_fn: Optional[RewardFunction] = None,
        num_vehicles: int = 10,
        num_pedestrians: int = 5,
        no_rendering: bool = False,
        sim_dt: float = 0.1,
    ):
        """åˆå§‹åŒ–CARLAç¯å¢ƒ

        Args:
            host: CARLAæœåŠ¡å™¨åœ°å€
            port: CARLAæœåŠ¡å™¨ç«¯å£
            town: åœ°å›¾åç§°
            dt: æ—¶é—´æ­¥é•¿
            max_episode_steps: æœ€å¤§æ­¥æ•°
            reward_fn: å¥–åŠ±å‡½æ•°ï¼ˆå¯é€‰ï¼‰
            num_vehicles: ç¯å¢ƒè½¦è¾†æ•°
            num_pedestrians: è¡Œäººæ•°
        """
        self.host = host
        self.port = port
        self.town = town
        self.dt = dt
        self.max_episode_steps = max_episode_steps
        self.reward_fn = reward_fn or create_default_reward()
        self.num_vehicles = num_vehicles
        self.num_pedestrians = num_pedestrians
        self.no_rendering = no_rendering
        self.sim_dt = sim_dt
        self._substeps = max(1, int(round(self.dt / self.sim_dt)))
        self._substeps = max(1, self._substeps)

        # åˆå§‹åŒ–CARLAä»¿çœŸå™¨
        self.simulator: Optional[CarlaSimulator] = None
        self._current_state: Optional[WorldState] = None
        self._step_count = 0
        self._episode_reward = 0.0

        # è½¨è¿¹è®°å½•
        self._episode_trajectory: List[Dict[str, Any]] = []
        self._previous_action: Optional[EgoControl] = None

    def _ensure_connected(self):
        """ç¡®ä¿ä¸CARLAæœåŠ¡å™¨è¿æ¥"""
        if self.simulator is None:
            # Lazy import to avoid loading CARLA dependencies at module import time
            from c2o_drive.environments.carla.simulator import CarlaSimulator
            self.simulator = CarlaSimulator(
                host=self.host,
                port=self.port,
                town=self.town,
                dt=self.sim_dt,
                no_rendering=self.no_rendering,
            )

    def reset(
        self,
        seed: Optional[int] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Tuple[WorldState, Dict[str, Any]]:
        """é‡ç½®ç¯å¢ƒå¹¶å¼€å§‹æ–°episode

        Args:
            seed: éšæœºç§å­
            options: é¢å¤–é€‰é¡¹ï¼Œå¯åŒ…å«ï¼š
                - 'scenario_config': åœºæ™¯é…ç½®
                - 'reference_path': å‚è€ƒè·¯å¾„

        Returns:
            (åˆå§‹WorldState, infoå­—å…¸)
        """
        if seed is not None:
            np.random.seed(seed)

        # ç¡®ä¿è¿æ¥
        self._ensure_connected()

        # åˆ›å»ºåœºæ™¯ï¼ˆcreate_scenarioå†…éƒ¨ä¼šè‡ªåŠ¨è°ƒç”¨cleanupï¼‰
        options = options or {}
        scenario_config = options.get('scenario_config', {})
        scenario_def = scenario_config.get('scenario')
        scenario_name = scenario_config.get('scenario_name')

        if isinstance(scenario_def, str):
            scenario_def = CarlaScenarioLibrary.get_scenario(scenario_def)
        if scenario_def is None and scenario_name:
            scenario_def = CarlaScenarioLibrary.get_scenario(scenario_name)

        if scenario_def is not None:
            ego_spawn = CarlaScenarioLibrary.spawn_to_transform(scenario_def.ego_spawn)
            agent_spawns = [
                CarlaScenarioLibrary.spawn_to_transform(spawn)
                for spawn in scenario_def.agent_spawns
            ]
            autopilot = scenario_def.autopilot
        else:
            # ä½¿ç”¨é»˜è®¤ scenario è€Œä¸æ˜¯ç©ºåœºæ™¯
            scenario_def = CarlaScenarioLibrary.get_scenario('s4_wrong_way')
            ego_spawn = CarlaScenarioLibrary.spawn_to_transform(scenario_def.ego_spawn)
            agent_spawns = [
                CarlaScenarioLibrary.spawn_to_transform(spawn)
                for spawn in scenario_def.agent_spawns
            ]
            autopilot = scenario_def.autopilot

        # ä½¿ç”¨simulatoråˆ›å»ºåœºæ™¯
        # ä¼ é€’scenarioçš„metadataä»¥æ”¯æŒä¸åŒç±»å‹çš„agentï¼ˆå¦‚è‡ªè¡Œè½¦ï¼‰
        metadata = scenario_def.metadata if scenario_def is not None else None
        self._current_state = self.simulator.create_scenario(
            ego_spawn=ego_spawn,
            agent_spawns=agent_spawns,
            agent_autopilot=autopilot,
            metadata=metadata,
        )

        self._step_count = 0
        self._episode_reward = 0.0
        self._episode_trajectory = []  # æ¸…ç©ºè½¨è¿¹è®°å½•
        self._previous_action = None

        # ä¿å­˜åˆå§‹ä½ç½®å’Œæœå‘ï¼Œç”¨äºè®¡ç®—ä¸­å¿ƒçº¿åç¦»
        # ego_spawnæ ¼å¼: (x, y, z, yaw)
        self._initial_ego_spawn = scenario_def.ego_spawn if scenario_def else (0, 0, 0, 0)
        self._initial_yaw = self._initial_ego_spawn[3]  # yawè§’åº¦ï¼ˆåº¦ï¼‰

        # å­˜å‚¨åœºæ™¯åç§°å’Œé¢„å®šä¹‰çš„agentè½¨è¿¹ï¼ˆå¦‚æœæœ‰ï¼‰
        self._scenario_name = scenario_def.name if scenario_def else None
        self._agent_trajectories = None
        self._agent_initial_z = {}  # å­˜å‚¨æ¯ä¸ªagentçš„åˆå§‹Zåæ ‡ï¼ˆç”¨äºä¿æŒé«˜åº¦ï¼‰
        if scenario_def is not None and scenario_def.metadata is not None:
            self._agent_trajectories = scenario_def.metadata.get('agent_trajectories')

        # è®°å½•æ‰€æœ‰agentçš„åˆå§‹Zåæ ‡
        if hasattr(self.simulator, 'env_vehicles'):
            for i, vehicle in enumerate(self.simulator.env_vehicles):
                try:
                    self._agent_initial_z[i] = vehicle.get_location().z
                except:
                    self._agent_initial_z[i] = 0.5  # é»˜è®¤åœ°é¢é«˜åº¦

        reference_path = options.get('reference_path')
        if reference_path is None and scenario_def is not None:
            reference_path = CarlaScenarioLibrary.get_reference_path(
                scenario_def,
                horizon=self.max_episode_steps,
                dt=self.dt,
            )

        info = {
            'town': self.town,
            'episode': 0,
            'reference_path': reference_path,
            'scenario': scenario_def.name if scenario_def else 'default',
        }

        return self._current_state, info

    def step(self, action: EgoControl) -> StepResult[WorldState]:
        """æ‰§è¡Œä¸€æ­¥ä»¿çœŸ

        Args:
            action: æ§åˆ¶åŠ¨ä½œ

        Returns:
            StepResultåŒ…å«ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ç­‰
        """
        if self._current_state is None:
            raise RuntimeError("Must call reset() before step()")

        # æ‰§è¡ŒåŠ¨ä½œï¼Œè€ƒè™‘CARLAå†…éƒ¨æ›´å°çš„æ—¶é—´æ­¥
        next_state = self._current_state
        collision_occurred = False

        # å¦‚æœæœ‰é¢„å®šä¹‰çš„agentè½¨è¿¹ï¼Œæ‰§è¡Œå®ƒä»¬
        if self._agent_trajectories is not None:
            for agent_idx, trajectory in self._agent_trajectories.items():
                if agent_idx < len(self.simulator.env_vehicles) and self._step_count < len(trajectory) - 1:
                    # è·å–å½“å‰å’Œä¸‹ä¸€ä¸ªä½ç½®
                    current_pos = trajectory[self._step_count]
                    next_pos = trajectory[self._step_count + 1]

                    # ä¸ºè¯¥è½¦è¾†è®¾ç½®ç›®æ ‡é€Ÿåº¦å‘é‡
                    vehicle = self.simulator.env_vehicles[agent_idx]

                    # è·³è¿‡é™æ€éšœç¢ç‰©
                    if 'static' in vehicle.type_id.lower() or 'prop' in vehicle.type_id.lower():
                        continue

                    # åˆ¤æ–­æ˜¯å¦æ˜¯è¡Œäºº
                    is_walker = 'walker' in vehicle.type_id.lower() or 'pedestrian' in vehicle.type_id.lower()

                    if is_walker:
                        # è¡Œäººä½¿ç”¨ç›´æ¥ä½ç½®å’Œé€Ÿåº¦æ§åˆ¶ï¼ˆä¸ä½¿ç”¨å¯¼èˆªç³»ç»Ÿï¼‰
                        dx = next_pos[0] - current_pos[0]
                        dy = next_pos[1] - current_pos[1]

                        # è®¡ç®—é€Ÿåº¦
                        vx = dx / self.dt
                        vy = dy / self.dt

                        try:
                            from carla import Vector3D, Transform, Location, Rotation
                            import math

                            # è®¡ç®—æœå‘
                            if abs(dx) > 0.001 or abs(dy) > 0.001:
                                target_yaw = math.degrees(math.atan2(dy, dx))
                            else:
                                # å¦‚æœå‡ ä¹ä¸åŠ¨ï¼Œä¿æŒå½“å‰æœå‘
                                target_yaw = vehicle.get_transform().rotation.yaw

                            # ç›´æ¥è®¾ç½®ä½ç½®å’Œæœå‘
                            # å¦‚æœè½¨è¿¹ç‚¹åŒ…å«Zåæ ‡ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤åœ°é¢é«˜åº¦
                            z_height = next_pos[2] if len(next_pos) >= 3 else 0.5
                            new_location = Location(x=next_pos[0], y=next_pos[1], z=z_height)
                            new_rotation = Rotation(yaw=target_yaw, pitch=0, roll=0)
                            new_transform = Transform(new_location, new_rotation)
                            vehicle.set_transform(new_transform)

                        except Exception as e:
                            pass
                    else:
                        # è½¦è¾†ä½¿ç”¨ç›´æ¥ä½ç½®æ§åˆ¶ï¼ˆç±»ä¼¼è¡Œäººï¼‰
                        dx = next_pos[0] - current_pos[0]
                        dy = next_pos[1] - current_pos[1]

                        # è½¨è¿¹æ§åˆ¶å·²éªŒè¯æ­£å¸¸å·¥ä½œ
                        # if agent_idx == 0 and self._step_count < 3:
                        #     actual_loc = vehicle.get_location()
                        #     print(f"ğŸš— Agent {agent_idx} è½¨è¿¹æ§åˆ¶: step={self._step_count}, "
                        #           f"å®é™…ä½ç½®=({actual_loc.x:.1f}, {actual_loc.y:.1f})")

                        # è®¡ç®—æœå‘
                        import math
                        if abs(dx) > 0.01 or abs(dy) > 0.01:
                            target_yaw = math.degrees(math.atan2(dy, dx))

                            # ç›´æ¥è®¾ç½®ä½ç½®å’Œæœå‘ï¼ˆç¡®ä¿è½¦è¾†çœŸçš„ç§»åŠ¨ï¼‰
                            try:
                                from carla import Transform, Location, Rotation
                                current_transform = vehicle.get_transform()

                                # å¦‚æœè½¨è¿¹ç‚¹åŒ…å«Zåæ ‡ï¼ˆ3å…ƒç»„ï¼‰ï¼Œä½¿ç”¨å®ƒ
                                # å¦åˆ™ä½¿ç”¨å½“å‰Zåæ ‡ï¼Œè®©è½¦è¾†è·Ÿéšåœ°é¢ï¼ˆé¿å…åœ¨ä¸‹å¡æ—¶æ‚¬ç©ºï¼‰
                                if len(next_pos) >= 3:
                                    z_height = next_pos[2]
                                else:
                                    z_height = current_transform.location.z

                                # ç›´æ¥è®¾ç½®åˆ°ç›®æ ‡ä½ç½®
                                new_location = Location(x=next_pos[0], y=next_pos[1], z=z_height)
                                # ä¿æŒå½“å‰pitchï¼ˆé€‚åº”åœ°å½¢å¡åº¦ï¼‰ï¼Œåªè®¾ç½®yawå’Œroll=0
                                new_rotation = Rotation(
                                    pitch=current_transform.rotation.pitch,  # ä¿æŒå½“å‰pitchï¼Œé€‚åº”åœ°å½¢
                                    yaw=target_yaw,
                                    roll=0.0   # ä¿æŒæ°´å¹³ï¼Œé¿å…ä¾§ç¿»
                                )
                                new_transform = Transform(new_location, new_rotation)
                                vehicle.set_transform(new_transform)
                            except Exception as e:
                                print(f"âš ï¸ è®¾ç½®è½¦è¾†{agent_idx}ä½ç½®å¤±è´¥: {e}")
                                pass

        for _ in range(self._substeps):
            next_state = self.simulator.step(action)
            if self._check_collision(next_state):
                collision_occurred = True
                break

        # æ£€æµ‹ç»ˆæ­¢æ¡ä»¶
        terminated = collision_occurred or self._check_collision(next_state)
        truncated = (self._step_count >= self.max_episode_steps - 1)

        # è®¡ç®—åŠ¨åŠ›å­¦ä¿¡æ¯ï¼ˆç”¨äºå¥–åŠ±å’Œinfoï¼‰- å¿…é¡»åœ¨rewardè®¡ç®—ä¹‹å‰
        acceleration = self._calculate_acceleration(action, next_state)
        jerk = self._calculate_jerk(action)

        # è·å–ç¢°æ’ä¼ æ„Ÿå™¨çŠ¶æ€
        collision_sensor_triggered = self.simulator.is_collision_occurred() if self.simulator else False

        # æ£€æµ‹near-missï¼ˆä½¿ç”¨OBBæ‰©å¤§1ç±³æ£€æµ‹ï¼‰
        near_miss_detected = False
        min_distance_to_agents = float('inf')
        if self.simulator:
            buffer_m = 1.0  # OBBæ‰©å±•è·ç¦»ï¼šè½¦è¾†å°ºå¯¸+1ç±³buffer
            near_miss_detected, min_distance_to_agents = self.simulator.check_near_miss(buffer_m)

        # è®¡ç®—ä¸­å¿ƒçº¿åç¦»å’Œå‰è¿›è·ç¦»ï¼ˆæ ¹æ®åˆå§‹yawåˆ¤æ–­æ²¿å“ªä¸ªè½´ç§»åŠ¨ï¼‰
        # CARLAåæ ‡ç³»: yaw=0Â°æœä¸œ(+X), 90Â°æœå—(+Y), 180Â°æœè¥¿(-X), -90Â°/270Â°æœåŒ—(-Y)
        ego_x, ego_y = next_state.ego.position_m
        prev_x, prev_y = self._current_state.ego.position_m
        init_x, init_y = self._initial_ego_spawn[0], self._initial_ego_spawn[1]
        yaw = self._initial_yaw

        # åˆ¤æ–­ä¸»è¦ç§»åŠ¨æ–¹å‘ï¼Œè®¡ç®—æ¨ªå‘åç¦»å’Œå‰è¿›è·ç¦»
        # yawæ¥è¿‘0Â°ï¼šæœä¸œ(+X)ï¼Œå‰è¿›=dxï¼Œåç¦»=|dy|
        # yawæ¥è¿‘90Â°ï¼šæœå—(+Y)ï¼Œå‰è¿›=dyï¼Œåç¦»=|dx|
        # yawæ¥è¿‘180Â°ï¼šæœè¥¿(-X)ï¼Œå‰è¿›=-dxï¼Œåç¦»=|dy|
        # yawæ¥è¿‘-90Â°ï¼šæœåŒ—(-Y)ï¼Œå‰è¿›=-dyï¼Œåç¦»=|dx|
        if abs(yaw) < 45:
            # æœä¸œ(+X)
            lateral_deviation = abs(ego_y - init_y)
            forward_progress = ego_x - prev_x
        elif abs(yaw) > 135:
            # æœè¥¿(-X)
            lateral_deviation = abs(ego_y - init_y)
            forward_progress = prev_x - ego_x
        elif yaw > 0:
            # æœå—(+Y)ï¼Œyawåœ¨45Â°~135Â°ä¹‹é—´
            lateral_deviation = abs(ego_x - init_x)
            forward_progress = ego_y - prev_y
        else:
            # æœåŒ—(-Y)ï¼Œyawåœ¨-135Â°~-45Â°ä¹‹é—´
            lateral_deviation = abs(ego_x - init_x)
            forward_progress = prev_y - ego_y

        # æ„å»ºinfoå­—å…¸ - å¿…é¡»åœ¨rewardè®¡ç®—ä¹‹å‰ä¼ å…¥
        info = {
            'collision': terminated,
            'collision_sensor': collision_sensor_triggered,
            'near_miss': near_miss_detected,
            'min_distance_to_agents': min_distance_to_agents,
            'step': self._step_count,
            'acceleration': acceleration,
            'jerk': jerk,
            'lateral_deviation': lateral_deviation,
            'forward_progress': forward_progress,
        }

        # è®¡ç®—å¥–åŠ±ï¼ˆä¼ å…¥infoå­—å…¸ï¼‰
        reward = self._calculate_reward(
            self._current_state,
            action,
            next_state,
            info,
        )

        # æ›´æ–°çŠ¶æ€
        self._current_state = next_state
        self._step_count += 1
        self._episode_reward += reward

        # æ›´æ–°infoä¸­çš„episode_reward
        info['episode_reward'] = self._episode_reward

        # è®°å½•è½¨è¿¹
        self._episode_trajectory.append({
            'step': self._step_count - 1,
            'state': next_state,
            'action': action,
            'reward': reward,
            'terminated': terminated,
            'truncated': truncated,
            'acceleration': acceleration,
            'jerk': jerk,
        })
        self._previous_action = action

        return StepResult(
            observation=next_state,
            reward=reward,
            terminated=terminated,
            truncated=truncated,
            info=info,
        )


    def _calculate_reward(
        self,
        state: WorldState,
        action: EgoControl,
        next_state: WorldState,
        info: dict,
    ) -> float:
        """è®¡ç®—å¥–åŠ±"""
        return self.reward_fn.compute(state, action, next_state, info)

    def _check_collision(self, state: WorldState) -> bool:
        """æ£€æµ‹æ˜¯å¦å‘ç”Ÿç¢°æ’

        ä½¿ç”¨CARLAç¢°æ’ä¼ æ„Ÿå™¨æ•°æ®ã€‚
        """
        # ä½¿ç”¨CARLAç¢°æ’ä¼ æ„Ÿå™¨
        if self.simulator and self.simulator.is_collision_occurred():
            print(f"âš ï¸ ç¢°æ’ä¼ æ„Ÿå™¨è§¦å‘ï¼è‡ªè½¦ä½ç½®: {state.ego.position_m}")
            return True

        # # å¤‡ä»½ï¼šç®€å•è·ç¦»æ£€æµ‹ï¼ˆæ ¹æ®agentç±»å‹ä½¿ç”¨ä¸åŒé˜ˆå€¼ï¼‰
        # ego_pos = np.array(state.ego.position_m)
        #
        # # å¯¼å…¥AgentTypeæšä¸¾
        # from c2o_drive.core.types import AgentType
        #
        # for agent in state.agents:
        #     agent_pos = np.array(agent.position_m)
        #     distance = np.linalg.norm(ego_pos - agent_pos)
        #
        #     # æ ¹æ®agentç±»å‹è®¾ç½®ä¸åŒçš„ç¢°æ’é˜ˆå€¼
        #     if agent.agent_type == AgentType.BICYCLE:
        #         collision_threshold = 2.0  # è‡ªè¡Œè½¦ï¼šè¾ƒå°çš„ç¢°æ’è·ç¦»
        #     elif agent.agent_type == AgentType.PEDESTRIAN:
        #         collision_threshold = 1.5  # è¡Œäººï¼šæœ€å°çš„ç¢°æ’è·ç¦»
        #     elif agent.agent_type == AgentType.MOTORCYCLE:
        #         collision_threshold = 2.5  # æ‘©æ‰˜è½¦ï¼šä¸­ç­‰ç¢°æ’è·ç¦»
        #     elif agent.agent_type == AgentType.OBSTACLE:
        #         collision_threshold = 0.5  # é™æ€éšœç¢ç‰©ï¼ˆé”¥æ¡¶ã€ç®±å­ï¼‰
        #     else:  # VEHICLE
        #         collision_threshold = 3.5  # æ±½è½¦ï¼šè¾ƒå¤§çš„ç¢°æ’è·ç¦»
        #
        #     if distance < collision_threshold:
        #         print(f"âš ï¸ è·ç¦»ç¢°æ’æ£€æµ‹è§¦å‘ï¼")
        #         print(f"   è‡ªè½¦ä½ç½®: {state.ego.position_m}")
        #         print(f"   {agent.agent_type.value}ä½ç½®: {agent.position_m}")
        #         print(f"   è·ç¦»: {distance:.2f}m < é˜ˆå€¼: {collision_threshold}m")
        #         return True

        return False

    def _calculate_acceleration(self, action: EgoControl, next_state: WorldState) -> float:
        """è®¡ç®—åŠ é€Ÿåº¦ï¼ˆç®€åŒ–ä¼°è®¡ï¼‰

        ä½¿ç”¨æ²¹é—¨å’Œåˆ¹è½¦æ¥ä¼°è®¡åŠ é€Ÿåº¦å¤§å°ã€‚
        """
        max_accel = 3.0  # m/s^2
        max_decel = 6.0  # m/s^2

        if action.throttle > 0:
            return action.throttle * max_accel
        elif action.brake > 0:
            return -action.brake * max_decel
        return 0.0

    def _calculate_jerk(self, action: EgoControl) -> float:
        """è®¡ç®—æ€¥åŠ¨åº¦ï¼ˆåŠ é€Ÿåº¦å˜åŒ–ç‡ï¼‰

        ç®€åŒ–ä¸ºç›¸é‚»ä¸¤ä¸ªåŠ¨ä½œä¹‹é—´çš„åŠ é€Ÿåº¦å·®å¼‚ã€‚
        """
        if self._previous_action is None:
            return 0.0

        current_accel = self._calculate_acceleration(action, self._current_state)
        previous_accel = self._calculate_acceleration(self._previous_action, self._current_state)

        # æ€¥åŠ¨åº¦ = åŠ é€Ÿåº¦å˜åŒ– / æ—¶é—´æ­¥é•¿
        jerk = abs(current_accel - previous_accel) / self.dt
        return jerk

    def get_episode_trajectory(self) -> List[Dict[str, Any]]:
        """è·å–å½“å‰episodeçš„å®Œæ•´è½¨è¿¹è®°å½•

        Returns:
            è½¨è¿¹è®°å½•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            - step: æ­¥æ•°
            - state: WorldState
            - action: EgoControl
            - reward: å¥–åŠ±å€¼
            - terminated: æ˜¯å¦ç»ˆæ­¢
            - truncated: æ˜¯å¦æˆªæ–­
            - acceleration: åŠ é€Ÿåº¦
            - jerk: æ€¥åŠ¨åº¦
        """
        return self._episode_trajectory

    @property
    def observation_space(self):
        """è§‚æµ‹ç©ºé—´ï¼ˆWorldStateæ— å›ºå®šshapeï¼Œè¿”å›Noneï¼‰"""
        return None  # WorldStateæ˜¯ç»“æ„åŒ–æ•°æ®

    @property
    def action_space(self):
        """åŠ¨ä½œç©ºé—´"""
        # EgoControl: throttle[0,1], steer[-1,1], brake[0,1]
        return Box(
            low=np.array([0.0, -1.0, 0.0]),
            high=np.array([1.0, 1.0, 1.0]),
            shape=(3,),
        )

    def render(self, mode: str = 'human'):
        """æ¸²æŸ“ï¼ˆCARLAè‡ªå¸¦æ¸²æŸ“ï¼‰"""
        pass

    def visualize_trajectory(
        self,
        save_path: Optional[str] = None,
        show_agents: bool = True,
        show_rewards: bool = True,
    ):
        """å¯è§†åŒ–episodeè½¨è¿¹

        ç”Ÿæˆmatplotlibå›¾è¡¨ï¼Œæ˜¾ç¤ºegoå’Œagentsçš„è½¨è¿¹ã€‚

        Args:
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºå›¾è¡¨
            show_agents: æ˜¯å¦æ˜¾ç¤ºagentè½¨è¿¹
            show_rewards: æ˜¯å¦æ˜¾ç¤ºå¥–åŠ±æ›²çº¿
        """
        try:
            import matplotlib.pyplot as plt
            import matplotlib.patches as patches
        except ImportError:
            print("è­¦å‘Š: matplotlibæœªå®‰è£…ï¼Œæ— æ³•ç”Ÿæˆå¯è§†åŒ–")
            return

        if len(self._episode_trajectory) == 0:
            print("è­¦å‘Š: æ²¡æœ‰è½¨è¿¹æ•°æ®å¯ä¾›å¯è§†åŒ–")
            return

        # åˆ›å»ºå­å›¾
        if show_rewards:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        else:
            fig, ax1 = plt.subplots(1, 1, figsize=(10, 10))

        # æå–è½¨è¿¹æ•°æ®
        ego_positions = []
        agent_trajectories = {}  # {agent_id: [(x, y), ...]}
        rewards = []
        collision_step = None

        for i, record in enumerate(self._episode_trajectory):
            state = record['state']

            # Egoè½¨è¿¹
            ego_positions.append(state.ego.position_m)

            # Agentè½¨è¿¹
            for agent in state.agents:
                if agent.agent_id not in agent_trajectories:
                    agent_trajectories[agent.agent_id] = []
                agent_trajectories[agent.agent_id].append(agent.position_m)

            # å¥–åŠ±
            rewards.append(record['reward'])

            # ç¢°æ’æ£€æµ‹
            if record['terminated'] and collision_step is None:
                collision_step = i

        # ç»˜åˆ¶è½¨è¿¹å›¾
        ego_positions = np.array(ego_positions)
        ego_plot = ego_positions.copy()
        ego_plot[:, 1] *= -1.0
        ax1.plot(ego_plot[:, 0], ego_plot[:, 1],
                'b-o', linewidth=2, markersize=4, label='Ego Vehicle')

        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        ax1.plot(ego_plot[0, 0], ego_plot[0, 1],
                'go', markersize=12, label='Start')
        ax1.plot(ego_plot[-1, 0], ego_plot[-1, 1],
                'rs' if collision_step is not None else 'r*',
                markersize=12, label='End (Collision)' if collision_step else 'End')

        # ç»˜åˆ¶agentè½¨è¿¹
        if show_agents:
            colors = ['orange', 'purple', 'cyan', 'magenta', 'yellow', 'pink']
            for idx, (agent_id, positions) in enumerate(agent_trajectories.items()):
                positions = np.array(positions)
                positions[:, 1] *= -1.0
                color = colors[idx % len(colors)]
                ax1.plot(positions[:, 0], positions[:, 1],
                        '--', color=color, linewidth=1.5, alpha=0.7,
                        label=f'Agent {agent_id}')

        ax1.set_xlabel('X Position (m)', fontsize=12)
        ax1.set_ylabel('Y Position (m)', fontsize=12)
        ax1.set_title('Vehicle Trajectories', fontsize=14, fontweight='bold')
        ax1.legend(loc='best')
        ax1.grid(True, alpha=0.3)
        ax1.axis('equal')

        # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
        if show_rewards and len(rewards) > 0:
            steps = np.arange(len(rewards))
            ax2.plot(steps, rewards, 'g-', linewidth=2, label='Step Reward')
            ax2.plot(steps, np.cumsum(rewards), 'b--', linewidth=2, label='Cumulative Reward')

            if collision_step is not None:
                ax2.axvline(x=collision_step, color='r', linestyle='--',
                           linewidth=2, label=f'Collision (step {collision_step})')

            ax2.set_xlabel('Step', fontsize=12)
            ax2.set_ylabel('Reward', fontsize=12)
            ax2.set_title('Reward Progression', fontsize=14, fontweight='bold')
            ax2.legend(loc='best')
            ax2.grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜æˆ–æ˜¾ç¤º
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        else:
            plt.show()

        plt.close(fig)

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if self.simulator is not None:
            # CarlaSimulatorä¼šè‡ªåŠ¨æ¸…ç†
            self.simulator = None


# Backwards compatible alias used throughout the repo
CarlaEnv = CarlaEnvironment

__all__ = ['CarlaEnvironment', 'CarlaEnv']
