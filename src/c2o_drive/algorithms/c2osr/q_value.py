"""
Qå€¼è®¡ç®—æ¨¡å— - é‡æ–°è®¾è®¡ç‰ˆæœ¬

åŸºäºå†å²è½¬ç§»æ•°æ®å’ŒDirichletåˆ†å¸ƒçš„Qå€¼è®¡ç®—ç³»ç»Ÿã€‚
"""

from __future__ import annotations
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from c2o_drive.core.types import EgoState, AgentState, WorldState, AgentType
from c2o_drive.algorithms.c2osr.grid_mapper import GridMapper
from c2o_drive.algorithms.c2osr.dirichlet import SpatialDirichletBank, DirichletParams, MultiTimestepSpatialDirichletBank, OptimizedMultiTimestepSpatialDirichletBank
from c2o_drive.algorithms.c2osr.trajectory_buffer import TrajectoryBuffer
from c2o_drive.config import get_global_config, RewardConfig
from c2o_drive.utils.collision import ShapeBasedCollisionDetector


@dataclass
class QValueConfig:
    """Qå€¼è®¡ç®—é…ç½®

    é»˜è®¤ä»å…¨å±€é…ç½®è¯»å–å‚æ•°ã€‚å¦‚æœéœ€è¦è‡ªå®šä¹‰,è¯·æ˜¾å¼ä¼ å…¥å‚æ•°å€¼ã€‚
    """
    horizon: Optional[int] = None  # é¢„æµ‹æ—¶é—´æ­¥é•¿
    n_samples: Optional[int] = None  # Dirichleté‡‡æ ·æ•°é‡
    dirichlet_alpha_in: Optional[float] = None  # å¯è¾¾é›†å†…çš„å…ˆéªŒå¼ºåº¦
    dirichlet_alpha_out: Optional[float] = None  # å¯è¾¾é›†å¤–çš„å…ˆéªŒå¼ºåº¦
    learning_rate: Optional[float] = None  # å†å²æ•°æ®æ›´æ–°å­¦ä¹ ç‡
    q_selection_percentile: Optional[float] = None  # Qå€¼é€‰æ‹©ç™¾åˆ†ä½æ•°ï¼ˆä»å…¨å±€é…ç½®è¯»å–ï¼‰
    gamma: Optional[float] = None  # æŠ˜æ‰£å› å­

    def __post_init__(self):
        """ä»å…¨å±€é…ç½®è¯»å–æœªè®¾ç½®çš„å‚æ•°"""
        global_config = get_global_config()
        if self.horizon is None:
            self.horizon = global_config.time.default_horizon
        if self.n_samples is None:
            self.n_samples = global_config.sampling.q_value_samples
        if self.dirichlet_alpha_in is None:
            self.dirichlet_alpha_in = global_config.dirichlet.alpha_in
        if self.dirichlet_alpha_out is None:
            self.dirichlet_alpha_out = global_config.dirichlet.alpha_out
        if self.learning_rate is None:
            self.learning_rate = global_config.dirichlet.learning_rate
        if self.q_selection_percentile is None:
            self.q_selection_percentile = global_config.c2osr.q_selection_percentile
        if self.gamma is None:
            self.gamma = global_config.c2osr.gamma

    @classmethod
    def from_global_config(cls):
        """ä»å…¨å±€é…ç½®åˆ›å»ºQå€¼é…ç½®ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        return cls()


class RewardCalculator:
    """æ¨¡å—åŒ–å¥–åŠ±è®¡ç®—å™¨"""
    
    def __init__(self, config: RewardConfig):
        self.config = config
    
    def calculate_collision_reward(self, collision_occurred: bool) -> float:
        """è®¡ç®—ç¢°æ’å¥–åŠ±"""
        if collision_occurred:
            return self.config.collision_penalty
        return 0.0
    
    def calculate_comfort_reward(self, ego_trajectory: List[Tuple[float, float]], dt: Optional[float] = None) -> float:
        """è®¡ç®—èˆ’é€‚æ€§å¥–åŠ±ï¼ˆåŸºäºåŠ é€Ÿåº¦å’Œæ€¥åŠ¨ï¼‰

        Args:
            ego_trajectory: è‡ªè½¦è½¨è¿¹
            dt: æ—¶é—´æ­¥é•¿ï¼Œé»˜è®¤ä»å…¨å±€é…ç½®è¯»å–
        """
        if dt is None:
            dt = get_global_config().time.dt

        if len(ego_trajectory) < 3:
            return 0.0

        reward = 0.0

        # è®¡ç®—åŠ é€Ÿåº¦
        for i in range(1, len(ego_trajectory) - 1):
            v_prev = np.array(ego_trajectory[i]) - np.array(ego_trajectory[i-1])
            v_curr = np.array(ego_trajectory[i+1]) - np.array(ego_trajectory[i])

            accel = (v_curr - v_prev) / dt
            accel_magnitude = np.linalg.norm(accel)
            
            # åŠ é€Ÿåº¦æƒ©ç½š
            if accel_magnitude > self.config.max_comfortable_accel:
                reward -= (accel_magnitude - self.config.max_comfortable_accel) * self.config.acceleration_penalty_weight
            
            # æ€¥åŠ¨æƒ©ç½šï¼ˆåŠ é€Ÿåº¦å˜åŒ–ç‡ï¼‰
            if i > 1:
                v_prev_prev = np.array(ego_trajectory[i-1]) - np.array(ego_trajectory[i-2])
                accel_prev = (v_prev - v_prev_prev) / dt
                jerk = np.linalg.norm(accel - accel_prev) / dt
                reward -= jerk * self.config.jerk_penalty_weight
        
        return reward
    
    def calculate_efficiency_reward(self, ego_trajectory: List[Tuple[float, float]], dt: Optional[float] = None) -> float:
        """è®¡ç®—é©¾é©¶æ•ˆç‡å¥–åŠ±ï¼ˆé€Ÿåº¦å’Œå‰è¿›è·ç¦»ï¼‰

        Args:
            ego_trajectory: è‡ªè½¦è½¨è¿¹
            dt: æ—¶é—´æ­¥é•¿ï¼Œé»˜è®¤ä»å…¨å±€é…ç½®è¯»å–
        """
        if dt is None:
            dt = get_global_config().time.dt

        if len(ego_trajectory) < 2:
            return 0.0

        reward = 0.0
        total_distance = 0.0

        for i in range(1, len(ego_trajectory)):
            # è®¡ç®—é€Ÿåº¦
            velocity = np.array(ego_trajectory[i]) - np.array(ego_trajectory[i-1])
            speed = np.linalg.norm(velocity) / dt
            
            # é€Ÿåº¦å¥–åŠ±ï¼ˆé¼“åŠ±æ¥è¿‘ç›®æ ‡é€Ÿåº¦ï¼‰
            speed_reward = -abs(speed - self.config.target_speed) * self.config.speed_reward_weight
            reward += speed_reward
            
            # å‰è¿›è·ç¦»å¥–åŠ±
            distance = np.linalg.norm(velocity)
            total_distance += distance
        
        # å‰è¿›è·ç¦»å¥–åŠ±
        reward += total_distance * self.config.progress_reward_weight
        
        return reward
    
    def calculate_safety_reward(self, ego_trajectory: List[Tuple[float, float]], 
                              agent_trajectories: Dict[int, List[Tuple[float, float]]]) -> float:
        """è®¡ç®—å®‰å…¨è·ç¦»å¥–åŠ±"""
        reward = 0.0
        
        for t in range(min(len(ego_trajectory), min(len(traj) for traj in agent_trajectories.values()) if agent_trajectories else len(ego_trajectory))):
            ego_pos = np.array(ego_trajectory[t])
            
            for agent_id, agent_traj in agent_trajectories.items():
                if t < len(agent_traj):
                    agent_pos = np.array(agent_traj[t])
                    distance = np.linalg.norm(ego_pos - agent_pos)
                    
                    if distance < self.config.safe_distance:
                        penalty = -(self.config.safe_distance - distance) * self.config.distance_penalty_weight
                        reward += penalty
        
        return reward
    
    def calculate_centerline_offset_reward(self, ego_trajectory: List[Tuple[float, float]],
                                          reference_path: Optional[List] = None) -> float:
        """è®¡ç®—ä¸­å¿ƒçº¿åç§»å¥–åŠ±

        Args:
            ego_trajectory: è‡ªè½¦è½¨è¿¹
            reference_path: å‚è€ƒè·¯å¾„ï¼ˆä¸­å¿ƒçº¿ï¼‰

        Returns:
            ä¸­å¿ƒçº¿åç§»å¥–åŠ±ï¼ˆè´Ÿå€¼ä¸ºæƒ©ç½šï¼‰
        """
        if reference_path is None or len(reference_path) == 0:
            return 0.0

        from c2o_drive.algorithms.c2osr.rewards import calculate_distance_to_path

        reward = 0.0
        for pos in ego_trajectory:
            offset = calculate_distance_to_path(pos, reference_path)
            # ä½¿ç”¨é…ç½®ä¸­çš„æƒé‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            weight = getattr(self.config, 'centerline_offset_penalty_weight', 1.0)
            reward -= offset * weight

        return reward

    def calculate_total_reward(self, ego_trajectory: List[Tuple[float, float]],
                             agent_trajectories: Dict[int, List[Tuple[float, float]]],
                             collision_occurred: bool,
                             reference_path: Optional[List] = None) -> float:
        """è®¡ç®—æ€»å¥–åŠ±"""
        collision_reward = self.calculate_collision_reward(collision_occurred)

        # å¦‚æœå‘ç”Ÿç¢°æ’ï¼Œç›´æ¥è¿”å›ç¢°æ’æƒ©ç½š
        if collision_occurred:
            return collision_reward

        comfort_reward = self.calculate_comfort_reward(ego_trajectory)
        efficiency_reward = self.calculate_efficiency_reward(ego_trajectory)
        safety_reward = self.calculate_safety_reward(ego_trajectory, agent_trajectories)
        centerline_reward = self.calculate_centerline_offset_reward(ego_trajectory, reference_path)

        return collision_reward + comfort_reward + efficiency_reward + safety_reward + centerline_reward


class QValueCalculator:
    """ç»ˆæä¼˜åŒ–çš„Qå€¼è®¡ç®—å™¨ - å®Œå…¨æ¶ˆé™¤é‡‡æ ·ï¼Œçº¯æœŸæœ›è®¡ç®—"""
    
    def __init__(self, config: QValueConfig, reward_config: RewardConfig):
        self.config = config
        self.reward_config = reward_config
        self.collision_detector = ShapeBasedCollisionDetector()

        # åˆ›å»ºDirichletå‚æ•°ï¼ˆä»å…¨å±€é…ç½®è¯»å–ï¼‰
        from c2o_drive.config import get_global_config
        global_config = get_global_config()

        self.dirichlet_params = DirichletParams(
            alpha_in=config.dirichlet_alpha_in,
            alpha_out=config.dirichlet_alpha_out,
            delta=global_config.dirichlet.delta,
            cK=global_config.dirichlet.cK
        )
    
    def compute_q_value(self,
                       current_world_state: WorldState,
                       ego_action_trajectory: List[Tuple[float, float]],
                       trajectory_buffer: TrajectoryBuffer,
                       grid: GridMapper,
                       bank: Optional[MultiTimestepSpatialDirichletBank] = None,
                       rng: Optional[np.random.Generator] = None,
                       reference_path: Optional[List] = None) -> Tuple[List[float], Dict]:
        """ç»ˆæä¼˜åŒ–çš„Qå€¼è®¡ç®— - å®Œå…¨æ¶ˆé™¤é‡‡æ ·ï¼Œçº¯æœŸæœ›è®¡ç®—

        Args:
            current_world_state: å½“å‰ä¸–ç•ŒçŠ¶æ€
            ego_action_trajectory: è‡ªè½¦åŠ¨ä½œåºåˆ—ï¼ˆæœªæ¥horizonä¸ªä½ç½®ï¼‰
            trajectory_buffer: å†å²è½¨è¿¹ç¼“å†²åŒº
            grid: ç½‘æ ¼æ˜ å°„å™¨
            bank: æŒä¹…åŒ–çš„Dirichlet Bankï¼ˆå¦‚æœä¸ºNoneåˆ™åˆ›å»ºä¸´æ—¶Bankï¼‰
            rng: éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼‰
            reference_path: å‚è€ƒè·¯å¾„ï¼ˆä¸­å¿ƒçº¿ï¼‰ï¼Œç”¨äºè®¡ç®—åç§»æƒ©ç½š

        Returns:
            (æ‰€æœ‰Qå€¼åˆ—è¡¨, è¯¦ç»†ä¿¡æ¯å­—å…¸)
        """
        # å°† reference_path å­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä¾›å†…éƒ¨æ–¹æ³•ä½¿ç”¨
        self._reference_path = reference_path
        if rng is None:
            rng = np.random.default_rng()
        
        horizon = len(ego_action_trajectory)

        # CRITICAL FIX: Bank must be provided and persistent
        # Do NOT create temporary banks as they discard alpha updates
        if bank is None:
            raise ValueError(
                "Dirichlet bank must be provided. "
                "Cannot create temporary bank as it would discard all learned alpha values."
            )

        if not isinstance(bank, OptimizedMultiTimestepSpatialDirichletBank):
            raise TypeError(
                f"Expected OptimizedMultiTimestepSpatialDirichletBank, got {type(bank).__name__}. "
                "Ensure planner passes the persistent bank instance."
            )

        optimized_bank = bank
        
        # è·å–verboseçº§åˆ«
        verbose = get_global_config().visualization.verbose_level

        if verbose >= 3:  # Only show in trace mode
            print(f"  [Q-Value] Starting optimized calculation")

        # ç¬¬1æ­¥:è®¡ç®—ä¸agentå®Œå…¨æ— å…³çš„å¥–åŠ±(åªè®¡ç®—ä¸€æ¬¡!)
        agent_independent_reward = self._calculate_agent_independent_rewards(ego_action_trajectory)

        # ç¬¬2æ­¥:å»ºç«‹agentçš„transitionåˆ†å¸ƒ
        agent_transition_samples = self._build_agent_transition_distributions(
            current_world_state, ego_action_trajectory, trajectory_buffer, grid, optimized_bank, horizon
        )

        # ç¬¬3æ­¥:ç›´æ¥è®¡ç®—æœŸæœ›çš„agentç›¸å…³å¥–åŠ±(æ— é‡‡æ ·!)
        q_values = []
        collision_probabilities = []  # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æœŸæœ›ç¢°æ’æ¦‚ç‡
        
        for sample_idx in range(self.config.n_samples):
            # è·å–è¯¥æ ·æœ¬çš„transitionåˆ†å¸ƒ
            sample_distributions = self._extract_sample_distributions(
                agent_transition_samples, sample_idx
            )
            
            # ç›´æ¥è®¡ç®—æœŸæœ›çš„agentç›¸å…³å¥–åŠ±ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
            expected_collision_reward, expected_collision_prob = self._calculate_expected_collision_reward_directly(
                ego_action_trajectory, sample_distributions, grid, current_world_state
            )
            
            expected_safety_reward = self._calculate_expected_safety_reward_directly(
                ego_action_trajectory, sample_distributions, grid
            )
            
            # ç»„åˆæœ€ç»ˆQå€¼
            total_agent_dependent_reward = expected_collision_reward + expected_safety_reward
            final_q_value = agent_independent_reward + total_agent_dependent_reward
            q_values.append(final_q_value)
            collision_probabilities.append(expected_collision_prob)

        # è®¡ç®—percentileå¯¹åº”çš„ç¢°æ’ç‡ï¼ˆä¸percentile Qå¯¹åº”ï¼‰
        if len(q_values) > 0 and len(collision_probabilities) > 0:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            q_values_array = np.array(q_values)
            collision_probs_array = np.array(collision_probabilities)

            # å¯¹Qå€¼æ’åºï¼Œè·å–percentileå¯¹åº”çš„ç´¢å¼•
            sorted_indices = np.argsort(q_values_array)
            percentile_position = int(self.config.q_selection_percentile * (len(q_values_array) - 1))
            percentile_index = sorted_indices[percentile_position]

            # ä½¿ç”¨percentile Qå¯¹åº”çš„ç¢°æ’ç‡
            percentile_collision_rate = float(collision_probs_array[percentile_index])

            # åŒæ—¶ä¿ç•™å¹³å‡ç¢°æ’ç‡ç”¨äºå¯¹æ¯”
            mean_collision_probability = float(np.mean(collision_probabilities))
        else:
            percentile_collision_rate = 0.0
            mean_collision_probability = 0.0

        # ç»Ÿè®¡ä¿¡æ¯
        detailed_info = {
            'calculation_method': 'ç»ˆæä¼˜åŒ–ï¼šçº¯æœŸæœ›è®¡ç®—ï¼Œé›¶é‡‡æ ·',
            'agent_independent_reward': agent_independent_reward,
            'agent_dependent_rewards': [q - agent_independent_reward for q in q_values],
            'computational_savings': f'æ¶ˆé™¤äº† {self.config.n_samples} Ã— trajectory_samples æ¬¡é‡‡æ ·',
            'reward_breakdown': {
                'mean_q_value': np.mean(q_values),
                'q_value_std': np.std(q_values),
                'q_value_min': np.min(q_values),
                'q_value_max': np.max(q_values),
                'collision_rate': percentile_collision_rate,  # æ”¹ä¸ºä½¿ç”¨percentileå¯¹åº”çš„ç¢°æ’ç‡
                'mean_collision_rate': mean_collision_probability,  # ä¿ç•™å¹³å‡å€¼ç”¨äºå¯¹æ¯”
                'all_q_values': q_values,
                'collision_probabilities': collision_probabilities  # ä¿å­˜æ‰€æœ‰æ ·æœ¬çš„ç¢°æ’æ¦‚ç‡
            },
            'agent_info': {}
        }

        if verbose >= 3:  # Only show detailed stats in trace mode
            # Enhanced Q distribution statistics
            q_arr = np.array(q_values)
            p5 = np.percentile(q_arr, 5)
            p50 = np.percentile(q_arr, 50)
            p95 = np.percentile(q_arr, 95)
            print(f"  [Q-Value] Distribution: P5={p5:.3f}, P50={p50:.3f}, P95={p95:.3f}, "
                  f"Mean={np.mean(q_arr):.3f}, Std={np.std(q_arr):.3f}")
        
        return q_values, detailed_info
    
    def _calculate_agent_independent_rewards(self, ego_trajectory: List[Tuple[float, float]]) -> float:
        """è®¡ç®—ä¸agentå®Œå…¨æ— å…³çš„å¥–åŠ±"""
        total_reward = 0.0
        dt = get_global_config().time.dt

        # 1. èˆ’é€‚æ€§å¥–åŠ±ï¼ˆåŸºäºè‡ªè½¦åŠ é€Ÿåº¦å’Œæ€¥åŠ¨ï¼‰
        if len(ego_trajectory) >= 3:
            for i in range(1, len(ego_trajectory) - 1):
                v_prev = np.array(ego_trajectory[i]) - np.array(ego_trajectory[i-1])
                v_curr = np.array(ego_trajectory[i+1]) - np.array(ego_trajectory[i])
                
                accel = (v_curr - v_prev) / dt
                accel_magnitude = np.linalg.norm(accel)
                
                # åŠ é€Ÿåº¦æƒ©ç½š
                if accel_magnitude > self.reward_config.max_comfortable_accel:
                    total_reward -= (accel_magnitude - self.reward_config.max_comfortable_accel) * self.reward_config.acceleration_penalty_weight
                
                # æ€¥åŠ¨æƒ©ç½š
                if i > 1:
                    v_prev_prev = np.array(ego_trajectory[i-1]) - np.array(ego_trajectory[i-2])
                    accel_prev = (v_prev - v_prev_prev) / dt
                    jerk = np.linalg.norm(accel - accel_prev) / dt
                    total_reward -= jerk * self.reward_config.jerk_penalty_weight
        
        # 2. é€Ÿåº¦å¥–åŠ±ï¼ˆåŸºäºè‡ªè½¦é€Ÿåº¦ï¼‰
        if len(ego_trajectory) >= 2:
            for i in range(1, len(ego_trajectory)):
                velocity = np.array(ego_trajectory[i]) - np.array(ego_trajectory[i-1])
                speed = np.linalg.norm(velocity) / dt
                speed_reward = -abs(speed - self.reward_config.target_speed) * self.reward_config.speed_reward_weight
                total_reward += speed_reward
        
        # 3. è¿›åº¦å¥–åŠ±ï¼ˆåŸºäºå‰è¿›è·ç¦»ï¼‰
        if len(ego_trajectory) >= 2:
            total_distance = 0.0
            for i in range(1, len(ego_trajectory)):
                velocity = np.array(ego_trajectory[i]) - np.array(ego_trajectory[i-1])
                distance = np.linalg.norm(velocity)
                total_distance += distance
            total_reward += total_distance * self.reward_config.progress_reward_weight

        # 4. ä¸­å¿ƒçº¿åç§»æƒ©ç½š
        if hasattr(self, '_reference_path') and self._reference_path is not None:
            from c2o_drive.algorithms.c2osr.rewards import calculate_distance_to_path
            for pos in ego_trajectory:
                offset = calculate_distance_to_path(pos, self._reference_path)
                weight = getattr(self.reward_config, 'centerline_offset_penalty_weight', 1.0)
                total_reward -= offset * weight

        return total_reward
    
    def _calculate_expected_collision_reward_directly(self,
                                                    ego_trajectory: List[Tuple[float, float]],
                                                    agent_distributions: Dict[int, Dict[int, Tuple[List[int], np.ndarray]]],
                                                    grid: GridMapper,
                                                    current_world_state: WorldState) -> Tuple[float, float]:
        """ç›´æ¥è®¡ç®—æœŸæœ›ç¢°æ’å¥–åŠ±å’ŒæœŸæœ›ç¢°æ’æ¦‚ç‡ - ä½¿ç”¨ç²¾ç¡®è½¦è¾†å½¢çŠ¶ç¢°æ’æ£€æµ‹ + Cellå‰ªæä¼˜åŒ–ï¼

        Returns:
            (expected_reward, expected_collision_probability)
        """
        expected_reward = 0.0
        expected_collision_prob = 0.0  # æœŸæœ›ç¢°æ’æ¦‚ç‡
        collision_count = 0  # è°ƒè¯•ï¼šç¢°æ’è®¡æ•°

        # ğŸš€ ä¼˜åŒ–1: é¢„è®¡ç®—egoè½¨è¿¹å æ®çš„cellé›†åˆï¼ˆç”¨äºå‰ªæï¼‰
        # ä½¿ç”¨é…ç½®çš„å‰ªæåŠå¾„ï¼ˆé»˜è®¤radius=10ï¼Œçº¦5ç±³ï¼Œè¦†ç›–è½¦è¾†é•¿åº¦4.5mï¼‰
        ego_cells_set = set()
        for ego_pos in ego_trajectory:
            ego_cell = grid.world_to_cell(ego_pos)
            # æ‰©å±•åˆ°é‚»åŸŸï¼ˆè€ƒè™‘è½¦è¾†å°ºå¯¸ï¼‰
            neighbors = grid.get_neighbors(ego_cell, radius=self.reward_config.collision_check_cell_radius)
            ego_cells_set.update(neighbors)

        # ç»Ÿè®¡å‰ªææ•ˆæœ
        total_checks = 0
        pruned_checks = 0

        # è®¡ç®—è‡ªè½¦æœå‘åºåˆ—ï¼ˆå‡è®¾ç›´è¡Œï¼Œå®é™…åº”æ ¹æ®è½¨è¿¹è®¡ç®—ï¼‰
        ego_headings = []
        ego_initial_heading = current_world_state.ego.yaw_rad

        for i in range(len(ego_trajectory)):
            if i == 0:
                ego_headings.append(ego_initial_heading)
            else:
                # æ ¹æ®è½¨è¿¹è®¡ç®—æœå‘
                dx = ego_trajectory[i][0] - ego_trajectory[i-1][0]
                dy = ego_trajectory[i][1] - ego_trajectory[i-1][1]
                if abs(dx) > 1e-6 or abs(dy) > 1e-6:
                    heading = np.arctan2(dy, dx)
                    ego_headings.append(heading)
                else:
                    ego_headings.append(ego_headings[-1])

        for timestep, ego_pos in enumerate(ego_trajectory):
            timestep_key = timestep + 1  # timestepä»1å¼€å§‹
            
            if timestep >= len(ego_headings):
                continue
                
            for agent_id, distributions in agent_distributions.items():
                if timestep_key in distributions:
                    reachable_cells, probabilities = distributions[timestep_key]
                    
                    # è·å–agentç±»å‹ï¼ˆä»å½“å‰ä¸–ç•ŒçŠ¶æ€ï¼‰
                    if agent_id <= len(current_world_state.agents):
                        agent_type = current_world_state.agents[agent_id - 1].agent_type
                    else:
                        agent_type = AgentType.VEHICLE  # é»˜è®¤ç±»å‹
                    
                    # ç›´æ¥è®¡ç®—æœŸæœ›ï¼šE[ç¢°æ’å¥–åŠ±] = Î£ P(ä½ç½®i) Ã— I(ç¢°æ’) Ã— æƒ©ç½š
                    for cell_idx, prob in enumerate(probabilities):
                        if prob > 0:
                            cell = reachable_cells[cell_idx]
                            total_checks += 1

                            # ğŸš€ ä¼˜åŒ–2: Cellå‰ªæ - å¿«é€Ÿè·³è¿‡ä¸å¯èƒ½ç¢°æ’çš„cells
                            if cell not in ego_cells_set:
                                pruned_checks += 1
                                continue  # ä¸åœ¨egoé™„è¿‘ï¼Œè·³è¿‡ç²¾ç¡®æ£€æµ‹ï¼

                            cell_center = grid.index_to_xy_center(cell)
                            world_pos = grid.grid_to_world(np.array(cell_center))

                            # ä½¿ç”¨ç²¾ç¡®çš„è½¦è¾†å½¢çŠ¶ç¢°æ’æ£€æµ‹
                            # å‡è®¾agentæœå‘ä¸è‡ªè½¦ç›¸åŒï¼ˆç®€åŒ–å¤„ç†ï¼‰
                            agent_heading = ego_headings[timestep]

                            collision_occurred = self.collision_detector.check_point_collision(
                                ego_pos=ego_pos,
                                ego_heading=ego_headings[timestep],
                                agent_pos=tuple(world_pos),
                                agent_heading=agent_heading,
                                ego_type=AgentType.VEHICLE,
                                agent_type=agent_type
                            )

                            if collision_occurred:
                                # ç›´æ¥ç´¯åŠ æœŸæœ›å€¼ï¼šæ¦‚ç‡ Ã— ç¢°æ’æƒ©ç½š
                                collision_contribution = prob * self.reward_config.collision_penalty
                                expected_reward += collision_contribution
                                
                                # ç´¯åŠ æœŸæœ›ç¢°æ’æ¦‚ç‡ï¼šæ¦‚ç‡ Ã— 1ï¼ˆç¢°æ’å‘ç”Ÿï¼‰
                                expected_collision_prob += prob
                                collision_count += 1

        # æ‰“å°å‰ªææ•ˆæœç»Ÿè®¡(ä»…åœ¨debugæ¨¡å¼)
        if total_checks > 0:
            verbose = get_global_config().visualization.verbose_level
            if verbose >= 3:  # Only show in trace mode (verbose=3)
                prune_rate = (pruned_checks / total_checks) * 100
                actual_checks = total_checks - pruned_checks
                radius_m = self.reward_config.collision_check_cell_radius * self.grid_spec.cell_m
                print(f"  [Collision] Cell pruning: total={total_checks}, pruned={pruned_checks}, "
                      f"actual={actual_checks}, rate={prune_rate:.1f}%")

        return expected_reward, expected_collision_prob
    
    def _calculate_expected_safety_reward_directly(self,
                                                 ego_trajectory: List[Tuple[float, float]],
                                                 agent_distributions: Dict[int, Dict[int, Tuple[List[int], np.ndarray]]],
                                                 grid: GridMapper) -> float:
        """ç›´æ¥è®¡ç®—æœŸæœ›å®‰å…¨è·ç¦»å¥–åŠ± - æ‚¨çš„å¦ä¸€ä¸ªæ ¸å¿ƒæ´å¯Ÿï¼"""
        expected_reward = 0.0
        
        for timestep, ego_pos in enumerate(ego_trajectory):
            timestep_key = timestep + 1
            
            for agent_id, distributions in agent_distributions.items():
                if timestep_key in distributions:
                    reachable_cells, probabilities = distributions[timestep_key]
                    
                    # ç›´æ¥è®¡ç®—æœŸæœ›ï¼šE[å®‰å…¨å¥–åŠ±] = Î£ P(ä½ç½®i) Ã— å®‰å…¨å¥–åŠ±(è·ç¦»i)
                    for cell_idx, prob in enumerate(probabilities):
                        if prob > 0:
                            cell = reachable_cells[cell_idx]
                            cell_center = grid.index_to_xy_center(cell)
                            world_pos = grid.grid_to_world(np.array(cell_center))
                            
                            distance = np.linalg.norm(np.array(ego_pos) - np.array(world_pos))
                            if distance < self.reward_config.safe_distance:
                                penalty = -(self.reward_config.safe_distance - distance) * self.reward_config.distance_penalty_weight
                                # ç›´æ¥ç´¯åŠ æœŸæœ›å€¼ï¼šæ¦‚ç‡ Ã— å®‰å…¨è·ç¦»æƒ©ç½š
                                expected_reward += prob * penalty
        
        return expected_reward
    
    def _build_agent_transition_distributions(self, current_world_state, ego_action_trajectory, 
                                            trajectory_buffer, grid, bank, horizon):
        """å»ºç«‹agentçš„transitionåˆ†å¸ƒï¼ˆå¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼‰"""
        current_ego_state = (
            current_world_state.ego.position_m[0], 
            current_world_state.ego.position_m[1], 
            current_world_state.ego.yaw_rad
        )
        current_agents_states = []
        for agent in current_world_state.agents:
            current_agents_states.append((
                agent.position_m[0], agent.position_m[1],
                agent.velocity_mps[0], agent.velocity_mps[1],
                agent.heading_rad, agent.agent_type.value
            ))
        # é‡è¦ï¼šå¿…é¡»æŒ‰agent_typeæ’åºï¼Œä¸å­˜å‚¨æ—¶ä¿æŒä¸€è‡´
        current_agents_states = sorted(current_agents_states, key=lambda x: x[5])
        
        agent_transition_samples = {}
        
        for i, agent in enumerate(current_world_state.agents):
            agent_id = i + 1
            
            config = get_global_config()
            reachable_sets = grid.multi_timestep_successor_cells(
                agent, horizon=horizon, dt=config.time.dt,
                n_samples=config.sampling.reachable_set_samples
            )
            if not reachable_sets:
                continue

            # CRITICAL FIX: Only initialize if agent doesn't exist in bank
            # This preserves accumulated alpha values across Q-value calculations
            if agent_id not in bank.agent_alphas:
                bank.init_agent(agent_id, reachable_sets)
                if config.visualization.verbose_level >= 2:
                    print(f"  [Bank Init] Agent {agent_id} initialized with {len(reachable_sets)} timesteps")
            # If agent already exists, keep existing alpha values (don't reset!)
            
            # ä»å…¨å±€é…ç½®è¯»å–åŒ¹é…é˜ˆå€¼
            config = get_global_config()
            historical_transitions_by_timestep = trajectory_buffer.get_agent_historical_transitions_strict_matching(
                agent_id=agent_id,
                current_ego_state=current_ego_state,
                current_agents_states=current_agents_states,
                ego_action_trajectory=ego_action_trajectory,
                ego_state_threshold=config.matching.ego_state_threshold,
                agents_state_threshold=config.matching.agents_state_threshold,
                ego_action_threshold=config.matching.ego_action_threshold,
                debug=(config.visualization.verbose_level >= 2)  # Enable debug logging when verbose
            )

            # Log matching statistics
            total_matched = sum(len(cells) for cells in historical_transitions_by_timestep.values())
            matched_timesteps = len(historical_transitions_by_timestep)

            # Update Dirichlet bank with matched historical data
            update_count = 0
            for timestep, historical_cells in historical_transitions_by_timestep.items():
                if len(historical_cells) > 0 and timestep in reachable_sets:
                    bank.update_with_softcount(
                        agent_id, timestep, historical_cells,
                        lr=self.config.learning_rate
                    )
                    update_count += 1

            # Log update statistics with matching density (only if verbose)
            if get_global_config().visualization.verbose_level >= 1:
                buffer_size = len(trajectory_buffer)
                # Calculate matching density
                total_reachable = sum(len(cells) for cells in reachable_sets.values())
                match_density = (total_matched / total_reachable * 100) if total_reachable > 0 else 0.0
                print(f"  [Agent {agent_id}] Matched {total_matched}/{total_reachable} cells ({match_density:.1f}%) "
                      f"across {matched_timesteps} timesteps, Updated {update_count} timesteps, Buffer size: {buffer_size}")
            
            transition_distributions = bank.sample_transition_distributions(
                agent_id, n_samples=self.config.n_samples
            )

            # CRITICAL FIX: Use bank's stored reachable_sets to match distribution dimensions
            # distributions are sampled from bank.agent_alphas (using bank's dimensions)
            # so reachable_sets must also come from bank to ensure dimension consistency
            agent_transition_samples[agent_id] = {
                'distributions': transition_distributions,
                'reachable_sets': bank.agent_reachable_sets[agent_id]  # Use bank's stored sets
            }
        
        return agent_transition_samples
    
    def _extract_sample_distributions(self, agent_transition_samples, sample_idx):
        """æå–æŒ‡å®šæ ·æœ¬çš„transitionåˆ†å¸ƒ"""
        sample_distributions = {}
        
        for agent_id, transition_info in agent_transition_samples.items():
            distributions = transition_info['distributions']
            reachable_sets = transition_info['reachable_sets']
            
            agent_distributions = {}
            for timestep in distributions:
                if sample_idx < len(distributions[timestep]):
                    reachable_cells = reachable_sets[timestep]
                    probabilities = distributions[timestep][sample_idx]
                    agent_distributions[timestep] = (reachable_cells, probabilities)
            
            sample_distributions[agent_id] = agent_distributions
        
        return sample_distributions
