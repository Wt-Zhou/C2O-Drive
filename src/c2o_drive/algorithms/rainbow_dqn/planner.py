"""Rainbow DQN Planner

Main planner class that integrates all Rainbow components and provides
a unified interface compatible with C2OSR.
"""

from typing import List, Tuple, Optional
import numpy as np
import torch
import torch.nn as nn

from c2o_drive.algorithms.base import EpisodicAlgorithmPlanner
from c2o_drive.algorithms.rainbow_dqn.config import RainbowDQNConfig
from c2o_drive.algorithms.rainbow_dqn.network import RainbowNetwork
from c2o_drive.algorithms.rainbow_dqn.replay_buffer import PrioritizedReplayBuffer
from c2o_drive.core.planner import Transition, UpdateMetrics
from c2o_drive.core.types import WorldState, EgoControl, EgoState
from c2o_drive.utils.lattice_planner import LatticePlanner, LatticeTrajectory
from c2o_drive.algorithms.c2osr.grid_mapper import GridMapper, GridSpec


class RainbowDQNPlanner(EpisodicAlgorithmPlanner[WorldState, EgoControl]):
    """Rainbow DQN Planner for trajectory selection.

    Implements the complete Rainbow DQN algorithm with all six components:
    1. Double DQN - reduce overestimation bias
    2. Dueling Network - separate value and advantage estimation
    3. Prioritized Experience Replay - sample important transitions
    4. Multi-step Learning - use n-step returns
    5. Distributional RL (C51) - model value distribution
    6. Noisy Nets - learned exploration

    State Space: WorldState (directly used, no feature extraction)
    Action Space: Trajectory ID (discrete, generated by LatticePlanner)

    Attributes:
        config: Rainbow DQN configuration
        lattice_planner: Trajectory generator (reused from C2OSR)
        grid_mapper: Spatial discretization (reused from C2OSR)
        q_network: Main Q-network with Rainbow components
        target_network: Target network for stable learning
        replay_buffer: Prioritized experience replay
        optimizer: Adam optimizer
        current_reference_path: Current reference path for trajectory generation
        last_selected_trajectory: Last selected trajectory
        last_trajectory_idx: Last selected trajectory index
        episode_step_count: Steps in current episode
    """

    def __init__(self, config: RainbowDQNConfig):
        """Initialize Rainbow DQN planner.

        Args:
            config: Rainbow DQN configuration
        """
        super().__init__(config)

        self.config = config

        # === Reuse C2OSR components ===
        self.lattice_planner = LatticePlanner(
            lateral_offsets=config.lattice.lateral_offsets,
            speed_variations=config.lattice.speed_variations,
            num_trajectories=config.lattice.num_trajectories
        )

        # GridMapper initialization (following C2OSR pattern)
        x_range = config.grid.bounds_x[1] - config.grid.bounds_x[0]
        y_range = config.grid.bounds_y[1] - config.grid.bounds_y[0]
        size_m = max(x_range, y_range)
        center_x = 0.5 * (config.grid.bounds_x[0] + config.grid.bounds_x[1])
        center_y = 0.5 * (config.grid.bounds_y[0] + config.grid.bounds_y[1])

        self.grid_spec = GridSpec(
            size_m=size_m,
            cell_m=config.grid.grid_size_m,
            macro=False
        )
        self.grid_mapper = GridMapper(self.grid_spec, world_center=(center_x, center_y))

        # === Rainbow components ===
        self.device = torch.device(config.device if torch.cuda.is_available() else "cpu")

        self.q_network = RainbowNetwork(config).to(self.device)
        self.target_network = RainbowNetwork(config).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()  # Always in eval mode

        self.replay_buffer = PrioritizedReplayBuffer(
            capacity=config.replay.capacity,
            alpha=config.replay.alpha,
            beta_start=config.replay.beta_start,
            beta_frames=config.replay.beta_frames
        )

        self.optimizer = torch.optim.Adam(
            self.q_network.parameters(),
            lr=config.training.learning_rate
        )

        # === Internal state ===
        self.current_reference_path: Optional[List[Tuple[float, float]]] = None
        self.last_selected_trajectory: Optional[LatticeTrajectory] = None
        self.last_trajectory_idx: Optional[int] = None
        self._update_count = 0
        self.episode_step_count = 0

    def select_action(
        self,
        observation: WorldState,
        deterministic: bool = False,
        reference_path: Optional[List[Tuple[float, float]]] = None,
        **kwargs
    ) -> EgoControl:
        """Select action based on observation.

        Workflow:
        1. Generate candidate trajectories (LatticePlanner)
        2. Reset Noisy Nets noise (if training)
        3. Forward pass to get Q-value distributions
        4. Select best trajectory (argmax expected Q)
        5. Convert to EgoControl

        Args:
            observation: Current WorldState
            deterministic: If True, use deterministic policy (eval mode)
            reference_path: Reference path for trajectory generation
            **kwargs: Additional arguments

        Returns:
            EgoControl command
        """
        # 1. Update reference path
        if reference_path is not None:
            self.current_reference_path = reference_path

        # 2. Generate candidate trajectories
        ego_state = (
            observation.ego.position_m[0],
            observation.ego.position_m[1],
            observation.ego.yaw_rad
        )

        ref_path = self.current_reference_path
        if ref_path is None:
            # Create default forward reference path
            ego_x, ego_y = observation.ego.position_m
            ref_path = [
                (ego_x + i * 5.0, ego_y)
                for i in range(self.config.lattice.horizon + 1)
            ]

        candidate_trajectories = self.lattice_planner.generate_trajectories(
            reference_path=ref_path,
            horizon=self.config.lattice.horizon,
            dt=self.config.lattice.dt,
            ego_state=ego_state
        )

        if not candidate_trajectories:
            # Fallback: emergency stop
            return EgoControl(throttle=0.0, steer=0.0, brake=1.0)

        # 3. Noisy Nets exploration (training only)
        if not deterministic:
            self.q_network.reset_noise()

        # 4. Network forward pass
        self.q_network.eval() if deterministic else self.q_network.train()

        with torch.no_grad():
            q_dist, q_values = self.q_network([observation])

        # 5. Select best trajectory
        best_traj_idx = q_values.argmax(dim=1).item()
        best_trajectory = candidate_trajectories[best_traj_idx]

        # 6. Store for update()
        self.last_selected_trajectory = best_trajectory
        self.last_trajectory_idx = best_traj_idx

        # 7. Convert to control
        action = self._trajectory_to_control(observation.ego, best_trajectory)

        self.episode_step_count += 1
        self._step_count += 1

        return action

    def update(self, transition: Transition[WorldState, EgoControl]) -> UpdateMetrics:
        """Update planner with new transition.

        Implements:
        - Prioritized Experience Replay
        - Double DQN target computation
        - C51 distributional Bellman update
        - Multi-step returns (simplified single-step version)
        - Target network updates

        Args:
            transition: Transition to learn from

        Returns:
            UpdateMetrics with loss and Q-value information
        """
        # 1. Store transition
        self.replay_buffer.push(
            state=transition.state,
            action=self.last_trajectory_idx if self.last_trajectory_idx is not None else 0,
            reward=transition.reward,
            next_state=transition.next_state,
            done=transition.terminated or transition.truncated
        )

        # Log statistics
        self._log_stat('reward', transition.reward)
        self._log_stat('episode_step', float(self.episode_step_count))

        # 2. Check if ready to train
        if len(self.replay_buffer) < self.config.training.batch_size:
            return UpdateMetrics(custom={'buffer_size': len(self.replay_buffer)})

        # Episode-level training uses one transition per episode, so gate warmup
        # on buffer size rather than per-step count.
        if len(self.replay_buffer) < self.config.training.warmup_steps:
            return UpdateMetrics(custom={'warmup': True})

        # 3. Sample batch
        batch, indices, weights = self.replay_buffer.sample(self.config.training.batch_size)

        # 4. Prepare data
        states = [t.state for t in batch]
        actions = torch.LongTensor([t.action for t in batch]).to(self.device)
        rewards = torch.FloatTensor([t.reward for t in batch]).to(self.device)
        next_states = [t.next_state for t in batch]
        dones = torch.FloatTensor([float(t.done) for t in batch]).to(self.device)
        weights_tensor = torch.FloatTensor(weights).to(self.device)

        # 5. Current Q distribution
        self.q_network.train()
        q_dist, _ = self.q_network(states)  # (batch, actions, atoms)
        q_dist = q_dist[range(len(batch)), actions, :]  # (batch, atoms)
        q_dist = q_dist.clamp(min=1e-8)  # Numerical stability

        # 6. Target Q distribution (Double DQN + C51)
        with torch.no_grad():
            # Double DQN: online network selects action
            _, next_q_values = self.q_network(next_states)
            next_actions = next_q_values.argmax(dim=1)

            # Target network evaluates
            next_q_dist, _ = self.target_network(next_states)
            next_q_dist = next_q_dist[range(len(batch)), next_actions, :]

            # C51 projection
            target_dist = self._project_distribution(rewards, next_q_dist, dones)

        # 7. Compute loss (KL divergence)
        log_q_dist = q_dist.log()
        loss_elementwise = -(target_dist * log_q_dist).sum(dim=1)
        loss = (weights_tensor * loss_elementwise).mean()

        # 8. Update priorities
        td_errors = loss_elementwise.detach().cpu().numpy()
        self.replay_buffer.update_priorities(indices, td_errors)

        # 9. Backpropagation
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            self.q_network.parameters(),
            self.config.training.gradient_clip
        )
        self.optimizer.step()

        # 10. Update target network
        self._update_count += 1
        if self._update_count % self.config.training.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        # 11. Episode cleanup
        if transition.terminated or transition.truncated:
            self.episode_step_count = 0

        return UpdateMetrics(
            loss=loss.item(),
            q_value=next_q_values.mean().item(),
            custom={
                'buffer_size': len(self.replay_buffer),
                'update_count': self._update_count,
                'td_error_mean': td_errors.mean(),
            }
        )

    def _train_step(self) -> UpdateMetrics:
        """执行一次训练步骤（从replay buffer采样并更新网络）

        这个方法从update()中抽取出来，用于episode-level训练。
        与PPO的_ppo_update()类似，在episode结束后调用。

        Returns:
            UpdateMetrics with loss and Q-value information
        """
        # 1. Check if ready to train
        if len(self.replay_buffer) < self.config.training.batch_size:
            return UpdateMetrics(custom={'buffer_size': len(self.replay_buffer)})

        # Episode-level training uses one transition per episode, so gate warmup
        # on buffer size rather than per-step count.
        if len(self.replay_buffer) < self.config.training.warmup_steps:
            return UpdateMetrics(custom={'warmup': True})

        # 2. Sample batch
        batch, indices, weights = self.replay_buffer.sample(self.config.training.batch_size)

        # 3. Prepare data
        states = [t.state for t in batch]
        actions = torch.LongTensor([t.action for t in batch]).to(self.device)
        rewards = torch.FloatTensor([t.reward for t in batch]).to(self.device)
        next_states = [t.next_state for t in batch]
        dones = torch.FloatTensor([float(t.done) for t in batch]).to(self.device)
        weights_tensor = torch.FloatTensor(weights).to(self.device)

        # 4. Current Q distribution (Noisy Nets: reset noise each update)
        self.q_network.train()
        self.q_network.reset_noise()
        q_dist, q_values = self.q_network(states)  # (batch, actions, atoms)
        q_dist = q_dist[range(len(batch)), actions, :]  # (batch, atoms)
        q_dist = q_dist.clamp(min=1e-8)  # Numerical stability

        # 5. Target distribution for episode-level (single-decision) training
        # Use reward-only distribution; no bootstrap from next state.
        with torch.no_grad():
            target_dist = self._project_reward_distribution(rewards)

        # 6. Compute loss (KL divergence)
        log_q_dist = q_dist.log()
        loss_elementwise = -(target_dist * log_q_dist).sum(dim=1)
        loss = (weights_tensor * loss_elementwise).mean()

        # 7. Update priorities
        td_errors = loss_elementwise.detach().cpu().numpy()
        self.replay_buffer.update_priorities(indices, td_errors)

        # 8. Backpropagation
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            self.q_network.parameters(),
            self.config.training.gradient_clip
        )
        self.optimizer.step()

        # 9. Update target network
        self._update_count += 1
        if self._update_count % self.config.training.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        # 10. Increment step count
        self._step_count += 1

        return UpdateMetrics(
            loss=loss.item(),
            q_value=q_values.mean().item(),
            custom={
                'buffer_size': len(self.replay_buffer),
                'update_count': self._update_count,
                'td_error_mean': td_errors.mean(),
            }
        )

    def plan_trajectory(
        self,
        observation: WorldState,
        horizon: int,
        **kwargs
    ) -> List[EgoControl]:
        """Plan trajectory of actions.

        Simple implementation: repeatedly call select_action().

        Args:
            observation: Current observation
            horizon: Planning horizon
            **kwargs: Additional arguments

        Returns:
            List of planned actions
        """
        trajectory = []
        for _ in range(horizon):
            action = self.select_action(observation, deterministic=True, **kwargs)
            trajectory.append(action)
        return trajectory

    def reset(self):
        """Reset planner for new episode."""
        super().reset()
        self.episode_step_count = 0
        self.current_reference_path = None
        self.last_selected_trajectory = None
        self.last_trajectory_idx = None

    def save(self, path: str):
        """Save planner state.

        Args:
            path: Path to save checkpoint
        """
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'update_count': self._update_count,
            'step_count': self._step_count,
            'episode_count': self._episode_count,
            'config': self.config,
        }, path)

    def load(self, path: str):
        """Load planner state.

        Args:
            path: Path to load checkpoint from
        """
        checkpoint = torch.load(path, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self._update_count = checkpoint['update_count']
        self._step_count = checkpoint['step_count']
        self._episode_count = checkpoint['episode_count']

    # === Helper Methods ===

    def _trajectory_to_control(
        self,
        current_state: EgoState,
        trajectory: LatticeTrajectory
    ) -> EgoControl:
        """Convert trajectory to control command.

        Uses simple P-controller for heading and speed.

        Args:
            current_state: Current ego state
            trajectory: Selected trajectory

        Returns:
            EgoControl command
        """
        if len(trajectory.waypoints) < 2:
            return EgoControl(throttle=0.0, steer=0.0, brake=1.0)

        # Target: next waypoint
        target_x, target_y = trajectory.waypoints[1]
        current_x, current_y = current_state.position_m

        # Heading control (P-controller)
        dx = target_x - current_x
        dy = target_y - current_y
        target_heading = np.arctan2(dy, dx)
        heading_error = target_heading - current_state.yaw_rad
        heading_error = np.arctan2(np.sin(heading_error), np.cos(heading_error))  # Normalize to [-pi, pi]

        steer = np.clip(heading_error * 0.5, -1.0, 1.0)

        # Speed control
        current_speed = np.linalg.norm(np.array(current_state.velocity_mps))
        speed_error = trajectory.target_speed - current_speed

        if speed_error > 0.5:
            throttle, brake = 0.6, 0.0
        elif speed_error < -0.5:
            throttle, brake = 0.0, 0.5
        else:
            throttle, brake = 0.3, 0.0

        return EgoControl(throttle=throttle, steer=steer, brake=brake)

    def _project_distribution(
        self,
        rewards: torch.Tensor,
        next_dist: torch.Tensor,
        dones: torch.Tensor
    ) -> torch.Tensor:
        """Project Bellman update onto categorical distribution (C51).

        Implements the projection step from C51:
        1. Compute Bellman update: T_z = r + γ * z
        2. Project onto support atoms

        Args:
            rewards: Rewards tensor, shape (batch,)
            next_dist: Next state distributions, shape (batch, num_atoms)
            dones: Done flags, shape (batch,)

        Returns:
            Projected target distributions, shape (batch, num_atoms)
        """
        batch_size = rewards.size(0)
        support = self.q_network.support
        delta_z = (self.config.network.v_max - self.config.network.v_min) / (self.config.network.num_atoms - 1)

        # Bellman update: T_z = r + γ * z
        Tz = rewards.unsqueeze(1) + self.config.training.gamma * support * (1 - dones.unsqueeze(1))
        Tz = Tz.clamp(self.config.network.v_min, self.config.network.v_max)

        # Project onto support
        b = (Tz - self.config.network.v_min) / delta_z
        l = b.floor().long()
        u = b.ceil().long()

        # Fix edge cases
        l[(u > 0) * (l == u)] -= 1
        u[(l < (self.config.network.num_atoms - 1)) * (l == u)] += 1

        # Distribute probability
        target_dist = torch.zeros_like(next_dist)
        offset = torch.arange(
            0,
            batch_size * self.config.network.num_atoms,
            self.config.network.num_atoms,
            device=self.device
        ).unsqueeze(1)

        # Lower bound projection
        target_dist.view(-1).index_add_(
            0,
            (l + offset).view(-1),
            (next_dist * (u.float() - b)).view(-1)
        )

        # Upper bound projection
        target_dist.view(-1).index_add_(
            0,
            (u + offset).view(-1),
            (next_dist * (b - l.float())).view(-1)
        )

        return target_dist

    def _project_reward_distribution(self, rewards: torch.Tensor) -> torch.Tensor:
        """Project reward-only distribution onto categorical support (C51).

        Episode-level setting: single decision, return equals total reward.
        This creates a delta distribution at reward and projects onto support.

        Args:
            rewards: Rewards tensor, shape (batch,)

        Returns:
            Projected target distributions, shape (batch, num_atoms)
        """
        batch_size = rewards.size(0)
        delta_z = (self.config.network.v_max - self.config.network.v_min) / (self.config.network.num_atoms - 1)

        # Tz = r (no bootstrap)
        Tz = rewards.unsqueeze(1).clamp(self.config.network.v_min, self.config.network.v_max)

        # Project onto support
        b = (Tz - self.config.network.v_min) / delta_z
        l = b.floor().long()
        u = b.ceil().long()

        # Distribute probability
        target_dist = torch.zeros(batch_size, self.config.network.num_atoms, device=self.device)
        offset = torch.arange(
            0,
            batch_size * self.config.network.num_atoms,
            self.config.network.num_atoms,
            device=self.device
        ).unsqueeze(1)

        # Lower bound projection
        target_dist.view(-1).index_add_(
            0,
            (l + offset).view(-1),
            (u.float() - b).view(-1)
        )

        # Upper bound projection
        target_dist.view(-1).index_add_(
            0,
            (u + offset).view(-1),
            (b - l.float()).view(-1)
        )

        # Handle l == u (all mass on a single atom)
        eq_mask = (u == l)
        if eq_mask.any():
            eq_idx = eq_mask.squeeze(1)  # shape: (batch,)
            target_dist[eq_idx] = 0.0
            target_dist[eq_idx, l.squeeze(1)[eq_idx]] = 1.0

        return target_dist
