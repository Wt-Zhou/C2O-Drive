# Rainbow DQN vs PPO å®Œæ•´æµç¨‹å¯¹æ¯”æ£€æŸ¥

## æ—¥æœŸ
2026-01-26

## ç›®çš„
ç³»ç»Ÿæ€§æ£€æŸ¥Rainbow DQNå®ç°ï¼Œå¯¹æ¯”PPOæ‰¾å‡ºæ‰€æœ‰æ½œåœ¨é—®é¢˜

---

## 1. ç¯å¢ƒé‡ç½® (Environment Reset)

### PPO
```python
# run_ppo_carla.py:298-309
state, info = self.env.reset(seed=seed, options=reset_options)
reference_path = info.get('reference_path', [])
self.planner.reset()
```

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:238-242
state, info = self.env.reset(seed=seed, options=reset_options)
reference_path = info.get('reference_path', [])
self.planner.reset()
```

**çŠ¶æ€**: âœ… ä¸€è‡´

---

## 2. è½¨è¿¹ç”Ÿæˆ (Trajectory Generation)

### PPO
```python
# run_ppo_carla.py:314-333
candidate_trajectories = self.planner.lattice_planner.generate_trajectories(
    reference_path=reference_path,
    horizon=self.planner.config.lattice.horizon,
    dt=self.planner.config.lattice.dt,
    ego_state=ego_state_tuple,
)
```

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:258-263
candidate_trajectories = self.planner.lattice_planner.generate_trajectories(
    reference_path=reference_path,
    horizon=self.planner.config.lattice.horizon,
    dt=self.planner.config.lattice.dt,
    ego_state=ego_state_tuple,
)
```

**çŠ¶æ€**: âœ… ä¸€è‡´

---

## 3. çŠ¶æ€ç‰¹å¾æå– (State Feature Extraction)

### PPO
```python
# run_ppo_carla.py:345
state_features = self.planner._extract_state_features(state)

# planner.py:391-442
def _extract_state_features(self, world_state: WorldState) -> torch.Tensor:
    features = []

    # Ego: [x/100, y/100, speed/30, cos(yaw), sin(yaw)]
    ego_speed = np.linalg.norm(ego.velocity_mps)
    features.extend([
        ego.position_m[0] / 100.0,
        ego.position_m[1] / 100.0,
        ego_speed / 30.0,
        np.cos(ego.yaw_rad),
        np.sin(ego.yaw_rad),
    ])

    # Goal: [rel_x/100, rel_y/100]
    if hasattr(world_state, 'goal') and world_state.goal is not None:
        rel_x = (goal.position_m[0] - ego.position_m[0]) / 100.0
        rel_y = (goal.position_m[1] - ego.position_m[1]) / 100.0
        features.extend([rel_x, rel_y])
    else:
        features.extend([0.0, 0.0])

    # Agents (max 10): [rel_x/100, rel_y/100, speed/30, cos(heading), sin(heading)]
    for agent in world_state.agents[:10]:
        rel_x = (agent.position_m[0] - ego.position_m[0]) / 100.0
        rel_y = (agent.position_m[1] - ego.position_m[1]) / 100.0
        agent_speed = np.linalg.norm(agent.velocity_mps)
        features.extend([
            rel_x, rel_y,
            agent_speed / 30.0,
            np.cos(agent.heading_rad),
            np.sin(agent.heading_rad),
        ])

    # Pad to state_dim
    return torch.tensor(features[:state_dim])
```

**ç‰¹å¾æ€»ç»“**:
- Ego: 5ç»´ (å½’ä¸€åŒ–ä½ç½®ã€é€Ÿåº¦ã€æœå‘)
- Goal: 2ç»´ (ç›¸å¯¹è·ç¦»ï¼Œå½’ä¸€åŒ–)
- Agents: 10ä¸ª Ã— 5ç»´ = 50ç»´ (ç›¸å¯¹ä½ç½®ã€é€Ÿåº¦ã€æœå‘ï¼Œå½’ä¸€åŒ–)
- **æ€»ç»´åº¦**: 5 + 2 + 50 = 57ç»´

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:276
initial_state = state  # ç›´æ¥ä½¿ç”¨WorldState

# WorldStateEncoder (trajectory_encoder.py:79-148)
def forward(self, world_state_batch: List[WorldState]) -> torch.Tensor:
    # Ego features: [pos_x, pos_y, vel_x, vel_y, yaw]
    ego_feat = torch.tensor([
        ws.ego.position_m[0],      # ç»å¯¹åæ ‡ï¼Œæ— å½’ä¸€åŒ–
        ws.ego.position_m[1],
        ws.ego.velocity_mps[0],
        ws.ego.velocity_mps[1],
        ws.ego.yaw_rad
    ])

    # Agent features: [pos_x, pos_y, vel_x, vel_y, heading, type]
    agent_feat = torch.tensor([
        agent.position_m[0],       # ç»å¯¹åæ ‡ï¼Œæ— å½’ä¸€åŒ–
        agent.position_m[1],
        agent.velocity_mps[0],
        agent.velocity_mps[1],
        agent.heading_rad,
        agent_type_encoding
    ])

    # Self-attentionèšåˆ
    agent_aggregated, _ = self.attention(ego_query, agent_features, ...)

    return self.fusion([ego_features, agent_aggregated])
```

**ç‰¹å¾æ€»ç»“**:
- Ego: 5ç»´ (ç»å¯¹åæ ‡ã€é€Ÿåº¦å‘é‡ã€æœå‘)
- Agents: å¯å˜æ•°é‡ Ã— 6ç»´ (ç»å¯¹åæ ‡ã€é€Ÿåº¦å‘é‡ã€æœå‘ã€ç±»å‹)
- **æ— Goalä¿¡æ¯**
- **æ— å½’ä¸€åŒ–**
- **æ— ç›¸å¯¹è·ç¦»**

**å…³é”®å·®å¼‚**:
| ç‰¹æ€§ | PPO | Rainbow DQN |
|------|-----|-------------|
| åæ ‡ç³» | ç›¸å¯¹åæ ‡ (ego-centric) | ç»å¯¹åæ ‡ |
| å½’ä¸€åŒ– | âœ“ (/100, /30) | âœ— |
| Goalä¿¡æ¯ | âœ“ ç›¸å¯¹è·ç¦» | âœ— æ—  |
| Agentæ•°é‡ | å›ºå®š10ä¸ª | å¯å˜ (attention) |
| è·ç¦»ç¼–ç  | æ‰‹åŠ¨è®¡ç®—ç›¸å¯¹è·ç¦» | ç½‘ç»œå­¦ä¹  |

**æ½œåœ¨é—®é¢˜**:
1. âŒ **ä½ç½®ä¸å˜æ€§ç¼ºå¤±**: ç›¸åŒç›¸å¯¹é…ç½®åœ¨ä¸åŒç»å¯¹ä½ç½®äº§ç”Ÿä¸åŒç‰¹å¾
2. âŒ **ç¼ºå°‘ç›®æ ‡å¯¼å‘**: æ— æ³•çŸ¥é“ç›®æ ‡åœ¨å“ªé‡Œ
3. âŒ **æ•°å€¼èŒƒå›´å¤§**: ç»å¯¹åæ ‡å¯èƒ½[-100, 100]ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§
4. âœ… **ä¼˜åŠ¿**: å¯å˜agentæ•°é‡ï¼Œæ›´çµæ´»

---

## 4. åŠ¨ä½œé€‰æ‹© (Action Selection)

### PPO
```python
# run_ppo_carla.py:345-356
state_features = self.planner._extract_state_features(state)
with torch.no_grad():
    logits, value = self.planner.network(state_features)
    action_probs = F.softmax(logits, dim=-1)
    action_dist = Categorical(probs=action_probs)
    action_idx = action_dist.sample().item()
    log_prob = action_dist.log_prob(torch.tensor(action_idx))

# å…³é”®ï¼šè®¾ç½®plannerå†…éƒ¨å˜é‡
self.planner._last_action_idx = action_idx
self.planner._last_log_prob = log_prob
self.planner._last_value = value
```

**æ¢ç´¢æœºåˆ¶**: Categoricalé‡‡æ · + Entropy bonus

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:278-286
# ä¿å­˜åˆå§‹state
initial_state = state

# æ¢ç´¢ï¼šé‡ç½®Noisy Netså™ªå£°
self.planner.q_network.reset_noise()
self.planner.q_network.train()

with torch.no_grad():
    q_dist, q_values = self.planner.q_network([state])
    action_idx = q_values.argmax(dim=1).item()
```

**æ¢ç´¢æœºåˆ¶**: Noisy Nets (å‚æ•°å™ªå£°)

**çŠ¶æ€**: âœ… éƒ½æœ‰æ¢ç´¢æœºåˆ¶ï¼Œä½†æ–¹å¼ä¸åŒ
- PPO: ç­–ç•¥ç½‘ç»œè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œé‡‡æ ·
- Rainbow DQN: Q-networkå‚æ•°å™ªå£°ï¼Œargmax

**é—®é¢˜æ£€æŸ¥**:
- âœ… Rainbow DQNè°ƒç”¨äº†`reset_noise()`å¯ç”¨æ¢ç´¢
- âœ… è®¾ç½®äº†`train()`æ¨¡å¼

---

## 5. è½¨è¿¹æ‰§è¡Œ (Trajectory Execution)

### PPO
```python
# run_ppo_carla.py:387-443
for step in range(max_steps):
    control = self._trajectory_to_control(state, selected_trajectory, step)
    step_result = self.env.step(control)

    # ä¸è°ƒç”¨planner.update()

    # ç´¯ç§¯reward
    episode_reward += step_result.reward

    # è·å–OBBæ£€æµ‹ç»“æœ
    step_near_miss = step_result.info.get('near_miss', False)
    obb_min_dist = step_result.info.get('min_distance_to_agents', float('inf'))

    # è®°å½•è·ç¦»
    step_min_distances.append(current_min_dist)
    step_obb_distances.append(obb_min_dist)
    step_near_miss_flags.append(step_near_miss)

    # ç´¯ç§¯reward breakdown
    if 'reward_breakdown' in step_result.info:
        for comp_name, comp_data in step_result.info['reward_breakdown'].items():
            reward_breakdown_accum[comp_name]['raw'] += comp_data['raw']
            reward_breakdown_accum[comp_name]['weighted'] += comp_data['weighted']

    state = step_result.observation

    if step_result.terminated or step_result.truncated:
        break
```

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:318-376
for step in range(max_steps):
    control = self._trajectory_to_control(state, selected_trajectory, step)
    step_result = self.env.step(control)

    # ä¸è°ƒç”¨planner.update()

    # è·å–OBBæ£€æµ‹ç»“æœ
    step_near_miss = step_result.info.get('near_miss', False)
    obb_min_dist = step_result.info.get('min_distance_to_agents', float('inf'))

    # è®°å½•è·ç¦»
    step_min_distances.append(current_min_dist)
    step_obb_distances.append(obb_min_dist)
    step_near_miss_flags.append(step_near_miss)

    # ç´¯ç§¯reward breakdown
    if 'reward_breakdown' in step_result.info:
        for comp_name, comp_data in step_result.info['reward_breakdown'].items():
            reward_breakdown_accum[comp_name]['raw'] += comp_data['raw']
            reward_breakdown_accum[comp_name]['weighted'] += comp_data['weighted']

    # ç´¯ç§¯reward
    episode_reward += step_result.reward

    state = step_result.observation

    if step_result.terminated or step_result.truncated:
        break
```

**çŠ¶æ€**: âœ… å®Œå…¨ä¸€è‡´
- éƒ½ä¸åœ¨å¾ªç¯å†…è°ƒç”¨update()
- éƒ½ä½¿ç”¨OBBæ£€æµ‹
- éƒ½ç´¯ç§¯reward breakdown

---

## 6. Episodeç»“æŸå­˜å‚¨ (Episode-level Storage)

### PPO
```python
# run_ppo_carla.py:449-457
if self.planner._last_log_prob is not None:
    self.planner.rollout_buffer.push(
        state=state_features,          # åˆå§‹çŠ¶æ€ç‰¹å¾ (tensor)
        action=action_idx,             # è½¨è¿¹ç´¢å¼•
        reward=episode_reward,         # æ€»episode reward
        value=self.planner._last_value,
        log_prob=self.planner._last_log_prob,
        done=True,
    )
```

**å­˜å‚¨æ•°æ®**:
- state: åˆå§‹çŠ¶æ€çš„ç‰¹å¾tensor (57ç»´)
- action: è½¨è¿¹ç´¢å¼•
- reward: episodeæ€»reward
- é™„åŠ : value, log_prob (PPOéœ€è¦)

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:384-390
self.planner.replay_buffer.push(
    state=initial_state,        # åˆå§‹WorldStateå¯¹è±¡
    action=action_idx,          # è½¨è¿¹ç´¢å¼•
    reward=episode_reward,      # æ€»episode reward
    next_state=final_state,     # æœ€ç»ˆWorldStateå¯¹è±¡
    done=True
)
```

**å­˜å‚¨æ•°æ®**:
- state: åˆå§‹WorldStateå¯¹è±¡
- action: è½¨è¿¹ç´¢å¼•
- reward: episodeæ€»reward
- next_state: æœ€ç»ˆWorldStateå¯¹è±¡
- done: True

**å…³é”®å·®å¼‚**:
| é¡¹ç›® | PPO | Rainbow DQN |
|------|-----|-------------|
| stateç±»å‹ | Tensor (ç‰¹å¾) | WorldStateå¯¹è±¡ |
| next_state | æ—  (on-policyä¸éœ€è¦) | WorldStateå¯¹è±¡ |
| reward | episodeæ€»å’Œ | episodeæ€»å’Œ |
| é™„åŠ ä¿¡æ¯ | value, log_prob | æ—  |

**çŠ¶æ€**: âœ… ç¬¦åˆå„è‡ªç®—æ³•è¦æ±‚
- PPOæ˜¯on-policyï¼Œåªéœ€å½“å‰stateçš„ç‰¹å¾
- Rainbow DQNæ˜¯off-policyï¼Œéœ€è¦(s, a, r, s', done)äº”å…ƒç»„

**æ½œåœ¨é—®é¢˜æ£€æŸ¥**:
- âœ… éƒ½å­˜å‚¨episode-level transition (æ¯ä¸ªepisodeä¸€æ¡è®°å½•)
- âœ… éƒ½å­˜å‚¨æ€»episode reward
- âœ… Rainbow DQNçš„WorldStateå¯¹è±¡å¯ä»¥è¢«encoderå¤„ç†
- â“ **ç–‘é—®**: Rainbow DQNçš„initial_stateå’Œfinal_stateåœ¨featureä¸Šå·®å¼‚å¤§å—ï¼Ÿ
  - initial_state: episodeå¼€å§‹æ—¶çš„WorldState
  - final_state: æ‰§è¡Œ50æ­¥åçš„WorldState
  - è¿™ä¸¤ä¸ªçŠ¶æ€çš„ç›¸å¯¹å…³ç³»ï¼ˆegoåˆ°agentsçš„è·ç¦»ï¼‰å¯èƒ½ç±»ä¼¼ï¼Œä½†ç»å¯¹ä½ç½®å®Œå…¨ä¸åŒ
  - å¦‚æœç½‘ç»œå­¦ä¹ çš„æ˜¯ç»å¯¹ä½ç½®ï¼Œå¯èƒ½æ— æ³•æ³›åŒ–

---

## 7. è®­ç»ƒæ—¶æœº (Training Trigger)

### PPO
```python
# run_ppo_carla.py:459-468
metrics = None
buffer_len = len(self.planner.rollout_buffer)
if buffer_len >= self.planner.config.batch_size:
    print(f"  ğŸ”„ PPOæ›´æ–°! buffer={buffer_len}, batch_size={self.planner.config.batch_size}")
    metrics = self.planner._ppo_update()
    if metrics:
        print(f"     policy_loss={metrics.get('policy_loss', 0):.4f}, "
              f"value_loss={metrics.get('value_loss', 0):.4f}, "
              f"entropy={metrics.get('entropy', 0):.4f}")
```

**è§¦å‘æ¡ä»¶**: `buffer_len >= batch_size`
**è®­ç»ƒæ–¹æ³•**: `_ppo_update()`

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:393-402
metrics = None
buffer_len = len(self.planner.replay_buffer)
if buffer_len >= self.planner.config.training.batch_size:
    if self.verbose:
        print(f"  ğŸ”„ Rainbow DQNæ›´æ–°! buffer={buffer_len}")
    if hasattr(self.planner, '_train_step'):
        metrics = self.planner._train_step()
```

**è§¦å‘æ¡ä»¶**: `buffer_len >= batch_size`
**è®­ç»ƒæ–¹æ³•**: `_train_step()`

**çŠ¶æ€**: âœ… ä¸€è‡´

**é—®é¢˜æ£€æŸ¥**:
- âœ… æœ‰hasattræ£€æŸ¥ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
- âš ï¸ **ç¼ºå°‘metricsæ‰“å°**: PPOæ‰“å°lossä¿¡æ¯ï¼ŒRainbow DQNæ²¡æ‰“å°

---

## 8. è®­ç»ƒé€»è¾‘ (Training Logic)

### PPO (_ppo_update)
```python
# planner.py:241-363
def _ppo_update(self) -> Dict[str, float]:
    # 1. ä»bufferè·å–æ‰€æœ‰æ•°æ®
    states, actions, rewards, values, log_probs, advantages = self.rollout_buffer.get()

    # 2. è®¡ç®—returns (GAEæˆ–ç®€å•ç´¯ç§¯)
    returns = advantages + values

    # 3. å½’ä¸€åŒ–advantages
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # 4. å¤šè½®è®­ç»ƒ (K epochs)
    for epoch in range(self.config.ppo_epochs):
        # Mini-batchè®­ç»ƒ
        for batch_idx in range(num_mini_batches):
            # è®¡ç®—æ–°çš„log_probå’Œvalue
            new_logits, new_values = self.network(batch_states)
            new_log_probs = new_dist.log_prob(batch_actions)

            # PPO clip loss
            ratio = (new_log_probs - batch_log_probs).exp()
            surr1 = ratio * batch_advantages
            surr2 = torch.clamp(ratio, 1-eps, 1+eps) * batch_advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # Value loss
            value_loss = F.mse_loss(new_values, batch_returns)

            # Entropy bonus
            entropy = new_dist.entropy().mean()

            # Total loss
            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy

            # æ›´æ–°
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(parameters, max_grad_norm)
            optimizer.step()

    # 5. æ¸…ç©ºbuffer
    self.rollout_buffer.clear()

    return metrics
```

**å…³é”®ç‰¹æ€§**:
- On-policy: æ¯æ¬¡æ›´æ–°åæ¸…ç©ºbuffer
- å¤šè½®è®­ç»ƒ: Kä¸ªepochs
- Mini-batch: åˆ†æ‰¹è®­ç»ƒ
- PPOç‰¹æœ‰: ratio clipping, entropy bonus

### Rainbow DQN (_train_step)
```python
# planner.py:293-373
def _train_step(self) -> UpdateMetrics:
    # 1. æ£€æŸ¥æ˜¯å¦å‡†å¤‡å¥½è®­ç»ƒ
    if len(self.replay_buffer) < batch_size:
        return UpdateMetrics(...)

    # 2. é‡‡æ ·batch (prioritized)
    batch, indices, weights = self.replay_buffer.sample(batch_size)

    # 3. å‡†å¤‡æ•°æ®
    states = [t.state for t in batch]  # List[WorldState]
    actions = torch.LongTensor([t.action for t in batch])
    rewards = torch.FloatTensor([t.reward for t in batch])
    next_states = [t.next_state for t in batch]
    dones = torch.FloatTensor([float(t.done) for t in batch])
    weights_tensor = torch.FloatTensor(weights)

    # 4. å½“å‰Qåˆ†å¸ƒ
    self.q_network.train()
    q_dist, _ = self.q_network(states)  # (batch, actions, atoms)
    q_dist = q_dist[range(len(batch)), actions, :]  # é€‰æ‹©å®é™…actionçš„åˆ†å¸ƒ

    # 5. ç›®æ ‡Qåˆ†å¸ƒ (Double DQN + C51)
    with torch.no_grad():
        # Double DQN: online networké€‰æ‹©action
        _, next_q_values = self.q_network(next_states)
        next_actions = next_q_values.argmax(dim=1)

        # Target networkè¯„ä¼°
        next_q_dist, _ = self.target_network(next_states)
        next_q_dist = next_q_dist[range(len(batch)), next_actions, :]

        # C51 projection
        target_dist = self._project_distribution(rewards, next_q_dist, dones)

    # 6. è®¡ç®—loss (KL divergence)
    log_q_dist = q_dist.log()
    loss_elementwise = -(target_dist * log_q_dist).sum(dim=1)
    loss = (weights_tensor * loss_elementwise).mean()

    # 7. æ›´æ–°priorities
    td_errors = loss_elementwise.detach().cpu().numpy()
    self.replay_buffer.update_priorities(indices, td_errors)

    # 8. åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    nn.utils.clip_grad_norm_(parameters, gradient_clip)
    optimizer.step()

    # 9. æ›´æ–°target network (è½¯æ›´æ–°æˆ–å‘¨æœŸæ€§ç¡¬æ›´æ–°)
    if update_count % target_update_freq == 0:
        target_network.load_state_dict(q_network.state_dict())

    return UpdateMetrics(...)
```

**å…³é”®ç‰¹æ€§**:
- Off-policy: bufferä¸æ¸…ç©ºï¼ŒæŒç»­ç§¯ç´¯
- å•æ¬¡è®­ç»ƒ: æ¯æ¬¡è°ƒç”¨åªè®­ç»ƒä¸€ä¸ªbatch
- Prioritized sampling: æ ¹æ®TD erroré‡‡æ ·
- Rainbowç‰¹æœ‰: distributional RL (C51), Double DQN, target network

**çŠ¶æ€**: âœ… ç¬¦åˆå„è‡ªç®—æ³•è®¾è®¡

**æ½œåœ¨é—®é¢˜**:
- âš ï¸ **è®­ç»ƒé¢‘ç‡**: Rainbow DQNæ¯æ¬¡åªè®­ç»ƒä¸€ä¸ªbatchï¼Œå¯èƒ½å­¦ä¹ è¾ƒæ…¢
  - PPO: bufferæ»¡æ—¶ï¼Œå¤šè½®è®­ç»ƒç›´åˆ°æ¸…ç©º
  - Rainbow DQN: bufferæ»¡æ—¶ï¼Œè®­ç»ƒä¸€ä¸ªbatchï¼Œbufferç»§ç»­ä¿ç•™
  - å»ºè®®: è€ƒè™‘æ¯æ¬¡è®­ç»ƒå¤šä¸ªbatchï¼Œæˆ–å¢åŠ è®­ç»ƒé¢‘ç‡

---

## 9. æ—¥å¿—è®°å½• (Logging)

### PPO
```python
# run_ppo_carla.py:509-563
# Reward breakdownå†™å…¥æ–‡ä»¶
with open(self.reward_log_path, 'a') as f:
    f.write(f"Episode {episode_id}\n")
    f.write(f"  Selected Action: {action_idx}\n")
    f.write(f"  Total Reward: {episode_reward:.4f}\n")
    f.write("  Reward Breakdown:\n")
    for comp_name, comp_data in reward_breakdown_accum.items():
        f.write(f"  {comp_name} {comp_data['weight']} {comp_data['raw']} {comp_data['weighted']}\n")

    # Step-by-stepè·ç¦»
    f.write("  Step-by-Step Distance Analysis:\n")
    for step_idx in range(len(step_min_distances)):
        f.write(f"  {step_idx} {center_dist} {obb_dist} {near_miss_flag}\n")

# Episode summary CSV
with open(self.summary_log_path, 'a') as f:
    f.write(f"{episode_id},{episode_reward:.2f},{collision},{near_miss},{steps},{action_idx},{outcome}\n")
```

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:443-474
# Reward breakdownå†™å…¥æ–‡ä»¶
with open(self.reward_log_path, 'a') as f:
    f.write(f"Episode {episode_id}\n")
    f.write(f"  Selected Action: {action_idx}\n")
    f.write(f"  Total Reward: {episode_reward:.4f}\n")
    f.write("  Reward Breakdown:\n")
    for comp_name, comp_data in reward_breakdown_accum.items():
        f.write(f"  {comp_name} {comp_data['weight']} {comp_data['raw']} {comp_data['weighted']}\n")

    # Step-by-stepè·ç¦»
    f.write("  Step-by-Step Distance Analysis:\n")
    for step_idx in range(len(step_min_distances)):
        f.write(f"  {step_idx} {center_dist} {obb_dist} {near_miss_flag}\n")

# Episode summary CSV
with open(self.summary_log_path, 'a') as f:
    f.write(f"{episode_id},{episode_reward:.2f},{collision},{near_miss},{steps},{action_idx},{outcome}\n")
```

**çŠ¶æ€**: âœ… å®Œå…¨ä¸€è‡´

---

## 10. è¾“å‡ºç›®å½•ç»“æ„

### PPO
```python
# run_ppo_carla.py:830-833
timestamp = time.strftime("%Y%m%d_%H%M%S")
run_name = f"{args.scenario}_{timestamp}"
output_dir = Path(args.output_dir) / run_name
log_dir = (Path(args.log_dir) / run_name) if TENSORBOARD_AVAILABLE else None
```

### Rainbow DQN
```python
# run_rainbow_dqn_carla.py:693-701
timestamp = time.strftime("%Y%m%d_%H%M%S")
run_name = f"{args.scenario}_{timestamp}"
output_dir = Path(args.output_dir) / run_name
log_dir = (Path(args.log_dir) / run_name) if TENSORBOARD_AVAILABLE else None
```

**çŠ¶æ€**: âœ… å®Œå…¨ä¸€è‡´

---

## æ€»ç»“

### âœ… å·²å¯¹é½çš„éƒ¨åˆ†
1. ç¯å¢ƒé‡ç½®æµç¨‹
2. è½¨è¿¹ç”Ÿæˆé€»è¾‘
3. è½¨è¿¹æ‰§è¡Œï¼ˆä½¿ç”¨OBBæ£€æµ‹ï¼Œç´¯ç§¯reward breakdownï¼‰
4. Episode-level transitionå­˜å‚¨
5. è®­ç»ƒæ—¶æœºè§¦å‘
6. æ—¥å¿—è®°å½•ï¼ˆreward breakdown, step-by-stepè·ç¦»ï¼‰
7. è¾“å‡ºç›®å½•ç»“æ„

### âŒ å…³é”®å·®å¼‚ï¼ˆè®¾è®¡ç†å¿µä¸åŒï¼Œæ— éœ€ä¿®æ”¹ï¼‰
1. **ç‰¹å¾æå–æ–¹å¼**:
   - PPO: æ‰‹åŠ¨æå–ï¼Œç›¸å¯¹è·ç¦»ï¼Œå½’ä¸€åŒ–
   - Rainbow DQN: ç¥ç»ç½‘ç»œç¼–ç ï¼Œç»å¯¹åæ ‡

2. **æ¢ç´¢æœºåˆ¶**:
   - PPO: Categoricalé‡‡æ · + Entropy bonus
   - Rainbow DQN: Noisy Netså‚æ•°å™ªå£°

3. **è®­ç»ƒèŒƒå¼**:
   - PPO: On-policyï¼Œbufferæ»¡æ—¶å¤šè½®è®­ç»ƒåæ¸…ç©º
   - Rainbow DQN: Off-policyï¼ŒbufferæŒç»­ç§¯ç´¯ï¼Œæ¯æ¬¡è®­ç»ƒä¸€ä¸ªbatch

### âš ï¸ æ½œåœ¨é—®é¢˜å’Œå»ºè®®

#### é—®é¢˜1: WorldStateEncoderç¼ºå°‘å…³é”®ç‰¹å¾
**ç°è±¡**: Rainbow DQNä½¿ç”¨ç»å¯¹åæ ‡ï¼Œæ— ç›¸å¯¹è·ç¦»ã€Goalä¿¡æ¯ã€å½’ä¸€åŒ–

**å½±å“**:
- ä½ç½®ä¸å˜æ€§ç¼ºå¤±: (ego at (0,0), agent at (10,0)) å’Œ (ego at (100,0), agent at (110,0)) äº§ç”Ÿä¸åŒç‰¹å¾
- ç¼ºå°‘ç›®æ ‡å¯¼å‘: ç½‘ç»œä¸çŸ¥é“goalåœ¨å“ªé‡Œ
- æ•°å€¼èŒƒå›´å¤§: å¯èƒ½å½±å“è®­ç»ƒç¨³å®šæ€§

**å»ºè®®**:
- å…ˆæµ‹è¯•å½“å‰ç‰ˆæœ¬æ•ˆæœ
- å¦‚æœæ€§èƒ½ä¸ä½³ï¼Œè€ƒè™‘æ”¹è¿›WorldStateEncoder:
  - æ·»åŠ ç›¸å¯¹è·ç¦»è®¡ç®—
  - æ·»åŠ goalä¿¡æ¯
  - æ·»åŠ ç‰¹å¾å½’ä¸€åŒ–
  - éœ€è¦ä¿®æ”¹network.pyå’Œtrajectory_encoder.py

#### é—®é¢˜2: è®­ç»ƒé¢‘ç‡å¯èƒ½è¿‡ä½
**ç°è±¡**: Rainbow DQNæ¯æ¬¡åªè®­ç»ƒä¸€ä¸ªbatchï¼ŒPPOä¼šå¤šè½®è®­ç»ƒ

**å½±å“**:
- å­¦ä¹ é€Ÿåº¦å¯èƒ½è¾ƒæ…¢
- Bufferç§¯ç´¯å¤§é‡æ•°æ®ä½†åˆ©ç”¨ç‡ä½

**å»ºè®®**:
- è€ƒè™‘æ¯æ¬¡è°ƒç”¨_train_stepæ—¶è®­ç»ƒå¤šä¸ªbatch:
  ```python
  if buffer_len >= batch_size:
      for _ in range(train_iterations_per_update):
          metrics = self.planner._train_step()
  ```
- æˆ–è€…å¢åŠ è®­ç»ƒé¢‘ç‡ï¼ˆæ¯Nä¸ªepisodeè®­ç»ƒä¸€æ¬¡ï¼Œä½†æ¯æ¬¡è®­ç»ƒå¤šä¸ªiterationsï¼‰

#### é—®é¢˜3: ç¼ºå°‘è®­ç»ƒmetricsæ‰“å°
**ç°è±¡**: PPOæ‰“å°lossä¿¡æ¯ï¼ŒRainbow DQNæ²¡æœ‰

**å»ºè®®**: åœ¨run_rainbow_dqn_carla.pyä¸­æ·»åŠ :
```python
if metrics:
    print(f"     loss={metrics.get('loss', 0):.4f}, "
          f"q_value={metrics.get('q_value', 0):.4f}, "
          f"td_error={metrics.custom.get('td_error_mean', 0):.4f}")
```

#### é—®é¢˜4: æ— warmupæ£€æŸ¥
**ç°è±¡**: _train_stepæœ‰warmupæ£€æŸ¥ï¼Œä½†run_episodeæ²¡æœ‰ç›¸åº”æç¤º

**å»ºè®®**: åœ¨è®­ç»ƒæ—¶æ·»åŠ warmupæç¤º:
```python
if buffer_len >= batch_size:
    if self._step_count < warmup_steps:
        print(f"  â³ Warmupé˜¶æ®µ: {self._step_count}/{warmup_steps}")
    else:
        print(f"  ğŸ”„ Rainbow DQNæ›´æ–°!")
        metrics = self.planner._train_step()
```

---

## éœ€è¦ç«‹å³ä¿®å¤çš„é—®é¢˜

### æ— ï¼ˆå·²å®Œæˆæ‰€æœ‰æ ¸å¿ƒä¿®å¤ï¼‰

å½“å‰Rainbow DQNå®ç°åœ¨ç®—æ³•å±‚é¢æ˜¯æ­£ç¡®çš„ï¼Œä¸PPOçš„å·®å¼‚ä¸»è¦æ¥è‡ªè®¾è®¡ç†å¿µä¸åŒï¼ˆon-policy vs off-policyï¼Œæ‰‹åŠ¨ç‰¹å¾ vs ç¥ç»ç½‘ç»œç¼–ç ï¼‰ã€‚

---

## å»ºè®®çš„åç»­ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

1. **æ·»åŠ è®­ç»ƒmetricsæ‰“å°** - æ–¹ä¾¿ç›‘æ§è®­ç»ƒè¿‡ç¨‹
2. **å¢åŠ è®­ç»ƒé¢‘ç‡æˆ–æ¯æ¬¡è®­ç»ƒå¤šä¸ªbatch** - æé«˜å­¦ä¹ æ•ˆç‡
3. **æµ‹è¯•å½“å‰ç‰ˆæœ¬** - åœ¨s4ç­‰åœºæ™¯æµ‹è¯•æ•ˆæœ
4. **å¦‚æœæ•ˆæœä¸ä½³ï¼Œæ”¹è¿›WorldStateEncoder** - æ·»åŠ ç›¸å¯¹è·ç¦»ã€goalã€å½’ä¸€åŒ–

---

## éªŒè¯æ¸…å•

è¿è¡Œ: `python examples/run_rainbow_dqn_carla.py --scenario s4 --episodes 10 --max-steps 50`

æ£€æŸ¥:
- [ ] æ¯ä¸ªepisodeåªå­˜å‚¨1ä¸ªtransition
- [ ] Episode rewardæ˜¯ç´¯ç§¯å€¼ï¼ˆå¦‚-5.0ï¼‰
- [ ] Near-missä½¿ç”¨OBBè·ç¦»
- [ ] è¾“å‡ºåˆ°`outputs/rainbow_dqn_carla/s4_YYYYMMDD_HHMMSS/`
- [ ] æœ‰æ¢ç´¢è¡Œä¸ºï¼ˆä¸åŒepisodeé€‰ä¸åŒè½¨è¿¹ï¼‰
- [ ] æ—¥å¿—æ–‡ä»¶åŒ…å«reward breakdownå’Œè·ç¦»åˆ†æ
- [ ] Bufferæ»¡æ—¶è§¦å‘è®­ç»ƒ
- [ ] è®­ç»ƒlossåˆç†ä¸‹é™
